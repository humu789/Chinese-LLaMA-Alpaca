WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-07-03 11:55:58,588] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
07/03/2023 11:55:59 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
07/03/2023 11:55:59 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
07/03/2023 11:55:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:666] 2023-07-03 11:55:59,548 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/config.json
[INFO|configuration_utils.py:720] 2023-07-03 11:55:59,549 >> Model config LlamaConfig {
  "_name_or_path": "/nvme/share_data/llama_ckpts/huggingface/7B",
  "architectures": [
    "LLaMAForCausalLM"
  ],
  "bos_token_id": 0,
  "eos_token_id": 1,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": -1,
  "rms_norm_eps": 1e-06,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:55:59,549 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:55:59,549 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:55:59,549 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:55:59,549 >> loading file tokenizer_config.json
07/03/2023 11:55:59 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
07/03/2023 11:55:59 - INFO - __main__ - training datasets-llama7b_generate_data_0 has been loaded from disk
07/03/2023 11:55:59 - INFO - __main__ - training datasets-llama7b_generate_data_1 has been loaded from disk
07/03/2023 11:55:59 - INFO - __main__ - training datasets-llama7b_generate_data_3 has been loaded from disk
07/03/2023 11:55:59 - INFO - __main__ - training datasets-llama7b_generate_data_2 has been loaded from disk
07/03/2023 11:55:59 - INFO - __main__ - training datasets-llama7b_generate_data_5 has been loaded from disk
07/03/2023 11:55:59 - INFO - __main__ - training datasets-llama7b_generate_data_4 has been loaded from disk
07/03/2023 11:55:59 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-8ec458195b19ede5.arrow
07/03/2023 11:55:59 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-ce64e31a0ac76c05.arrow
07/03/2023 11:56:01 - INFO - __main__ - Num train_samples  25802
07/03/2023 11:56:01 - INFO - __main__ - training example:
07/03/2023 11:56:01 - INFO - __main__ - “Genetic and Environmental Resilience in the Nucleus Accumbens Core and Shell in Adolescent Mice.” The Journal of Neuroscience 28(August 2008): 7042–50. ⁇  [20] L. G. Fonseca, U. B. Maniscalco, L. Cock, D. Boddy, E. C. Miller, J. M. P. Triberti, F. G. Villela, L. R. Geller, and L. P. Costa. “Reducing Stress-Induced Deficits in Neuroendocrine and Behavioral Response in Male Golden Hamsters: Further Evidence for the Influence of the Stress Hormone 17-Keto-Progesterone on the HPA Axis,” Neuro Endocrinol Lett. 17 (November 1996): 345–348. ⁇  [21] Benson and Ballow, “Breathing, the Pulse, Mindfulness, and the Emotions, Benson and Mindfulness and the Healing Power of the Breath,” pp. 25–7. ⁇  [22] Benson, Herbert. Mindfulness and the Relaxation Response, Truman Talley Books, New York, 1984 pp. 44–5. ⁇  [23] Herbert Benson, The Relaxation Response: W.W. Norton & Company, 2000. ⁇  [24] Benson Herbert, Relaxation Response, Wilder Press, 1984 or the Victor Pian Publishing Co. or Simon and Schuster, or the Riverhead Publishing Company ⁇  [25] Letha Dawson Scanzoni. Choosing. Continuum, NY: Continuum, 1994. pp. p. 137. ⁇  [26] Herbert Benson and Ilene Klaus. The Healing Power of the Breath. 1983, Benson Hyman Press, pp. 224–31. ⁇  [27] G. Morris, J. R. Evans and A. E. Duncan, “Physiological and Cognitive Aspects of intermittent Breathing
07/03/2023 11:56:01 - INFO - __main__ - Num eval_samples  26
07/03/2023 11:56:01 - INFO - __main__ - training example:
07/03/2023 11:56:01 - INFO - __main__ - 8, a total of nearly 4,000 suspected cases of Ebola have been reported to WHO by Guinea, Liberia, Sierra Leone and now Nigeria, according to WHO. Nearly half of these cases are fatal. People at greatest risk of becoming infected are those who have been in regular close contact with sick patients; they include family members, health workers, children who establish close contact while caring for sick loved ones, and household members. ⁇  Out of more than 4000 suspected cases, WHO’s current weekly tracking indicates nearly 2,000 suspected cases of Ebola have confirmed since these two countries were first informed of Ebola cases five months ago. ⁇  réalité, et, _précisément à cause de cette réalité_ , il la trahit au nom de ces valeurs... ⁇  Mais régulièrement, dans ses dialogues amoureux, l'époux ou l'amant le _raisonnent_ – parce qu'il y a quelque chose de réel qui les gêne. Et, dans cette raison, on comprend aussi, on endosse, on écoute. La jeune nouvelle, La boutonnière des actifs, raconte ce qui arrive à un jeune homme qui subit une distance de son épouse, à son insu logiquement, mais un silence injouable, dépourvu de toute économie du couperet. La distance elle-même finit par se révéler illusoire (l'épouse bêche sans que le jeune mari le sache; la régularité, alors, du cours ordinaire des choses conjure en soi des tensions). On peut noter, dans le déroulement de l'histoire, merveilleusement précis, combien la rationalisation des hommes s'accable seuls : y a-t-il du rite ou est-il essentiellement du dire? Lire cette nouvelle, ici par exemple : ⁇  On me demande librement quand j'oublierai de me demander ce qu'ont pu penser les autres en regardant le boutonnière de mon pantalon qui sera peut-être d'autant plus sage qu'il sera d'autant mieux sali. ⁇  Pas très démonstratif, en privé, mais enfin qui
[INFO|modeling_utils.py:2531] 2023-07-03 11:56:01,716 >> loading weights file /nvme/share_data/llama_ckpts/huggingface/7B/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-07-03 11:56:01,717 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:575] 2023-07-03 11:56:01,718 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": -1,
  "transformers_version": "4.28.1"
}

07/03/2023 11:56:01 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-8ec458195b19ede5.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-ce64e31a0ac76c05.arrow
07/03/2023 11:56:01 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-8ec458195b19ede5.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-ce64e31a0ac76c05.arrow
07/03/2023 11:56:01 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-8ec458195b19ede5.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-ce64e31a0ac76c05.arrow
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:04,  6.52it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  5.51it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  5.38it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:07,  4.08it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:04,  6.94it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.72it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.66it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:04,  7.17it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:07,  4.43it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.82it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.75it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  7.23it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:06,  4.66it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  5.90it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:03,  7.27it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  5.82it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:05,  4.85it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:00<00:03,  7.12it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  5.97it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  5.91it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:00<00:03,  7.14it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:04,  6.07it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:04,  6.01it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:06,  4.64it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:03,  7.18it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.13it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  5.96it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  7.26it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:05,  4.50it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.18it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.06it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  7.30it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.23it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.11it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:02,  7.34it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:05,  4.43it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.24it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:02,  7.27it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.10it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:05,  4.37it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:01<00:02,  7.28it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.26it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.14it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:01<00:02,  7.29it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.25it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:02<00:05,  4.35it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.16it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  7.28it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.27it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.17it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  7.34it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:05,  4.36it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  6.31it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  6.17it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  7.33it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.30it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:05,  4.36it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  7.35it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.16it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.26it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:02<00:01,  7.32it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.13it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:02<00:04,  4.36it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:02<00:01,  7.30it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.26it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.13it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:02<00:01,  7.34it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:04,  4.36it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.25it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.15it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  7.32it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.23it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:03<00:04,  4.37it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.14it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  7.35it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.25it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  7.32it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.10it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:03<00:04,  4.37it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.23it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:03<00:01,  7.31it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.10it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.20it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:03<00:00,  7.35it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:03<00:03,  4.39it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.10it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:03<00:00,  7.31it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.17it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.10it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:03<00:03,  4.41it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:03<00:00,  7.35it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.18it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.08it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:03<00:00,  7.32it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:04<00:03,  4.44it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  6.18it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  7.33it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  6.08it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.19it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:04<00:00,  7.37it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:04<00:03,  4.48it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.06it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.18it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:04<00:00,  7.33it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.05it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:04<00:02,  4.48it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.18it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:04<00:00,  7.12it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:04<00:00,  7.26it/s]
Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.06it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:04<00:02,  4.51it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.16it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.06it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.13it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:04<00:02,  4.48it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.04it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:05<00:00,  6.13it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:05<00:00,  6.01it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:05<00:02,  4.47it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  6.14it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  5.98it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:05<00:02,  4.47it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.86it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.13it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.57it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.00it/s]
Loading checkpoint shards:  76%|███████▌  | 25/33 [00:05<00:01,  4.41it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:05<00:01,  4.35it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:06<00:01,  4.31it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:06<00:01,  4.29it/s]Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Loading checkpoint shards:  88%|████████▊ | 29/33 [00:06<00:00,  4.29it/s]Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Loading checkpoint shards:  91%|█████████ | 30/33 [00:06<00:00,  4.32it/s]Convert model.layers.0.mlp.gate_proj to QLinear
Loading checkpoint shards:  94%|█████████▍| 31/33 [00:07<00:00,  4.34it/s]Convert model.layers.0.mlp.down_proj to QLinear
Loading checkpoint shards:  97%|█████████▋| 32/33 [00:07<00:00,  4.37it/s]Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  4.33it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  4.40it/s]
[INFO|modeling_utils.py:3190] 2023-07-03 11:56:09,318 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2023-07-03 11:56:09,318 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /nvme/share_data/llama_ckpts/huggingface/7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2023-07-03 11:56:09,321 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/generation_config.json
[INFO|configuration_utils.py:575] 2023-07-03 11:56:09,321 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
07/03/2023 11:56:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
Convert model.layers.29.mlp.down_proj to QLinear
07/03/2023 11:56:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
07/03/2023 11:56:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
[INFO|trainer.py:564] 2023-07-03 11:56:49,213 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:621] 2023-07-03 11:56:49,214 >> Using cuda_amp half precision backend
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-07-03 11:56:51,130] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
07/03/2023 11:56:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
07/03/2023 11:56:52 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2023 11:56:52 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2023 11:56:52 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2023 11:56:52 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-07-03 11:56:53,292] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-07-03 11:56:53,292] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-07-03 11:56:53,292] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-07-03 11:56:53,305] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-07-03 11:56:53,305] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-07-03 11:56:53,305] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-07-03 11:56:53,305] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-07-03 11:56:53,305] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000
[2023-07-03 11:56:53,305] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000
[2023-07-03 11:56:53,305] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-07-03 11:56:53,305] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/humu/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.18878793716430664 seconds
Loading extension module utils...
Time to load utils op: 0.20243501663208008 seconds
Loading extension module utils...
Time to load utils op: 0.20237088203430176 seconds
Loading extension module utils...
Time to load utils op: 0.20245122909545898 seconds
Rank: 0 partition count [4] and sizes[(1684603904, False)] 
Rank: 3 partition count [4] and sizes[(1684603904, False)] 
Rank: 1 partition count [4] and sizes[(1684603904, False)] 
Rank: 2 partition count [4] and sizes[(1684603904, False)] 
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003571510314941406 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00046253204345703125 seconds
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 720, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 686, in main
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 720, in <module>
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
    main()
  File "run_clm_pt_wo_peft.py", line 686, in main
        return inner_training_loop(train_result = trainer.train(resume_from_checkpoint=checkpoint)

  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1731, in _inner_training_loop
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
        deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(return inner_training_loop(

  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/deepspeed.py", line 398, in deepspeed_init
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1731, in _inner_training_loop
    raise ValueError(f"Can't find a valid checkpoint at {resume_from_checkpoint}")
ValueError: Can't find a valid checkpoint at None
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/deepspeed.py", line 398, in deepspeed_init
    raise ValueError(f"Can't find a valid checkpoint at {resume_from_checkpoint}")
ValueError: Can't find a valid checkpoint at None
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00067138671875 seconds
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 720, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 686, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1731, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/deepspeed.py", line 398, in deepspeed_init
    raise ValueError(f"Can't find a valid checkpoint at {resume_from_checkpoint}")
ValueError: Can't find a valid checkpoint at None
[2023-07-03 11:57:04,549] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-07-03 11:57:04,550] [INFO] [utils.py:786:see_memory_usage] MA 31.47 GB         Max_MA 34.61 GB         CA 34.62 GB         Max_CA 35 GB 
[2023-07-03 11:57:04,551] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 48.46 GB, percent = 4.8%
[2023-07-03 11:57:04,654] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-07-03 11:57:04,655] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 56.58 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-07-03 11:57:04,655] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 48.45 GB, percent = 4.8%
[2023-07-03 11:57:04,655] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-07-03 11:57:04,735] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-07-03 11:57:04,736] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 44.02 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-07-03 11:57:04,736] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 48.45 GB, percent = 4.8%
[2023-07-03 11:57:04,738] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-07-03 11:57:04,739] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-07-03 11:57:04,739] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f2c0a3839d0>
[2023-07-03 11:57:04,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-07-03 11:57:04,741] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   amp_enabled .................. False
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   amp_params ................... False
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
[2023-07-03 11:57:04,741] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2c0a3aaaf0>
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   communication_data_type ...... None
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   disable_allgather ............ False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   dump_state ................... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   elasticity_enabled ........... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   fp16_auto_cast ............... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   fp16_enabled ................. True
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   global_rank .................. 0
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   loss_scale ................... 0
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   memory_breakdown ............. False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   mics_shard_size .............. -1
[2023-07-03 11:57:04,742] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   optimizer_name ............... None
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   optimizer_params ............. None
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   pld_enabled .................. False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   pld_params ................... False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   prescale_gradients ........... False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   scheduler_name ............... None
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   scheduler_params ............. None
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   sparse_attention ............. None
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   steps_per_print .............. 2000
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   train_batch_size ............. 4
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   use_node_local_storage ....... False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   world_size ................... 4
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   zero_enabled ................. True
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
[2023-07-03 11:57:04,743] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2
[2023-07-03 11:57:04,743] [INFO] [config.py:945:print_user_config]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 100, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1e-10
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 1.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 4, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "zero_allow_untested_optimizer": true
}
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00034165382385253906 seconds
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 720, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 686, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/trainer.py", line 1731, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/deepspeed.py", line 398, in deepspeed_init
    raise ValueError(f"Can't find a valid checkpoint at {resume_from_checkpoint}")
ValueError: Can't find a valid checkpoint at None
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 100759) of binary: /home/humu/miniconda3/envs/cn_llama/bin/python
Traceback (most recent call last):
  File "/home/humu/miniconda3/envs/cn_llama/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_clm_pt_wo_peft.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-07-03_11:57:09
  host      : SH-IDC1-10-140-24-142
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 100761)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-07-03_11:57:09
  host      : SH-IDC1-10-140-24-142
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 100763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-07-03_11:57:09
  host      : SH-IDC1-10-140-24-142
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 100764)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-03_11:57:09
  host      : SH-IDC1-10-140-24-142
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 100759)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-07-03 11:58:45,607] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
07/03/2023 11:58:46 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
07/03/2023 11:58:46 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
07/03/2023 11:58:46 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
07/03/2023 11:58:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:666] 2023-07-03 11:58:46,386 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/config.json
[INFO|configuration_utils.py:720] 2023-07-03 11:58:46,386 >> Model config LlamaConfig {
  "_name_or_path": "/nvme/share_data/llama_ckpts/huggingface/7B",
  "architectures": [
    "LLaMAForCausalLM"
  ],
  "bos_token_id": 0,
  "eos_token_id": 1,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": -1,
  "rms_norm_eps": 1e-06,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:58:46,386 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:58:46,386 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:58:46,386 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2023-07-03 11:58:46,387 >> loading file tokenizer_config.json
07/03/2023 11:58:46 - INFO - __main__ - training datasets-llama7b_generate_data_0 has been loaded from disk
07/03/2023 11:58:46 - INFO - __main__ - training datasets-llama7b_generate_data_1 has been loaded from disk
07/03/2023 11:58:46 - INFO - __main__ - training datasets-llama7b_generate_data_3 has been loaded from disk
07/03/2023 11:58:46 - INFO - __main__ - training datasets-llama7b_generate_data_2 has been loaded from disk
07/03/2023 11:58:46 - INFO - __main__ - training datasets-llama7b_generate_data_5 has been loaded from disk
07/03/2023 11:58:46 - INFO - __main__ - training datasets-llama7b_generate_data_4 has been loaded from disk
07/03/2023 11:58:46 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-cfd9a93241e73985.arrow
07/03/2023 11:58:46 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-b4c8762a34a57e21.arrow
07/03/2023 11:58:48 - INFO - __main__ - Num train_samples  25802
07/03/2023 11:58:48 - INFO - __main__ - training example:
07/03/2023 11:58:48 - INFO - __main__ -  ⁇  Hobart Renewal Company chairman Syd Clark says the new bridge would create an “incredible” experience for travel on foot and bicycle and kick-start improvements to old urban roads in the City of Hobart. ⁇  Tags:architecture-pick, Hobart, Water, windows ⁇  Shaycarl ‘takes down’ controversial LandcareQ ad over Facebook ⁇  ‘We’re in better shape than we thought’: Coalition team tour east coast communities | Australia news ⁇  What the climate action in Death Valley can tell us about Australia’s climate future | Australia news ⁇  Before Kangaroos 360: the the heartwarming and heartbreaking Lisa Wilkinson ‘stuff-up’ | Australia news ⁇  Labour would ban sale of new petrol and diesel cars from 2030 | Business ⁇  Miyazaki, Rintaro ⁇  A peak plasma density of 8 Ã— 10 19 m âˆ’3 is achieved in toroidal plasma discharges by generating a high-density, uniform inner halo eroding liner beam and inserting a rotating magnetic flux surface into the plasma discharges. The upgraded 2.5MW T-3U tokamak advanced research plant (spear3-ant) at the National Institute for Fusion Science produces a plasma mass of 730 g anomalous to mode conversion, plasma current decay and beam-induced potential pumping are effectively suppressed while beam-driven density peaking is preserved. Beam injection was conducted at a speed matching resonance condition (Î· = 30%) with B = 0.12 T, Ip = 1.5 MA (177 Hz, 1.7 MW). The highest magnetic field is produced around 7cm from the Magnetic Center (MC), where density induced mode conversion rotates the toroidal fluxes. A peaked plasma density exceeding 1020m âˆ’3 is confirmed in the inner halo. A cross sectional view of the low-density doughnut shaped core plasma is realized, while the toroidal fluxes around the inner halo high-density core are elongated into a ring shape. Positive longitudinal beta-to-knee ratio without stabilizers is V Î² 2
07/03/2023 11:58:48 - INFO - __main__ - Num eval_samples  26
07/03/2023 11:58:48 - INFO - __main__ - training example:
07/03/2023 11:58:48 - INFO - __main__ -  ⁇  ⁇  String response = SIMPLE.getAndParseKeptResult(wws.buildGetBackedQueueBody(data), Configuracion.defaultConfig()); ⁇  if (response == null) ⁇      return null; ⁇  ⁇  final XMLHandler xmlHandler = XMLHandler.getInstance().getXmlHandler(configoracion); ⁇  final String xml = response; ⁇  xmlHandler.appendAfterPost(LongCollection.AFTER_POST, xml); ⁇  xmlHandler.toString(data, LongCollection.TO_PREVIOUS_RECO_ID + "=" + LongCollection.TO_PLACEHOLDER); ⁇  xmlHandler.toString(data, LongCollection.TO_NEW_RECO_ID + "=" + LongCollection.TO_PLACEHOLDER); ⁇  xmlHandler.toString(data, LongCollection.TO_RECORD_ID + "=" + LongCollection.TO_PLACEHOLDER); ⁇  ⁇  // save configured new records ⁇  ArrayList<Long> rids = null; ⁇  try { ⁇      xmlHandler.appendAfterPost(LongCollection.AFTER_POST, xml); ⁇      ClassificationRecordsDefinition typeRecords = ClassificationRecordsDefinition.fromString(xml, xmlHandler.config(), xmlHandler.params());\ ⁇      rids = xmlHandler.recordsId (typeRecords, configoracion); ⁇  } ⁇  catch (Exception e) { ⁇      return null; ⁇  } ⁇  ⁇  // update collection ⁇  ArrayList<Long> recordsIdsUpdatedAndRemoved = xmlHandler.updateRecordsId(typeRecords, batch, idremoved); ⁇  // update ⁇  ArrayList<Long> recordsIdsUpdated = xmlHandler.updateRecordsId(false, typeRecords, Configuracion.V2, recordsIdsUpdatedAndRemoved); ⁇  ArrayList<Long> recordsIdsUpdatedB2 = xmlHandler.updateRecordsId(false, typeRecords, Configuracion.V3, recordsIdsUpdatedAndRemoved); ⁇  ⁇  // save updated selected ids ⁇  ArrayList<Long> data2; ⁇  refreshExpiration(data, LongCollection.TO_REC_ID + "=" + LongCollection.TO_PLACEHOLDER); ⁇  if (BATCH_SIZE >= 0) ⁇      data2 = xmlHandler.intoBoundQueue(data, LongCollections.fromData(recordsIdsUpdatedAndRemoved), LongCollections.fromData(recordsIdsUpdated
[INFO|modeling_utils.py:2531] 2023-07-03 11:58:48,621 >> loading weights file /nvme/share_data/llama_ckpts/huggingface/7B/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-07-03 11:58:48,621 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:575] 2023-07-03 11:58:48,622 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": -1,
  "transformers_version": "4.28.1"
}

07/03/2023 11:58:48 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-cfd9a93241e73985.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-b4c8762a34a57e21.arrow
07/03/2023 11:58:48 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-cfd9a93241e73985.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-b4c8762a34a57e21.arrow
07/03/2023 11:58:48 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-cfd9a93241e73985.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-b4c8762a34a57e21.arrow
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  6.29it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:06,  5.29it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:06,  5.27it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:06,  5.25it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.63it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.45it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.44it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:06,  4.85it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.84it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.85it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.15it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:06,  4.59it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  5.98it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  5.99it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:05,  4.98it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.01it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.01it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:06,  4.26it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:05,  5.02it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:04,  5.79it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:04,  5.50it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:06,  4.38it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:05,  5.06it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  5.87it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:05,  5.16it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:05,  4.52it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:05,  5.07it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.05it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.05it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:05,  4.75it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  5.05it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:05,  4.53it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  5.89it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:04,  5.00it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:05,  4.51it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:05,  4.48it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  5.91it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:04,  5.02it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:05,  4.61it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:05,  4.36it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:02<00:03,  6.03it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:04,  5.06it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:04,  4.74it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.19it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:05,  4.15it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:02<00:04,  5.06it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  5.94it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:02<00:04,  4.71it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  5.06it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:03,  5.99it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:02<00:05,  3.97it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:02<00:04,  4.68it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  5.85it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  5.05it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:04,  4.74it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:05,  3.79it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  5.98it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:03,  5.07it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:03<00:04,  4.73it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:03<00:02,  6.02it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:03<00:03,  5.08it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:03<00:05,  3.68it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  5.85it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:03<00:03,  4.71it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:03<00:03,  5.09it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  5.91it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:03<00:04,  3.71it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:03<00:03,  4.78it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:03<00:02,  5.04it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:02,  5.95it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:03<00:03,  4.87it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:03<00:04,  3.77it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.10it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  4.97it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:03<00:03,  4.97it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.25it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  4.99it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:03<00:04,  3.78it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:04<00:01,  6.21it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:04<00:02,  5.01it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:04<00:02,  5.02it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  6.16it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:04<00:03,  3.79it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:04<00:02,  5.01it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:04<00:02,  5.00it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.12it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:04<00:02,  5.02it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:04<00:03,  3.83it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.19it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:04<00:02,  4.99it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:04<00:02,  5.06it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.31it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:04<00:03,  3.86it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:04<00:01,  4.81it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:04<00:01,  5.14it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.34it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  4.71it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.32it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:04<00:01,  5.23it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:05<00:03,  3.68it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:05<00:00,  5.99it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:05<00:01,  5.12it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:05<00:01,  4.59it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:05<00:03,  3.56it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  6.00it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:05<00:01,  5.03it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:05<00:01,  4.57it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.70it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.98it/s]
Loading checkpoint shards:  82%|████████▏ | 27/33 [00:05<00:01,  5.20it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:05<00:02,  3.53it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:05<00:01,  4.57it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:05<00:01,  4.90it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:05<00:00,  4.56it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:05<00:02,  3.26it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:06<00:00,  4.58it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:06<00:00,  4.61it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:06<00:00,  4.64it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:06<00:00,  4.31it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:06<00:02,  3.01it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:06<00:00,  4.64it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:06<00:00,  4.05it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:06<00:00,  4.48it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:06<00:00,  4.88it/s]
[INFO|modeling_utils.py:3190] 2023-07-03 11:58:55,490 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2023-07-03 11:58:55,490 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /nvme/share_data/llama_ckpts/huggingface/7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2023-07-03 11:58:55,493 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/generation_config.json
[INFO|configuration_utils.py:575] 2023-07-03 11:58:55,493 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

Loading checkpoint shards:  79%|███████▉  | 26/33 [00:06<00:02,  2.85it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:06<00:00,  3.95it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:07<00:02,  2.75it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  3.73it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  4.60it/s]
Loading checkpoint shards:  85%|████████▍ | 28/33 [00:07<00:01,  2.67it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:07<00:01,  2.64it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:08<00:01,  2.62it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:08<00:00,  2.60it/s]Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Loading checkpoint shards:  97%|█████████▋| 32/33 [00:09<00:00,  2.59it/s]Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Loading checkpoint shards: 100%|██████████| 33/33 [00:09<00:00,  2.49it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:09<00:00,  3.46it/s]
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
[INFO|trainer.py:564] 2023-07-03 11:59:31,242 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:621] 2023-07-03 11:59:31,242 >> Using cuda_amp half precision backend
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-07-03 11:59:32,761] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
07/03/2023 11:59:34 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
07/03/2023 11:59:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:2 (world_size=4, worker_count=1, timeout=0:30:00)
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
07/03/2023 11:59:47 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
07/03/2023 11:59:49 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
07/03/2023 11:59:53 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
07/03/2023 11:59:53 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2023 11:59:53 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2023 11:59:53 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2023 11:59:53 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-07-03 11:59:54,257] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-07-03 11:59:54,258] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-07-03 11:59:54,258] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-07-03 11:59:54,271] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-07-03 11:59:54,271] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-07-03 11:59:54,271] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-07-03 11:59:54,271] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-07-03 11:59:54,271] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000
[2023-07-03 11:59:54,271] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000
[2023-07-03 11:59:54,271] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-07-03 11:59:54,271] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/humu/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2224576473236084 seconds
Loading extension module utils...
Time to load utils op: 0.20260858535766602 seconds
Loading extension module utils...
Time to load utils op: 0.30348658561706543 seconds
Loading extension module utils...
Time to load utils op: 0.3032681941986084 seconds
Rank: 0 partition count [4] and sizes[(1684603904, False)] 
Rank: 2 partition count [4] and sizes[(1684603904, False)] 
Rank: 1 partition count [4] and sizes[(1684603904, False)] 
Rank: 3 partition count [4] and sizes[(1684603904, False)] 
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003933906555175781 seconds
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004916191101074219 seconds
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006358623504638672 seconds
[WARNING|logging.py:295] 2023-07-03 12:00:09,785 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-07-03 12:00:09,786 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-07-03 12:00:09,790 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-07-03 12:00:09,796] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-07-03 12:00:09,797] [INFO] [utils.py:786:see_memory_usage] MA 31.47 GB         Max_MA 34.61 GB         CA 34.62 GB         Max_CA 35 GB 
[2023-07-03 12:00:09,797] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.25 GB, percent = 2.4%
[2023-07-03 12:00:09,912] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-07-03 12:00:09,913] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 56.58 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-07-03 12:00:09,913] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.3 GB, percent = 2.4%
[2023-07-03 12:00:09,913] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-07-03 12:00:10,001] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-07-03 12:00:10,002] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 44.02 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-07-03 12:00:10,002] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.47 GB, percent = 2.4%
[2023-07-03 12:00:10,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-07-03 12:00:10,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-07-03 12:00:10,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f6c5972fa30>
[2023-07-03 12:00:10,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-07-03 12:00:10,007] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
[2023-07-03 12:00:10,007] [INFO] [config.py:959:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   amp_enabled .................. False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   amp_params ................... False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6c59757b50>
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   communication_data_type ...... None
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   disable_allgather ............ False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   dump_state ................... False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
[2023-07-03 12:00:10,008] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   elasticity_enabled ........... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   fp16_auto_cast ............... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   fp16_enabled ................. True
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   global_rank .................. 0
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   loss_scale ................... 0
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   memory_breakdown ............. False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   mics_shard_size .............. -1
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   optimizer_name ............... None
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   optimizer_params ............. None
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   pld_enabled .................. False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   pld_params ................... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   prescale_gradients ........... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   scheduler_name ............... None
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   scheduler_params ............. None
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   sparse_attention ............. None
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   steps_per_print .............. 2000
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   train_batch_size ............. 4
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   use_node_local_storage ....... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   world_size ................... 4
[2023-07-03 12:00:10,009] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True
[2023-07-03 12:00:10,010] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-07-03 12:00:10,010] [INFO] [config.py:959:print]   zero_enabled ................. True
[2023-07-03 12:00:10,010] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
[2023-07-03 12:00:10,010] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2
[2023-07-03 12:00:10,010] [INFO] [config.py:945:print_user_config]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 100, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1e-10
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 1.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 4, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "zero_allow_untested_optimizer": true
}
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003962516784667969 seconds
[INFO|trainer.py:1769] 2023-07-03 12:00:10,012 >> ***** Running training *****
[INFO|trainer.py:1770] 2023-07-03 12:00:10,012 >>   Num examples = 25,802
[INFO|trainer.py:1771] 2023-07-03 12:00:10,012 >>   Num Epochs = 8
[INFO|trainer.py:1772] 2023-07-03 12:00:10,012 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1773] 2023-07-03 12:00:10,012 >>   Total train batch size (w. parallel, distributed & accumulation) = 4
[INFO|trainer.py:1774] 2023-07-03 12:00:10,012 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1775] 2023-07-03 12:00:10,012 >>   Total optimization steps = 50,000
[INFO|trainer.py:1776] 2023-07-03 12:00:10,014 >>   Number of trainable parameters = 6,738,415,616
  0%|          | 0/50000 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-07-03 12:00:10,026 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-07-03 12:00:11,720] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  0%|          | 1/50000 [00:01<23:41:06,  1.71s/it]                                                    {'loss': 1669.5, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/50000 [00:01<23:41:06,  1.71s/it][2023-07-03 12:00:12,711] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
  0%|          | 2/50000 [00:02<17:51:09,  1.29s/it][2023-07-03 12:00:13,698] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
  0%|          | 3/50000 [00:03<15:57:41,  1.15s/it][2023-07-03 12:00:14,680] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
  0%|          | 4/50000 [00:04<15:02:21,  1.08s/it][2023-07-03 12:00:15,667] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
  0%|          | 5/50000 [00:05<14:33:33,  1.05s/it][2023-07-03 12:00:16,654] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
  0%|          | 6/50000 [00:06<14:16:33,  1.03s/it][2023-07-03 12:00:17,639] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
  0%|          | 7/50000 [00:07<14:04:28,  1.01s/it][2023-07-03 12:00:18,621] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
  0%|          | 8/50000 [00:08<13:56:10,  1.00s/it][2023-07-03 12:00:19,602] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
  0%|          | 9/50000 [00:09<13:50:16,  1.00it/s][2023-07-03 12:00:20,584] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
  0%|          | 10/50000 [00:10<13:46:30,  1.01it/s]                                                     {'loss': 1795.2361, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 10/50000 [00:10<13:46:30,  1.01it/s][2023-07-03 12:00:21,575] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
  0%|          | 11/50000 [00:11<13:46:06,  1.01it/s][2023-07-03 12:00:22,552] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
  0%|          | 12/50000 [00:12<13:42:25,  1.01it/s][2023-07-03 12:00:23,543] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
  0%|          | 13/50000 [00:13<13:43:25,  1.01it/s][2023-07-03 12:00:24,531] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
  0%|          | 14/50000 [00:14<13:43:09,  1.01it/s][2023-07-03 12:00:25,522] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
  0%|          | 15/50000 [00:15<13:43:55,  1.01it/s][2023-07-03 12:00:26,512] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
  0%|          | 16/50000 [00:16<13:44:20,  1.01it/s][2023-07-03 12:00:27,503] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  0%|          | 17/50000 [00:17<13:44:33,  1.01it/s]  0%|          | 18/50000 [00:18<14:47:11,  1.07s/it][2023-07-03 12:00:29,742] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
  0%|          | 19/50000 [00:19<14:30:47,  1.05s/it]  0%|          | 20/50000 [00:20<15:18:58,  1.10s/it]                                                     {'loss': 1868.1625, 'learning_rate': 1.6e-08, 'epoch': 0.0}
  0%|          | 20/50000 [00:20<15:18:58,  1.10s/it][2023-07-03 12:00:31,969] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 21/50000 [00:21<14:50:19,  1.07s/it]  0%|          | 22/50000 [00:23<15:33:11,  1.12s/it]  0%|          | 23/50000 [00:24<16:03:34,  1.16s/it]  0%|          | 24/50000 [00:25<16:23:42,  1.18s/it]  0%|          | 25/50000 [00:26<16:38:48,  1.20s/it]  0%|          | 26/50000 [00:28<16:49:11,  1.21s/it]  0%|          | 27/50000 [00:29<16:55:01,  1.22s/it]  0%|          | 28/50000 [00:30<17:00:16,  1.23s/it]  0%|          | 29/50000 [00:31<17:03:26,  1.23s/it]  0%|          | 30/50000 [00:33<17:05:41,  1.23s/it]                                                     {'loss': 1930.7, 'learning_rate': 8.800000000000001e-08, 'epoch': 0.0}
  0%|          | 30/50000 [00:33<17:05:41,  1.23s/it]  0%|          | 31/50000 [00:34<17:07:40,  1.23s/it]  0%|          | 32/50000 [00:35<17:08:33,  1.24s/it]  0%|          | 33/50000 [00:36<17:09:33,  1.24s/it]  0%|          | 34/50000 [00:38<17:09:23,  1.24s/it]  0%|          | 35/50000 [00:39<17:09:28,  1.24s/it]  0%|          | 36/50000 [00:40<17:09:37,  1.24s/it]  0%|          | 37/50000 [00:41<17:10:08,  1.24s/it]  0%|          | 38/50000 [00:43<17:09:46,  1.24s/it]  0%|          | 39/50000 [00:44<17:09:26,  1.24s/it]  0%|          | 40/50000 [00:45<17:09:20,  1.24s/it]                                                     {'loss': 1750.975, 'learning_rate': 1.68e-07, 'epoch': 0.01}
  0%|          | 40/50000 [00:45<17:09:20,  1.24s/it]  0%|          | 41/50000 [00:46<17:09:56,  1.24s/it]  0%|          | 42/50000 [00:47<17:09:37,  1.24s/it]  0%|          | 43/50000 [00:49<17:09:27,  1.24s/it]  0%|          | 44/50000 [00:50<17:08:55,  1.24s/it]  0%|          | 45/50000 [00:51<17:08:53,  1.24s/it]  0%|          | 46/50000 [00:52<17:08:54,  1.24s/it]  0%|          | 47/50000 [00:54<17:08:27,  1.24s/it]  0%|          | 48/50000 [00:55<17:08:17,  1.24s/it]  0%|          | 49/50000 [00:56<17:07:48,  1.23s/it]  0%|          | 50/50000 [00:57<17:07:31,  1.23s/it]                                                     {'loss': 1885.7625, 'learning_rate': 2.48e-07, 'epoch': 0.01}
  0%|          | 50/50000 [00:57<17:07:31,  1.23s/it]  0%|          | 51/50000 [00:59<17:07:28,  1.23s/it]  0%|          | 52/50000 [01:00<17:07:45,  1.23s/it]  0%|          | 53/50000 [01:01<17:07:23,  1.23s/it]  0%|          | 54/50000 [01:02<17:07:30,  1.23s/it]  0%|          | 55/50000 [01:04<17:09:07,  1.24s/it][2023-07-03 12:01:15,018] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 56/50000 [01:05<16:08:43,  1.16s/it]  0%|          | 57/50000 [01:06<16:28:24,  1.19s/it]  0%|          | 58/50000 [01:07<16:43:19,  1.21s/it][2023-07-03 12:01:18,508] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 59/50000 [01:08<15:51:58,  1.14s/it]  0%|          | 60/50000 [01:09<16:17:55,  1.17s/it]                                                     {'loss': 1593.625, 'learning_rate': 3.12e-07, 'epoch': 0.01}
  0%|          | 60/50000 [01:09<16:17:55,  1.17s/it]  0%|          | 61/50000 [01:10<16:36:32,  1.20s/it]  0%|          | 62/50000 [01:12<16:47:52,  1.21s/it]  0%|          | 63/50000 [01:13<16:57:11,  1.22s/it]  0%|          | 64/50000 [01:14<17:02:42,  1.23s/it]  0%|          | 65/50000 [01:15<17:05:23,  1.23s/it]  0%|          | 66/50000 [01:17<17:07:09,  1.23s/it]  0%|          | 67/50000 [01:18<17:08:24,  1.24s/it]  0%|          | 68/50000 [01:19<17:09:14,  1.24s/it]  0%|          | 69/50000 [01:20<17:11:28,  1.24s/it]  0%|          | 70/50000 [01:22<17:12:49,  1.24s/it]                                                     {'loss': 1508.9688, 'learning_rate': 3.92e-07, 'epoch': 0.01}
  0%|          | 70/50000 [01:22<17:12:49,  1.24s/it]  0%|          | 71/50000 [01:23<17:14:26,  1.24s/it]  0%|          | 72/50000 [01:24<17:14:39,  1.24s/it]  0%|          | 73/50000 [01:25<17:15:23,  1.24s/it]  0%|          | 74/50000 [01:27<17:15:28,  1.24s/it]  0%|          | 75/50000 [01:28<17:15:42,  1.24s/it]  0%|          | 76/50000 [01:29<17:15:46,  1.24s/it]  0%|          | 77/50000 [01:30<17:15:23,  1.24s/it]  0%|          | 78/50000 [01:32<17:15:36,  1.24s/it]  0%|          | 79/50000 [01:33<17:14:35,  1.24s/it]  0%|          | 80/50000 [01:34<17:13:19,  1.24s/it]                                                     {'loss': 1187.0125, 'learning_rate': 4.7200000000000004e-07, 'epoch': 0.01}
  0%|          | 80/50000 [01:34<17:13:19,  1.24s/it]  0%|          | 81/50000 [01:35<17:13:46,  1.24s/it]  0%|          | 82/50000 [01:37<17:13:42,  1.24s/it]  0%|          | 83/50000 [01:38<17:12:55,  1.24s/it]  0%|          | 84/50000 [01:39<17:13:31,  1.24s/it]  0%|          | 85/50000 [01:40<17:12:45,  1.24s/it]  0%|          | 86/50000 [01:42<17:12:07,  1.24s/it]  0%|          | 87/50000 [01:43<17:11:51,  1.24s/it]  0%|          | 88/50000 [01:44<17:11:59,  1.24s/it]  0%|          | 89/50000 [01:45<17:11:14,  1.24s/it]  0%|          | 90/50000 [01:47<17:11:00,  1.24s/it]                                                     {'loss': 1317.1063, 'learning_rate': 5.520000000000001e-07, 'epoch': 0.01}
  0%|          | 90/50000 [01:47<17:11:00,  1.24s/it]  0%|          | 91/50000 [01:48<17:11:16,  1.24s/it]  0%|          | 92/50000 [01:49<17:10:42,  1.24s/it]  0%|          | 93/50000 [01:50<17:09:43,  1.24s/it]  0%|          | 94/50000 [01:51<17:11:30,  1.24s/it]  0%|          | 95/50000 [01:53<17:11:27,  1.24s/it]  0%|          | 96/50000 [01:54<17:12:05,  1.24s/it]  0%|          | 97/50000 [01:55<17:11:38,  1.24s/it]  0%|          | 98/50000 [01:56<17:10:41,  1.24s/it]  0%|          | 99/50000 [01:58<17:10:27,  1.24s/it]  0%|          | 100/50000 [01:59<17:10:14,  1.24s/it]                                                      {'loss': 1157.775, 'learning_rate': 6.320000000000002e-07, 'epoch': 0.02}
  0%|          | 100/50000 [01:59<17:10:14,  1.24s/it]  0%|          | 101/50000 [02:00<17:11:51,  1.24s/it]  0%|          | 102/50000 [02:01<17:12:06,  1.24s/it]  0%|          | 103/50000 [02:03<17:12:42,  1.24s/it]  0%|          | 104/50000 [02:04<17:12:29,  1.24s/it]  0%|          | 105/50000 [02:05<17:11:38,  1.24s/it]  0%|          | 106/50000 [02:06<17:11:40,  1.24s/it]  0%|          | 107/50000 [02:08<17:12:15,  1.24s/it]  0%|          | 108/50000 [02:09<17:12:45,  1.24s/it]  0%|          | 109/50000 [02:10<17:11:58,  1.24s/it]  0%|          | 110/50000 [02:11<17:12:05,  1.24s/it]                                                      {'loss': 1038.8312, 'learning_rate': 7.12e-07, 'epoch': 0.02}
  0%|          | 110/50000 [02:11<17:12:05,  1.24s/it]  0%|          | 111/50000 [02:13<17:12:00,  1.24s/it]  0%|          | 112/50000 [02:14<17:11:54,  1.24s/it]  0%|          | 113/50000 [02:15<17:12:37,  1.24s/it]  0%|          | 114/50000 [02:16<17:11:39,  1.24s/it]  0%|          | 115/50000 [02:18<17:10:28,  1.24s/it]  0%|          | 116/50000 [02:19<17:11:25,  1.24s/it]  0%|          | 117/50000 [02:20<17:11:51,  1.24s/it]  0%|          | 118/50000 [02:21<17:12:26,  1.24s/it]  0%|          | 119/50000 [02:23<17:12:45,  1.24s/it]  0%|          | 120/50000 [02:24<17:12:32,  1.24s/it]                                                      {'loss': 787.5375, 'learning_rate': 7.920000000000001e-07, 'epoch': 0.02}
  0%|          | 120/50000 [02:24<17:12:32,  1.24s/it]  0%|          | 121/50000 [02:25<17:14:27,  1.24s/it]  0%|          | 122/50000 [02:26<17:13:22,  1.24s/it]  0%|          | 123/50000 [02:27<17:12:40,  1.24s/it]  0%|          | 124/50000 [02:29<17:12:06,  1.24s/it]  0%|          | 125/50000 [02:30<17:12:07,  1.24s/it]  0%|          | 126/50000 [02:31<17:13:03,  1.24s/it]  0%|          | 127/50000 [02:32<17:12:34,  1.24s/it]  0%|          | 128/50000 [02:34<17:12:56,  1.24s/it]  0%|          | 129/50000 [02:35<17:13:03,  1.24s/it]  0%|          | 130/50000 [02:36<17:12:27,  1.24s/it]                                                      {'loss': 781.6313, 'learning_rate': 8.720000000000001e-07, 'epoch': 0.02}
  0%|          | 130/50000 [02:36<17:12:27,  1.24s/it]  0%|          | 131/50000 [02:37<17:12:25,  1.24s/it]  0%|          | 132/50000 [02:39<17:12:01,  1.24s/it]  0%|          | 133/50000 [02:40<17:11:29,  1.24s/it]  0%|          | 134/50000 [02:41<17:11:55,  1.24s/it]  0%|          | 135/50000 [02:42<17:11:52,  1.24s/it]  0%|          | 136/50000 [02:44<17:12:21,  1.24s/it]  0%|          | 137/50000 [02:45<17:11:57,  1.24s/it]  0%|          | 138/50000 [02:46<17:11:38,  1.24s/it]  0%|          | 139/50000 [02:47<17:11:00,  1.24s/it]  0%|          | 140/50000 [02:49<17:11:21,  1.24s/it]                                                      {'loss': 673.7563, 'learning_rate': 9.520000000000002e-07, 'epoch': 0.02}
  0%|          | 140/50000 [02:49<17:11:21,  1.24s/it]  0%|          | 141/50000 [02:50<17:10:57,  1.24s/it]  0%|          | 142/50000 [02:51<17:11:31,  1.24s/it]  0%|          | 143/50000 [02:52<17:11:28,  1.24s/it]  0%|          | 144/50000 [02:54<17:10:59,  1.24s/it]  0%|          | 145/50000 [02:55<17:11:24,  1.24s/it]  0%|          | 146/50000 [02:56<17:11:03,  1.24s/it]  0%|          | 147/50000 [02:57<17:11:20,  1.24s/it]  0%|          | 148/50000 [02:59<17:11:29,  1.24s/it]  0%|          | 149/50000 [03:00<17:10:13,  1.24s/it]  0%|          | 150/50000 [03:01<17:09:22,  1.24s/it]                                                      {'loss': 551.0656, 'learning_rate': 1.032e-06, 'epoch': 0.02}
  0%|          | 150/50000 [03:01<17:09:22,  1.24s/it]  0%|          | 151/50000 [03:02<17:09:52,  1.24s/it]  0%|          | 152/50000 [03:03<17:09:23,  1.24s/it]  0%|          | 153/50000 [03:05<17:08:48,  1.24s/it]  0%|          | 154/50000 [03:06<17:08:26,  1.24s/it]  0%|          | 155/50000 [03:07<17:07:54,  1.24s/it]  0%|          | 156/50000 [03:08<17:07:46,  1.24s/it]  0%|          | 157/50000 [03:10<17:07:44,  1.24s/it]  0%|          | 158/50000 [03:11<17:07:25,  1.24s/it]  0%|          | 159/50000 [03:12<17:07:07,  1.24s/it]  0%|          | 160/50000 [03:13<17:07:00,  1.24s/it]                                                      {'loss': 659.3781, 'learning_rate': 1.1120000000000001e-06, 'epoch': 0.02}
  0%|          | 160/50000 [03:13<17:07:00,  1.24s/it]  0%|          | 161/50000 [03:15<17:06:43,  1.24s/it]  0%|          | 162/50000 [03:16<17:05:58,  1.24s/it]  0%|          | 163/50000 [03:17<17:05:31,  1.23s/it]  0%|          | 164/50000 [03:18<17:05:10,  1.23s/it]  0%|          | 165/50000 [03:20<17:04:52,  1.23s/it]  0%|          | 166/50000 [03:21<17:04:52,  1.23s/it]  0%|          | 167/50000 [03:22<17:05:45,  1.24s/it]  0%|          | 168/50000 [03:23<17:06:21,  1.24s/it]  0%|          | 169/50000 [03:24<17:06:46,  1.24s/it]  0%|          | 170/50000 [03:26<17:06:39,  1.24s/it]                                                      {'loss': 551.6875, 'learning_rate': 1.1920000000000002e-06, 'epoch': 0.03}
  0%|          | 170/50000 [03:26<17:06:39,  1.24s/it]  0%|          | 171/50000 [03:27<17:06:43,  1.24s/it]  0%|          | 172/50000 [03:28<17:06:55,  1.24s/it]  0%|          | 173/50000 [03:29<17:07:08,  1.24s/it]  0%|          | 174/50000 [03:31<17:06:42,  1.24s/it]  0%|          | 175/50000 [03:32<17:06:41,  1.24s/it]  0%|          | 176/50000 [03:33<17:06:47,  1.24s/it]  0%|          | 177/50000 [03:34<17:06:33,  1.24s/it]  0%|          | 178/50000 [03:36<17:06:26,  1.24s/it]  0%|          | 179/50000 [03:37<17:06:29,  1.24s/it]  0%|          | 180/50000 [03:38<17:06:28,  1.24s/it]                                                      {'loss': 457.7812, 'learning_rate': 1.2720000000000003e-06, 'epoch': 0.03}
  0%|          | 180/50000 [03:38<17:06:28,  1.24s/it]  0%|          | 181/50000 [03:39<17:07:05,  1.24s/it]  0%|          | 182/50000 [03:41<17:06:46,  1.24s/it][2023-07-03 12:03:52,051] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, but hysteresis is 2. Reducing hysteresis to 1
  0%|          | 183/50000 [03:42<16:04:59,  1.16s/it]  0%|          | 184/50000 [03:43<16:23:09,  1.18s/it]  0%|          | 185/50000 [03:44<16:36:26,  1.20s/it]  0%|          | 186/50000 [03:45<16:45:18,  1.21s/it]  0%|          | 187/50000 [03:46<16:52:03,  1.22s/it]  0%|          | 188/50000 [03:48<16:56:27,  1.22s/it]  0%|          | 189/50000 [03:49<16:59:18,  1.23s/it]  0%|          | 190/50000 [03:50<17:01:11,  1.23s/it]                                                      {'loss': 477.2094, 'learning_rate': 1.344e-06, 'epoch': 0.03}
  0%|          | 190/50000 [03:50<17:01:11,  1.23s/it]  0%|          | 191/50000 [03:51<17:03:06,  1.23s/it]  0%|          | 192/50000 [03:53<17:03:59,  1.23s/it]  0%|          | 193/50000 [03:54<17:04:59,  1.23s/it]  0%|          | 194/50000 [03:55<17:05:26,  1.24s/it]  0%|          | 195/50000 [03:56<17:05:49,  1.24s/it]  0%|          | 196/50000 [03:58<17:05:49,  1.24s/it]  0%|          | 197/50000 [03:59<17:05:54,  1.24s/it]  0%|          | 198/50000 [04:00<17:06:00,  1.24s/it]  0%|          | 199/50000 [04:01<17:06:17,  1.24s/it]  0%|          | 200/50000 [04:03<17:07:09,  1.24s/it]                                                      {'loss': 500.1031, 'learning_rate': 1.424e-06, 'epoch': 0.03}
  0%|          | 200/50000 [04:03<17:07:09,  1.24s/it]  0%|          | 201/50000 [04:04<17:07:53,  1.24s/it]  0%|          | 202/50000 [04:05<17:07:21,  1.24s/it]  0%|          | 203/50000 [04:06<17:06:57,  1.24s/it]  0%|          | 204/50000 [04:08<17:07:06,  1.24s/it]  0%|          | 205/50000 [04:09<17:07:22,  1.24s/it]  0%|          | 206/50000 [04:10<17:07:18,  1.24s/it]  0%|          | 207/50000 [04:11<17:07:54,  1.24s/it]  0%|          | 208/50000 [04:12<17:07:46,  1.24s/it]  0%|          | 209/50000 [04:14<17:07:33,  1.24s/it]  0%|          | 210/50000 [04:15<17:07:28,  1.24s/it]                                                      {'loss': 464.6031, 'learning_rate': 1.5040000000000001e-06, 'epoch': 0.03}
  0%|          | 210/50000 [04:15<17:07:28,  1.24s/it]  0%|          | 211/50000 [04:16<17:07:26,  1.24s/it]  0%|          | 212/50000 [04:17<17:07:11,  1.24s/it]  0%|          | 213/50000 [04:19<17:07:05,  1.24s/it][2023-07-03 12:04:30,160] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 214/50000 [04:20<16:04:40,  1.16s/it]  0%|          | 215/50000 [04:21<16:22:59,  1.18s/it]  0%|          | 216/50000 [04:22<16:36:01,  1.20s/it]  0%|          | 217/50000 [04:23<16:45:18,  1.21s/it]  0%|          | 218/50000 [04:25<16:51:39,  1.22s/it]  0%|          | 219/50000 [04:26<16:55:55,  1.22s/it]  0%|          | 220/50000 [04:27<16:58:59,  1.23s/it]                                                      {'loss': 544.5906, 'learning_rate': 1.576e-06, 'epoch': 0.03}
  0%|          | 220/50000 [04:27<16:58:59,  1.23s/it]  0%|          | 221/50000 [04:28<17:01:33,  1.23s/it]  0%|          | 222/50000 [04:30<17:02:59,  1.23s/it]  0%|          | 223/50000 [04:31<17:03:59,  1.23s/it]  0%|          | 224/50000 [04:32<17:05:11,  1.24s/it]  0%|          | 225/50000 [04:33<17:06:03,  1.24s/it]  0%|          | 226/50000 [04:34<17:06:15,  1.24s/it]  0%|          | 227/50000 [04:36<17:06:49,  1.24s/it]  0%|          | 228/50000 [04:37<17:06:52,  1.24s/it]  0%|          | 229/50000 [04:38<17:07:14,  1.24s/it]  0%|          | 230/50000 [04:39<17:07:15,  1.24s/it]                                                      {'loss': 474.125, 'learning_rate': 1.6560000000000001e-06, 'epoch': 0.04}
  0%|          | 230/50000 [04:39<17:07:15,  1.24s/it]  0%|          | 231/50000 [04:41<17:07:56,  1.24s/it]  0%|          | 232/50000 [04:42<17:07:34,  1.24s/it]  0%|          | 233/50000 [04:43<17:07:17,  1.24s/it]  0%|          | 234/50000 [04:44<17:07:21,  1.24s/it]  0%|          | 235/50000 [04:46<17:07:46,  1.24s/it]  0%|          | 236/50000 [04:47<17:07:29,  1.24s/it]  0%|          | 237/50000 [04:48<17:07:56,  1.24s/it]  0%|          | 238/50000 [04:49<17:07:57,  1.24s/it]  0%|          | 239/50000 [04:51<17:08:23,  1.24s/it]  0%|          | 240/50000 [04:52<17:07:52,  1.24s/it]                                                      {'loss': 509.1031, 'learning_rate': 1.7360000000000002e-06, 'epoch': 0.04}
  0%|          | 240/50000 [04:52<17:07:52,  1.24s/it]  0%|          | 241/50000 [04:53<17:08:26,  1.24s/it]  0%|          | 242/50000 [04:54<17:07:38,  1.24s/it]  0%|          | 243/50000 [04:56<17:07:05,  1.24s/it]  0%|          | 244/50000 [04:57<17:06:26,  1.24s/it]  0%|          | 245/50000 [04:58<17:06:12,  1.24s/it]  0%|          | 246/50000 [04:59<17:06:03,  1.24s/it]  0%|          | 247/50000 [05:01<17:05:56,  1.24s/it]  0%|          | 248/50000 [05:02<17:05:58,  1.24s/it]  0%|          | 249/50000 [05:03<17:06:02,  1.24s/it]  0%|          | 250/50000 [05:04<17:06:14,  1.24s/it]                                                      {'loss': 480.0562, 'learning_rate': 1.8160000000000003e-06, 'epoch': 0.04}
  0%|          | 250/50000 [05:04<17:06:14,  1.24s/it]  1%|          | 251/50000 [05:05<17:06:07,  1.24s/it]  1%|          | 252/50000 [05:07<17:05:57,  1.24s/it]  1%|          | 253/50000 [05:08<17:05:47,  1.24s/it]  1%|          | 254/50000 [05:09<17:05:37,  1.24s/it]  1%|          | 255/50000 [05:10<17:05:24,  1.24s/it]  1%|          | 256/50000 [05:12<17:05:32,  1.24s/it]  1%|          | 257/50000 [05:13<17:07:29,  1.24s/it]  1%|          | 258/50000 [05:14<17:08:26,  1.24s/it]  1%|          | 259/50000 [05:15<17:08:40,  1.24s/it]  1%|          | 260/50000 [05:17<17:08:04,  1.24s/it]                                                      {'loss': 455.8469, 'learning_rate': 1.8960000000000001e-06, 'epoch': 0.04}
  1%|          | 260/50000 [05:17<17:08:04,  1.24s/it]  1%|          | 261/50000 [05:18<17:08:36,  1.24s/it]  1%|          | 262/50000 [05:19<17:08:31,  1.24s/it]  1%|          | 263/50000 [05:20<17:07:01,  1.24s/it]  1%|          | 264/50000 [05:22<17:05:35,  1.24s/it]  1%|          | 265/50000 [05:23<17:04:26,  1.24s/it]  1%|          | 266/50000 [05:24<17:06:13,  1.24s/it]  1%|          | 267/50000 [05:25<17:04:50,  1.24s/it]  1%|          | 268/50000 [05:27<17:03:59,  1.24s/it]  1%|          | 269/50000 [05:28<17:03:41,  1.24s/it][2023-07-03 12:05:39,238] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  1%|          | 270/50000 [05:29<16:01:34,  1.16s/it]                                                      {'loss': 452.1719, 'learning_rate': 1.968e-06, 'epoch': 0.04}
  1%|          | 270/50000 [05:29<16:01:34,  1.16s/it]  1%|          | 271/50000 [05:30<16:19:34,  1.18s/it]  1%|          | 272/50000 [05:31<16:31:51,  1.20s/it]  1%|          | 273/50000 [05:32<16:40:35,  1.21s/it]  1%|          | 274/50000 [05:34<16:47:46,  1.22s/it]  1%|          | 275/50000 [05:35<16:54:06,  1.22s/it]  1%|          | 276/50000 [05:36<16:59:16,  1.23s/it]  1%|          | 277/50000 [05:37<17:01:48,  1.23s/it]  1%|          | 278/50000 [05:39<17:02:48,  1.23s/it]  1%|          | 279/50000 [05:40<17:03:31,  1.24s/it]  1%|          | 280/50000 [05:41<17:04:08,  1.24s/it]                                                      {'loss': 440.1781, 'learning_rate': 2.048e-06, 'epoch': 0.04}
  1%|          | 280/50000 [05:41<17:04:08,  1.24s/it]  1%|          | 281/50000 [05:42<17:05:32,  1.24s/it]  1%|          | 282/50000 [05:44<17:05:45,  1.24s/it]  1%|          | 283/50000 [05:45<17:06:36,  1.24s/it]  1%|          | 284/50000 [05:46<17:08:19,  1.24s/it]  1%|          | 285/50000 [05:47<17:11:57,  1.25s/it]  1%|          | 286/50000 [05:49<17:11:17,  1.24s/it]  1%|          | 287/50000 [05:50<17:10:17,  1.24s/it]  1%|          | 288/50000 [05:51<17:09:20,  1.24s/it]  1%|          | 289/50000 [05:52<17:08:02,  1.24s/it]  1%|          | 290/50000 [05:54<17:14:39,  1.25s/it]                                                      {'loss': 461.9438, 'learning_rate': 2.128e-06, 'epoch': 0.04}
  1%|          | 290/50000 [05:54<17:14:39,  1.25s/it]  1%|          | 291/50000 [05:55<17:23:27,  1.26s/it]  1%|          | 292/50000 [05:56<17:17:43,  1.25s/it]  1%|          | 293/50000 [05:57<17:13:33,  1.25s/it]  1%|          | 294/50000 [05:59<17:11:23,  1.24s/it]  1%|          | 295/50000 [06:00<17:09:40,  1.24s/it]  1%|          | 296/50000 [06:01<17:08:58,  1.24s/it]  1%|          | 297/50000 [06:02<17:08:55,  1.24s/it]  1%|          | 298/50000 [06:03<17:08:01,  1.24s/it]  1%|          | 299/50000 [06:05<17:07:17,  1.24s/it]  1%|          | 300/50000 [06:06<17:06:50,  1.24s/it]                                                      {'loss': 424.2969, 'learning_rate': 2.2080000000000003e-06, 'epoch': 0.05}
  1%|          | 300/50000 [06:06<17:06:50,  1.24s/it]  1%|          | 301/50000 [06:07<17:06:54,  1.24s/it]  1%|          | 302/50000 [06:08<17:06:42,  1.24s/it]  1%|          | 303/50000 [06:10<17:06:13,  1.24s/it]  1%|          | 304/50000 [06:11<17:06:11,  1.24s/it]  1%|          | 305/50000 [06:12<17:06:08,  1.24s/it]  1%|          | 306/50000 [06:13<17:05:44,  1.24s/it]  1%|          | 307/50000 [06:15<17:05:44,  1.24s/it]  1%|          | 308/50000 [06:16<17:05:28,  1.24s/it]  1%|          | 309/50000 [06:17<17:05:04,  1.24s/it]  1%|          | 310/50000 [06:18<17:04:41,  1.24s/it]                                                      {'loss': 401.9531, 'learning_rate': 2.2880000000000004e-06, 'epoch': 0.05}
  1%|          | 310/50000 [06:18<17:04:41,  1.24s/it]  1%|          | 311/50000 [06:20<17:05:17,  1.24s/it]  1%|          | 312/50000 [06:21<17:05:18,  1.24s/it]  1%|          | 313/50000 [06:22<17:05:26,  1.24s/it]  1%|          | 314/50000 [06:23<17:05:22,  1.24s/it]  1%|          | 315/50000 [06:25<17:05:13,  1.24s/it]  1%|          | 316/50000 [06:26<17:05:22,  1.24s/it]  1%|          | 317/50000 [06:27<17:05:01,  1.24s/it]  1%|          | 318/50000 [06:28<17:04:43,  1.24s/it]  1%|          | 319/50000 [06:29<17:04:34,  1.24s/it]  1%|          | 320/50000 [06:31<17:04:41,  1.24s/it]                                                      {'loss': 450.4344, 'learning_rate': 2.3680000000000005e-06, 'epoch': 0.05}
  1%|          | 320/50000 [06:31<17:04:41,  1.24s/it]  1%|          | 321/50000 [06:32<17:05:01,  1.24s/it]  1%|          | 322/50000 [06:33<17:04:57,  1.24s/it]  1%|          | 323/50000 [06:34<17:04:50,  1.24s/it]  1%|          | 324/50000 [06:36<17:05:47,  1.24s/it]  1%|          | 325/50000 [06:37<17:05:24,  1.24s/it]  1%|          | 326/50000 [06:38<17:05:09,  1.24s/it]  1%|          | 327/50000 [06:39<17:04:48,  1.24s/it]  1%|          | 328/50000 [06:41<17:04:32,  1.24s/it]  1%|          | 329/50000 [06:42<17:04:25,  1.24s/it]  1%|          | 330/50000 [06:43<17:04:41,  1.24s/it]                                                      {'loss': 446.8188, 'learning_rate': 2.448e-06, 'epoch': 0.05}
  1%|          | 330/50000 [06:43<17:04:41,  1.24s/it]  1%|          | 331/50000 [06:44<17:04:45,  1.24s/it]  1%|          | 332/50000 [06:46<17:04:20,  1.24s/it]  1%|          | 333/50000 [06:47<17:04:01,  1.24s/it]  1%|          | 334/50000 [06:48<17:03:36,  1.24s/it]  1%|          | 335/50000 [06:49<17:04:04,  1.24s/it]  1%|          | 336/50000 [06:51<17:03:59,  1.24s/it]  1%|          | 337/50000 [06:52<17:03:55,  1.24s/it]  1%|          | 338/50000 [06:53<17:03:37,  1.24s/it]  1%|          | 339/50000 [06:54<17:03:39,  1.24s/it]  1%|          | 340/50000 [06:55<17:03:25,  1.24s/it]                                                      {'loss': 352.9937, 'learning_rate': 2.5280000000000006e-06, 'epoch': 0.05}
  1%|          | 340/50000 [06:55<17:03:25,  1.24s/it]  1%|          | 341/50000 [06:57<17:03:54,  1.24s/it]  1%|          | 342/50000 [06:58<17:04:11,  1.24s/it]  1%|          | 343/50000 [06:59<17:03:49,  1.24s/it]  1%|          | 344/50000 [07:00<17:03:38,  1.24s/it]  1%|          | 345/50000 [07:02<17:03:46,  1.24s/it]  1%|          | 346/50000 [07:03<17:03:56,  1.24s/it]  1%|          | 347/50000 [07:04<17:04:07,  1.24s/it]  1%|          | 348/50000 [07:05<17:04:24,  1.24s/it]  1%|          | 349/50000 [07:07<17:04:10,  1.24s/it]  1%|          | 350/50000 [07:08<17:04:08,  1.24s/it]                                                      {'loss': 394.2563, 'learning_rate': 2.608e-06, 'epoch': 0.05}
  1%|          | 350/50000 [07:08<17:04:08,  1.24s/it]  1%|          | 351/50000 [07:09<17:04:09,  1.24s/it]  1%|          | 352/50000 [07:10<17:04:00,  1.24s/it]  1%|          | 353/50000 [07:12<17:03:54,  1.24s/it]  1%|          | 354/50000 [07:13<17:03:45,  1.24s/it]  1%|          | 355/50000 [07:14<17:03:44,  1.24s/it]  1%|          | 356/50000 [07:15<17:03:50,  1.24s/it]  1%|          | 357/50000 [07:17<17:03:38,  1.24s/it]  1%|          | 358/50000 [07:18<17:03:19,  1.24s/it]  1%|          | 359/50000 [07:19<17:03:24,  1.24s/it]  1%|          | 360/50000 [07:20<17:03:27,  1.24s/it]                                                      {'loss': 361.3469, 'learning_rate': 2.688e-06, 'epoch': 0.06}
  1%|          | 360/50000 [07:20<17:03:27,  1.24s/it]  1%|          | 361/50000 [07:21<17:03:37,  1.24s/it]  1%|          | 362/50000 [07:23<17:03:40,  1.24s/it]  1%|          | 363/50000 [07:24<17:03:40,  1.24s/it]  1%|          | 364/50000 [07:25<17:04:14,  1.24s/it]  1%|          | 365/50000 [07:26<17:03:53,  1.24s/it]  1%|          | 366/50000 [07:28<17:03:48,  1.24s/it]  1%|          | 367/50000 [07:29<17:03:58,  1.24s/it]  1%|          | 368/50000 [07:30<17:03:41,  1.24s/it]  1%|          | 369/50000 [07:31<17:03:49,  1.24s/it]  1%|          | 370/50000 [07:33<17:03:34,  1.24s/it]                                                      {'loss': 338.5344, 'learning_rate': 2.768e-06, 'epoch': 0.06}
  1%|          | 370/50000 [07:33<17:03:34,  1.24s/it]  1%|          | 371/50000 [07:34<17:03:42,  1.24s/it]  1%|          | 372/50000 [07:35<17:03:56,  1.24s/it]  1%|          | 373/50000 [07:36<17:03:46,  1.24s/it]  1%|          | 374/50000 [07:38<17:04:26,  1.24s/it]  1%|          | 375/50000 [07:39<17:04:06,  1.24s/it]  1%|          | 376/50000 [07:40<17:03:48,  1.24s/it]  1%|          | 377/50000 [07:41<17:03:44,  1.24s/it]  1%|          | 378/50000 [07:43<17:03:33,  1.24s/it]  1%|          | 379/50000 [07:44<17:03:43,  1.24s/it]  1%|          | 380/50000 [07:45<17:03:34,  1.24s/it]                                                      {'loss': 389.1656, 'learning_rate': 2.848e-06, 'epoch': 0.06}
  1%|          | 380/50000 [07:45<17:03:34,  1.24s/it]  1%|          | 381/50000 [07:46<17:04:54,  1.24s/it]  1%|          | 382/50000 [07:47<17:04:26,  1.24s/it]  1%|          | 383/50000 [07:49<17:04:07,  1.24s/it]  1%|          | 384/50000 [07:50<17:03:45,  1.24s/it]  1%|          | 385/50000 [07:51<17:03:36,  1.24s/it]  1%|          | 386/50000 [07:52<17:03:13,  1.24s/it]  1%|          | 387/50000 [07:54<17:03:11,  1.24s/it]  1%|          | 388/50000 [07:55<17:03:27,  1.24s/it]  1%|          | 389/50000 [07:56<17:03:24,  1.24s/it]  1%|          | 390/50000 [07:57<17:03:01,  1.24s/it]                                                      {'loss': 318.0437, 'learning_rate': 2.928e-06, 'epoch': 0.06}
  1%|          | 390/50000 [07:57<17:03:01,  1.24s/it]  1%|          | 391/50000 [07:59<17:02:59,  1.24s/it]  1%|          | 392/50000 [08:00<17:02:57,  1.24s/it]  1%|          | 393/50000 [08:01<17:02:59,  1.24s/it]  1%|          | 394/50000 [08:02<17:02:54,  1.24s/it]  1%|          | 395/50000 [08:04<17:02:42,  1.24s/it]  1%|          | 396/50000 [08:05<17:02:35,  1.24s/it]  1%|          | 397/50000 [08:06<17:02:39,  1.24s/it]  1%|          | 398/50000 [08:07<17:02:31,  1.24s/it]  1%|          | 399/50000 [08:09<17:02:13,  1.24s/it]  1%|          | 400/50000 [08:10<17:02:47,  1.24s/it]                                                      {'loss': 476.9062, 'learning_rate': 3.0080000000000003e-06, 'epoch': 0.06}
  1%|          | 400/50000 [08:10<17:02:47,  1.24s/it]  1%|          | 401/50000 [08:11<17:03:57,  1.24s/it]  1%|          | 402/50000 [08:12<17:04:06,  1.24s/it]  1%|          | 403/50000 [08:13<17:04:20,  1.24s/it]  1%|          | 404/50000 [08:15<17:04:48,  1.24s/it]  1%|          | 405/50000 [08:16<17:04:50,  1.24s/it]  1%|          | 406/50000 [08:17<17:05:12,  1.24s/it]  1%|          | 407/50000 [08:18<17:06:17,  1.24s/it]  1%|          | 408/50000 [08:20<17:07:33,  1.24s/it]  1%|          | 409/50000 [08:21<17:07:53,  1.24s/it]  1%|          | 410/50000 [08:22<17:10:33,  1.25s/it]                                                      {'loss': 348.05, 'learning_rate': 3.0880000000000003e-06, 'epoch': 0.06}
  1%|          | 410/50000 [08:22<17:10:33,  1.25s/it]  1%|          | 411/50000 [08:23<17:10:49,  1.25s/it]  1%|          | 412/50000 [08:25<17:09:41,  1.25s/it]  1%|          | 413/50000 [08:26<17:08:16,  1.24s/it]  1%|          | 414/50000 [08:27<17:07:14,  1.24s/it]  1%|          | 415/50000 [08:28<17:06:38,  1.24s/it]  1%|          | 416/50000 [08:30<17:07:31,  1.24s/it]  1%|          | 417/50000 [08:31<17:07:53,  1.24s/it]  1%|          | 418/50000 [08:32<17:08:16,  1.24s/it]  1%|          | 419/50000 [08:33<17:07:05,  1.24s/it]  1%|          | 420/50000 [08:35<17:07:13,  1.24s/it]                                                      {'loss': 346.2063, 'learning_rate': 3.1680000000000004e-06, 'epoch': 0.07}
  1%|          | 420/50000 [08:35<17:07:13,  1.24s/it]  1%|          | 421/50000 [08:36<17:07:09,  1.24s/it]  1%|          | 422/50000 [08:37<17:07:19,  1.24s/it]  1%|          | 423/50000 [08:38<17:06:05,  1.24s/it]  1%|          | 424/50000 [08:40<17:05:37,  1.24s/it]  1%|          | 425/50000 [08:41<17:05:36,  1.24s/it]  1%|          | 426/50000 [08:42<17:05:39,  1.24s/it]  1%|          | 427/50000 [08:43<17:06:30,  1.24s/it]  1%|          | 428/50000 [08:45<17:06:24,  1.24s/it]  1%|          | 429/50000 [08:46<17:05:37,  1.24s/it]  1%|          | 430/50000 [08:47<17:04:38,  1.24s/it]                                                      {'loss': 342.875, 'learning_rate': 3.248e-06, 'epoch': 0.07}
  1%|          | 430/50000 [08:47<17:04:38,  1.24s/it]  1%|          | 431/50000 [08:48<17:04:32,  1.24s/it]  1%|          | 432/50000 [08:49<17:03:57,  1.24s/it]  1%|          | 433/50000 [08:51<17:03:59,  1.24s/it]  1%|          | 434/50000 [08:52<17:04:33,  1.24s/it]  1%|          | 435/50000 [08:53<17:04:32,  1.24s/it]  1%|          | 436/50000 [08:54<17:04:42,  1.24s/it]  1%|          | 437/50000 [08:56<17:04:50,  1.24s/it]  1%|          | 438/50000 [08:57<17:04:23,  1.24s/it]  1%|          | 439/50000 [08:58<17:03:47,  1.24s/it]  1%|          | 440/50000 [08:59<17:03:30,  1.24s/it]                                                      {'loss': 399.2625, 'learning_rate': 3.328e-06, 'epoch': 0.07}
  1%|          | 440/50000 [08:59<17:03:30,  1.24s/it]  1%|          | 441/50000 [09:01<17:04:00,  1.24s/it]  1%|          | 442/50000 [09:02<17:04:12,  1.24s/it]  1%|          | 443/50000 [09:03<17:03:57,  1.24s/it]  1%|          | 444/50000 [09:04<17:04:02,  1.24s/it]  1%|          | 445/50000 [09:06<17:03:48,  1.24s/it]  1%|          | 446/50000 [09:07<17:03:20,  1.24s/it]  1%|          | 447/50000 [09:08<17:03:09,  1.24s/it]  1%|          | 448/50000 [09:09<17:03:01,  1.24s/it]  1%|          | 449/50000 [09:11<17:03:09,  1.24s/it]  1%|          | 450/50000 [09:12<17:03:26,  1.24s/it]                                                      {'loss': 316.3156, 'learning_rate': 3.4080000000000002e-06, 'epoch': 0.07}
  1%|          | 450/50000 [09:12<17:03:26,  1.24s/it]  1%|          | 451/50000 [09:13<17:03:28,  1.24s/it]  1%|          | 452/50000 [09:14<17:04:07,  1.24s/it]  1%|          | 453/50000 [09:16<17:04:03,  1.24s/it]  1%|          | 454/50000 [09:17<17:03:36,  1.24s/it]  1%|          | 455/50000 [09:18<17:03:37,  1.24s/it]  1%|          | 456/50000 [09:19<17:03:07,  1.24s/it]  1%|          | 457/50000 [09:20<17:03:11,  1.24s/it]  1%|          | 458/50000 [09:22<17:03:01,  1.24s/it]  1%|          | 459/50000 [09:23<17:02:56,  1.24s/it]  1%|          | 460/50000 [09:24<17:02:36,  1.24s/it]                                                      {'loss': 333.9563, 'learning_rate': 3.4880000000000003e-06, 'epoch': 0.07}
  1%|          | 460/50000 [09:24<17:02:36,  1.24s/it]  1%|          | 461/50000 [09:25<17:03:30,  1.24s/it]  1%|          | 462/50000 [09:27<17:03:44,  1.24s/it]  1%|          | 463/50000 [09:28<17:03:09,  1.24s/it]  1%|          | 464/50000 [09:29<17:03:07,  1.24s/it]  1%|          | 465/50000 [09:30<17:03:13,  1.24s/it]  1%|          | 466/50000 [09:32<17:03:00,  1.24s/it]  1%|          | 467/50000 [09:33<17:02:57,  1.24s/it]  1%|          | 468/50000 [09:34<17:02:39,  1.24s/it]  1%|          | 469/50000 [09:35<17:02:40,  1.24s/it]  1%|          | 470/50000 [09:37<17:02:34,  1.24s/it]                                                      {'loss': 380.6719, 'learning_rate': 3.5680000000000004e-06, 'epoch': 0.07}
  1%|          | 470/50000 [09:37<17:02:34,  1.24s/it]  1%|          | 471/50000 [09:38<17:03:01,  1.24s/it]  1%|          | 472/50000 [09:39<17:02:38,  1.24s/it]  1%|          | 473/50000 [09:40<17:02:27,  1.24s/it]  1%|          | 474/50000 [09:42<17:02:26,  1.24s/it]  1%|          | 475/50000 [09:43<17:02:19,  1.24s/it]  1%|          | 476/50000 [09:44<17:02:13,  1.24s/it]  1%|          | 477/50000 [09:45<17:03:01,  1.24s/it]  1%|          | 478/50000 [09:47<17:03:14,  1.24s/it]  1%|          | 479/50000 [09:48<17:02:58,  1.24s/it]  1%|          | 480/50000 [09:49<17:02:40,  1.24s/it]                                                      {'loss': 312.7469, 'learning_rate': 3.6480000000000005e-06, 'epoch': 0.07}
  1%|          | 480/50000 [09:49<17:02:40,  1.24s/it]  1%|          | 481/50000 [09:50<17:02:56,  1.24s/it]  1%|          | 482/50000 [09:51<17:02:53,  1.24s/it]  1%|          | 483/50000 [09:53<17:02:41,  1.24s/it]  1%|          | 484/50000 [09:54<17:03:30,  1.24s/it]  1%|          | 485/50000 [09:55<17:03:04,  1.24s/it]  1%|          | 486/50000 [09:56<17:02:36,  1.24s/it]  1%|          | 487/50000 [09:58<17:02:39,  1.24s/it]  1%|          | 488/50000 [09:59<17:02:42,  1.24s/it]  1%|          | 489/50000 [10:00<17:02:21,  1.24s/it]  1%|          | 490/50000 [10:01<17:02:11,  1.24s/it]                                                      {'loss': 352.7656, 'learning_rate': 3.7280000000000006e-06, 'epoch': 0.08}
  1%|          | 490/50000 [10:01<17:02:11,  1.24s/it]  1%|          | 491/50000 [10:03<17:02:11,  1.24s/it]  1%|          | 492/50000 [10:04<17:03:02,  1.24s/it]  1%|          | 493/50000 [10:05<17:02:41,  1.24s/it]  1%|          | 494/50000 [10:06<17:02:20,  1.24s/it]  1%|          | 495/50000 [10:08<17:02:22,  1.24s/it]  1%|          | 496/50000 [10:09<17:02:10,  1.24s/it]  1%|          | 497/50000 [10:10<17:01:58,  1.24s/it]  1%|          | 498/50000 [10:11<17:01:42,  1.24s/it]  1%|          | 499/50000 [10:13<17:01:54,  1.24s/it]  1%|          | 500/50000 [10:14<17:02:26,  1.24s/it]                                                      {'loss': 308.2094, 'learning_rate': 3.8080000000000006e-06, 'epoch': 0.08}
  1%|          | 500/50000 [10:14<17:02:26,  1.24s/it]  1%|          | 501/50000 [10:15<17:02:12,  1.24s/it]  1%|          | 502/50000 [10:16<17:02:01,  1.24s/it]  1%|          | 503/50000 [10:17<17:01:45,  1.24s/it]  1%|          | 504/50000 [10:19<17:01:44,  1.24s/it]  1%|          | 505/50000 [10:20<17:01:23,  1.24s/it]  1%|          | 506/50000 [10:21<17:01:23,  1.24s/it]  1%|          | 507/50000 [10:22<17:01:22,  1.24s/it]  1%|          | 508/50000 [10:24<17:01:56,  1.24s/it]  1%|          | 509/50000 [10:25<17:01:36,  1.24s/it]  1%|          | 510/50000 [10:26<17:01:12,  1.24s/it]                                                      {'loss': 372.3938, 'learning_rate': 3.888e-06, 'epoch': 0.08}
  1%|          | 510/50000 [10:26<17:01:12,  1.24s/it]  1%|          | 511/50000 [10:27<17:01:31,  1.24s/it]  1%|          | 512/50000 [10:29<17:01:37,  1.24s/it]  1%|          | 513/50000 [10:30<17:01:24,  1.24s/it]  1%|          | 514/50000 [10:31<17:01:31,  1.24s/it]  1%|          | 515/50000 [10:32<17:01:32,  1.24s/it]  1%|          | 516/50000 [10:34<17:01:25,  1.24s/it]  1%|          | 517/50000 [10:35<17:01:28,  1.24s/it]  1%|          | 518/50000 [10:36<17:01:21,  1.24s/it]  1%|          | 519/50000 [10:37<17:01:32,  1.24s/it]  1%|          | 520/50000 [10:39<17:01:35,  1.24s/it]                                                      {'loss': 301.9906, 'learning_rate': 3.968e-06, 'epoch': 0.08}
  1%|          | 520/50000 [10:39<17:01:35,  1.24s/it]  1%|          | 521/50000 [10:40<17:01:47,  1.24s/it]  1%|          | 522/50000 [10:41<17:01:36,  1.24s/it]  1%|          | 523/50000 [10:42<17:01:32,  1.24s/it]  1%|          | 524/50000 [10:43<17:01:44,  1.24s/it]  1%|          | 525/50000 [10:45<17:01:40,  1.24s/it]  1%|          | 526/50000 [10:46<17:01:26,  1.24s/it]  1%|          | 527/50000 [10:47<17:01:14,  1.24s/it]  1%|          | 528/50000 [10:48<17:01:43,  1.24s/it]  1%|          | 529/50000 [10:50<17:01:27,  1.24s/it]  1%|          | 530/50000 [10:51<17:01:21,  1.24s/it]                                                      {'loss': 349.3453, 'learning_rate': 4.048e-06, 'epoch': 0.08}
  1%|          | 530/50000 [10:51<17:01:21,  1.24s/it]  1%|          | 531/50000 [10:52<17:01:33,  1.24s/it]  1%|          | 532/50000 [10:53<17:01:13,  1.24s/it]  1%|          | 533/50000 [10:55<17:01:14,  1.24s/it]  1%|          | 534/50000 [10:56<17:01:08,  1.24s/it]  1%|          | 535/50000 [10:57<17:00:56,  1.24s/it]  1%|          | 536/50000 [10:58<17:01:17,  1.24s/it]  1%|          | 537/50000 [11:00<17:01:00,  1.24s/it]  1%|          | 538/50000 [11:01<17:00:50,  1.24s/it]  1%|          | 539/50000 [11:02<17:00:44,  1.24s/it]  1%|          | 540/50000 [11:03<17:01:02,  1.24s/it]                                                      {'loss': 329.4312, 'learning_rate': 4.128e-06, 'epoch': 0.08}
  1%|          | 540/50000 [11:03<17:01:02,  1.24s/it]  1%|          | 541/50000 [11:05<17:01:39,  1.24s/it]  1%|          | 542/50000 [11:06<17:02:01,  1.24s/it]  1%|          | 543/50000 [11:07<17:01:29,  1.24s/it]  1%|          | 544/50000 [11:08<17:01:59,  1.24s/it]  1%|          | 545/50000 [11:10<17:02:04,  1.24s/it]  1%|          | 546/50000 [11:11<17:02:15,  1.24s/it]  1%|          | 547/50000 [11:12<17:01:56,  1.24s/it]  1%|          | 548/50000 [11:13<17:01:34,  1.24s/it]  1%|          | 549/50000 [11:14<17:01:29,  1.24s/it]  1%|          | 550/50000 [11:16<17:01:48,  1.24s/it]                                                      {'loss': 338.1406, 'learning_rate': 4.208e-06, 'epoch': 0.09}
  1%|          | 550/50000 [11:16<17:01:48,  1.24s/it]  1%|          | 551/50000 [11:17<17:01:35,  1.24s/it]  1%|          | 552/50000 [11:18<17:01:17,  1.24s/it]  1%|          | 553/50000 [11:19<17:01:37,  1.24s/it]  1%|          | 554/50000 [11:21<17:01:19,  1.24s/it]  1%|          | 555/50000 [11:22<17:01:41,  1.24s/it]  1%|          | 556/50000 [11:23<17:01:44,  1.24s/it]  1%|          | 557/50000 [11:24<17:02:07,  1.24s/it]  1%|          | 558/50000 [11:26<17:02:36,  1.24s/it]  1%|          | 559/50000 [11:27<17:02:14,  1.24s/it]  1%|          | 560/50000 [11:28<17:01:43,  1.24s/it]                                                      {'loss': 302.3062, 'learning_rate': 4.288e-06, 'epoch': 0.09}
  1%|          | 560/50000 [11:28<17:01:43,  1.24s/it]  1%|          | 561/50000 [11:29<17:02:53,  1.24s/it]  1%|          | 562/50000 [11:31<17:02:11,  1.24s/it]  1%|          | 563/50000 [11:32<17:01:34,  1.24s/it]  1%|          | 564/50000 [11:33<17:01:01,  1.24s/it]  1%|          | 565/50000 [11:34<17:00:44,  1.24s/it]  1%|          | 566/50000 [11:36<17:00:50,  1.24s/it]  1%|          | 567/50000 [11:37<17:00:49,  1.24s/it]  1%|          | 568/50000 [11:38<17:01:21,  1.24s/it]  1%|          | 569/50000 [11:39<17:01:10,  1.24s/it]  1%|          | 570/50000 [11:41<17:00:49,  1.24s/it]                                                      {'loss': 282.9844, 'learning_rate': 4.368e-06, 'epoch': 0.09}
  1%|          | 570/50000 [11:41<17:00:49,  1.24s/it]  1%|          | 571/50000 [11:42<17:02:11,  1.24s/it]  1%|          | 572/50000 [11:43<17:01:57,  1.24s/it]  1%|          | 573/50000 [11:44<17:02:45,  1.24s/it]  1%|          | 574/50000 [11:45<17:03:46,  1.24s/it]  1%|          | 575/50000 [11:47<17:03:41,  1.24s/it]  1%|          | 576/50000 [11:48<17:04:40,  1.24s/it]  1%|          | 577/50000 [11:49<17:03:07,  1.24s/it]  1%|          | 578/50000 [11:50<17:01:54,  1.24s/it]  1%|          | 579/50000 [11:52<17:01:16,  1.24s/it]  1%|          | 580/50000 [11:53<17:03:10,  1.24s/it]                                                      {'loss': 306.5391, 'learning_rate': 4.4480000000000004e-06, 'epoch': 0.09}
  1%|          | 580/50000 [11:53<17:03:10,  1.24s/it]  1%|          | 581/50000 [11:54<17:04:48,  1.24s/it]  1%|          | 582/50000 [11:55<17:05:31,  1.25s/it]  1%|          | 583/50000 [11:57<17:05:42,  1.25s/it]  1%|          | 584/50000 [11:58<17:06:05,  1.25s/it]  1%|          | 585/50000 [11:59<17:07:08,  1.25s/it]  1%|          | 586/50000 [12:00<17:06:51,  1.25s/it]  1%|          | 587/50000 [12:02<17:06:26,  1.25s/it]  1%|          | 588/50000 [12:03<17:06:49,  1.25s/it]  1%|          | 589/50000 [12:04<17:05:51,  1.25s/it]  1%|          | 590/50000 [12:05<17:05:41,  1.25s/it]                                                      {'loss': 408.3375, 'learning_rate': 4.5280000000000005e-06, 'epoch': 0.09}
  1%|          | 590/50000 [12:05<17:05:41,  1.25s/it]  1%|          | 591/50000 [12:07<17:05:16,  1.25s/it]  1%|          | 592/50000 [12:08<17:05:09,  1.24s/it]  1%|          | 593/50000 [12:09<17:04:51,  1.24s/it]  1%|          | 594/50000 [12:10<17:03:18,  1.24s/it]  1%|          | 595/50000 [12:12<17:03:51,  1.24s/it]  1%|          | 596/50000 [12:13<17:03:20,  1.24s/it]  1%|          | 597/50000 [12:14<17:03:14,  1.24s/it]  1%|          | 598/50000 [12:15<17:03:27,  1.24s/it]  1%|          | 599/50000 [12:17<17:04:25,  1.24s/it]  1%|          | 600/50000 [12:18<17:05:12,  1.25s/it]                                                      {'loss': 292.5063, 'learning_rate': 4.608000000000001e-06, 'epoch': 0.09}
  1%|          | 600/50000 [12:18<17:05:12,  1.25s/it]  1%|          | 601/50000 [12:19<17:06:25,  1.25s/it]  1%|          | 602/50000 [12:20<17:05:36,  1.25s/it]  1%|          | 603/50000 [12:22<17:04:55,  1.24s/it]  1%|          | 604/50000 [12:23<17:04:29,  1.24s/it]  1%|          | 605/50000 [12:24<17:04:04,  1.24s/it]  1%|          | 606/50000 [12:25<17:05:18,  1.25s/it]  1%|          | 607/50000 [12:27<17:05:03,  1.25s/it]  1%|          | 608/50000 [12:28<17:05:28,  1.25s/it]  1%|          | 609/50000 [12:29<17:05:37,  1.25s/it]  1%|          | 610/50000 [12:30<17:05:09,  1.25s/it]                                                      {'loss': 287.7047, 'learning_rate': 4.688000000000001e-06, 'epoch': 0.09}
  1%|          | 610/50000 [12:30<17:05:09,  1.25s/it]  1%|          | 611/50000 [12:32<17:05:03,  1.25s/it]  1%|          | 612/50000 [12:33<17:04:49,  1.25s/it]  1%|          | 613/50000 [12:34<17:04:40,  1.24s/it]  1%|          | 614/50000 [12:35<17:04:31,  1.24s/it]  1%|          | 615/50000 [12:37<17:04:15,  1.24s/it]  1%|          | 616/50000 [12:38<17:04:10,  1.24s/it]  1%|          | 617/50000 [12:39<17:04:37,  1.24s/it]  1%|          | 618/50000 [12:40<17:04:15,  1.24s/it]  1%|          | 619/50000 [12:41<17:04:49,  1.25s/it]  1%|          | 620/50000 [12:43<17:03:45,  1.24s/it]                                                      {'loss': 282.6969, 'learning_rate': 4.768000000000001e-06, 'epoch': 0.1}
  1%|          | 620/50000 [12:43<17:03:45,  1.24s/it]  1%|          | 621/50000 [12:44<17:03:06,  1.24s/it]  1%|          | 622/50000 [12:45<17:02:48,  1.24s/it]  1%|          | 623/50000 [12:46<17:02:07,  1.24s/it]  1%|          | 624/50000 [12:48<17:02:11,  1.24s/it]  1%|▏         | 625/50000 [12:49<17:01:58,  1.24s/it]  1%|▏         | 626/50000 [12:50<17:02:30,  1.24s/it]  1%|▏         | 627/50000 [12:51<17:01:54,  1.24s/it]  1%|▏         | 628/50000 [12:53<17:01:47,  1.24s/it]  1%|▏         | 629/50000 [12:54<17:01:40,  1.24s/it]  1%|▏         | 630/50000 [12:55<17:02:19,  1.24s/it]                                                      {'loss': 263.5828, 'learning_rate': 4.848000000000001e-06, 'epoch': 0.1}
  1%|▏         | 630/50000 [12:55<17:02:19,  1.24s/it]  1%|▏         | 631/50000 [12:56<17:02:22,  1.24s/it]  1%|▏         | 632/50000 [12:58<17:02:11,  1.24s/it]  1%|▏         | 633/50000 [12:59<17:02:04,  1.24s/it]  1%|▏         | 634/50000 [13:00<17:01:52,  1.24s/it]  1%|▏         | 635/50000 [13:01<17:01:37,  1.24s/it]  1%|▏         | 636/50000 [13:03<17:01:14,  1.24s/it]  1%|▏         | 637/50000 [13:04<17:01:30,  1.24s/it]  1%|▏         | 638/50000 [13:05<17:02:13,  1.24s/it]  1%|▏         | 639/50000 [13:06<17:02:28,  1.24s/it]  1%|▏         | 640/50000 [13:08<17:02:10,  1.24s/it]                                                      {'loss': 435.1969, 'learning_rate': 4.928000000000001e-06, 'epoch': 0.1}
  1%|▏         | 640/50000 [13:08<17:02:10,  1.24s/it]  1%|▏         | 641/50000 [13:09<17:02:30,  1.24s/it]  1%|▏         | 642/50000 [13:10<17:01:59,  1.24s/it]  1%|▏         | 643/50000 [13:11<17:02:19,  1.24s/it]  1%|▏         | 644/50000 [13:13<17:02:19,  1.24s/it]  1%|▏         | 645/50000 [13:14<17:02:25,  1.24s/it]  1%|▏         | 646/50000 [13:15<17:02:31,  1.24s/it]  1%|▏         | 647/50000 [13:16<17:02:29,  1.24s/it]  1%|▏         | 648/50000 [13:18<17:02:37,  1.24s/it]  1%|▏         | 649/50000 [13:19<17:02:38,  1.24s/it]  1%|▏         | 650/50000 [13:20<17:03:16,  1.24s/it]                                                      {'loss': 304.0, 'learning_rate': 5.008000000000001e-06, 'epoch': 0.1}
  1%|▏         | 650/50000 [13:20<17:03:16,  1.24s/it]  1%|▏         | 651/50000 [13:21<17:03:15,  1.24s/it]  1%|▏         | 652/50000 [13:22<17:03:00,  1.24s/it]  1%|▏         | 653/50000 [13:24<17:02:44,  1.24s/it]  1%|▏         | 654/50000 [13:25<17:02:51,  1.24s/it]  1%|▏         | 655/50000 [13:26<17:03:03,  1.24s/it]  1%|▏         | 656/50000 [13:27<17:02:57,  1.24s/it]  1%|▏         | 657/50000 [13:29<17:02:47,  1.24s/it]  1%|▏         | 658/50000 [13:30<17:03:16,  1.24s/it]  1%|▏         | 659/50000 [13:31<17:03:02,  1.24s/it]  1%|▏         | 660/50000 [13:32<17:02:51,  1.24s/it]                                                      {'loss': 251.1625, 'learning_rate': 5.088000000000001e-06, 'epoch': 0.1}
  1%|▏         | 660/50000 [13:32<17:02:51,  1.24s/it]  1%|▏         | 661/50000 [13:34<17:03:24,  1.24s/it]  1%|▏         | 662/50000 [13:35<17:03:17,  1.24s/it]  1%|▏         | 663/50000 [13:36<17:02:55,  1.24s/it]  1%|▏         | 664/50000 [13:37<17:02:43,  1.24s/it]  1%|▏         | 665/50000 [13:39<17:02:43,  1.24s/it]  1%|▏         | 666/50000 [13:40<17:02:49,  1.24s/it]  1%|▏         | 667/50000 [13:41<17:02:40,  1.24s/it]  1%|▏         | 668/50000 [13:42<17:02:32,  1.24s/it]  1%|▏         | 669/50000 [13:44<17:02:28,  1.24s/it]  1%|▏         | 670/50000 [13:45<17:02:48,  1.24s/it]                                                      {'loss': 258.5359, 'learning_rate': 5.168000000000001e-06, 'epoch': 0.1}
  1%|▏         | 670/50000 [13:45<17:02:48,  1.24s/it]  1%|▏         | 671/50000 [13:46<17:02:35,  1.24s/it]  1%|▏         | 672/50000 [13:47<17:02:23,  1.24s/it]  1%|▏         | 673/50000 [13:49<17:02:18,  1.24s/it]  1%|▏         | 674/50000 [13:50<17:02:15,  1.24s/it]  1%|▏         | 675/50000 [13:51<17:02:21,  1.24s/it]  1%|▏         | 676/50000 [13:52<17:02:19,  1.24s/it]  1%|▏         | 677/50000 [13:54<17:02:14,  1.24s/it]  1%|▏         | 678/50000 [13:55<17:02:13,  1.24s/it]  1%|▏         | 679/50000 [13:56<17:02:08,  1.24s/it]  1%|▏         | 680/50000 [13:57<17:02:26,  1.24s/it]                                                      {'loss': 256.2437, 'learning_rate': 5.248000000000001e-06, 'epoch': 0.11}
  1%|▏         | 680/50000 [13:57<17:02:26,  1.24s/it]  1%|▏         | 681/50000 [13:59<17:02:25,  1.24s/it]  1%|▏         | 682/50000 [14:00<17:02:07,  1.24s/it]  1%|▏         | 683/50000 [14:01<17:01:48,  1.24s/it]  1%|▏         | 684/50000 [14:02<17:01:36,  1.24s/it]  1%|▏         | 685/50000 [14:04<17:01:21,  1.24s/it]  1%|▏         | 686/50000 [14:05<17:01:17,  1.24s/it]  1%|▏         | 687/50000 [14:06<17:01:14,  1.24s/it]  1%|▏         | 688/50000 [14:07<17:01:22,  1.24s/it]  1%|▏         | 689/50000 [14:09<17:01:08,  1.24s/it]  1%|▏         | 690/50000 [14:10<17:01:01,  1.24s/it]                                                      {'loss': 286.9219, 'learning_rate': 5.328000000000001e-06, 'epoch': 0.11}
  1%|▏         | 690/50000 [14:10<17:01:01,  1.24s/it]  1%|▏         | 691/50000 [14:11<17:01:12,  1.24s/it]  1%|▏         | 692/50000 [14:12<17:01:07,  1.24s/it]  1%|▏         | 693/50000 [14:13<17:01:06,  1.24s/it]  1%|▏         | 694/50000 [14:15<17:01:05,  1.24s/it]  1%|▏         | 695/50000 [14:16<17:00:56,  1.24s/it]  1%|▏         | 696/50000 [14:17<17:00:49,  1.24s/it]  1%|▏         | 697/50000 [14:18<17:00:45,  1.24s/it]  1%|▏         | 698/50000 [14:20<17:00:42,  1.24s/it]  1%|▏         | 699/50000 [14:21<17:00:59,  1.24s/it]  1%|▏         | 700/50000 [14:22<17:01:09,  1.24s/it]                                                      {'loss': 249.2688, 'learning_rate': 5.408e-06, 'epoch': 0.11}
  1%|▏         | 700/50000 [14:22<17:01:09,  1.24s/it]  1%|▏         | 701/50000 [14:23<17:01:02,  1.24s/it]  1%|▏         | 702/50000 [14:25<17:00:53,  1.24s/it]  1%|▏         | 703/50000 [14:26<17:00:55,  1.24s/it]  1%|▏         | 704/50000 [14:27<17:00:46,  1.24s/it]  1%|▏         | 705/50000 [14:28<17:01:01,  1.24s/it]  1%|▏         | 706/50000 [14:30<17:00:53,  1.24s/it]  1%|▏         | 707/50000 [14:31<17:00:57,  1.24s/it]  1%|▏         | 708/50000 [14:32<17:00:40,  1.24s/it]  1%|▏         | 709/50000 [14:33<17:00:42,  1.24s/it]  1%|▏         | 710/50000 [14:35<17:00:42,  1.24s/it]                                                      {'loss': 260.0922, 'learning_rate': 5.488e-06, 'epoch': 0.11}
  1%|▏         | 710/50000 [14:35<17:00:42,  1.24s/it]  1%|▏         | 711/50000 [14:36<17:00:38,  1.24s/it]  1%|▏         | 712/50000 [14:37<17:00:47,  1.24s/it]  1%|▏         | 713/50000 [14:38<17:00:40,  1.24s/it]  1%|▏         | 714/50000 [14:40<17:00:40,  1.24s/it]  1%|▏         | 715/50000 [14:41<17:01:40,  1.24s/it]  1%|▏         | 716/50000 [14:42<17:01:12,  1.24s/it]  1%|▏         | 717/50000 [14:43<17:00:52,  1.24s/it]  1%|▏         | 718/50000 [14:45<17:00:43,  1.24s/it]  1%|▏         | 719/50000 [14:46<17:00:33,  1.24s/it]  1%|▏         | 720/50000 [14:47<17:00:26,  1.24s/it]                                                      {'loss': 247.8016, 'learning_rate': 5.568e-06, 'epoch': 0.11}
  1%|▏         | 720/50000 [14:47<17:00:26,  1.24s/it]  1%|▏         | 721/50000 [14:48<17:00:38,  1.24s/it]  1%|▏         | 722/50000 [14:50<17:00:24,  1.24s/it]  1%|▏         | 723/50000 [14:51<17:00:44,  1.24s/it]  1%|▏         | 724/50000 [14:52<17:00:32,  1.24s/it]  1%|▏         | 725/50000 [14:53<17:00:25,  1.24s/it]  1%|▏         | 726/50000 [14:54<17:00:23,  1.24s/it]  1%|▏         | 727/50000 [14:56<17:00:17,  1.24s/it]  1%|▏         | 728/50000 [14:57<17:00:40,  1.24s/it]  1%|▏         | 729/50000 [14:58<17:00:22,  1.24s/it]  1%|▏         | 730/50000 [14:59<17:00:08,  1.24s/it]                                                      {'loss': 273.7406, 'learning_rate': 5.648e-06, 'epoch': 0.11}
  1%|▏         | 730/50000 [14:59<17:00:08,  1.24s/it]  1%|▏         | 731/50000 [15:01<17:00:07,  1.24s/it]  1%|▏         | 732/50000 [15:02<17:00:09,  1.24s/it]  1%|▏         | 733/50000 [15:03<16:59:59,  1.24s/it]  1%|▏         | 734/50000 [15:04<17:00:00,  1.24s/it]  1%|▏         | 735/50000 [15:06<17:00:16,  1.24s/it]  1%|▏         | 736/50000 [15:07<17:00:03,  1.24s/it]  1%|▏         | 737/50000 [15:08<17:00:03,  1.24s/it]  1%|▏         | 738/50000 [15:09<17:00:01,  1.24s/it]  1%|▏         | 739/50000 [15:11<16:59:49,  1.24s/it]  1%|▏         | 740/50000 [15:12<16:59:50,  1.24s/it]                                                      {'loss': 207.7734, 'learning_rate': 5.728e-06, 'epoch': 0.11}
  1%|▏         | 740/50000 [15:12<16:59:50,  1.24s/it]  1%|▏         | 741/50000 [15:13<16:59:55,  1.24s/it]  1%|▏         | 742/50000 [15:14<16:59:43,  1.24s/it]  1%|▏         | 743/50000 [15:16<16:59:53,  1.24s/it]  1%|▏         | 744/50000 [15:17<16:59:47,  1.24s/it]  1%|▏         | 745/50000 [15:18<16:59:55,  1.24s/it]  1%|▏         | 746/50000 [15:19<16:59:59,  1.24s/it]  1%|▏         | 747/50000 [15:21<17:00:25,  1.24s/it]  1%|▏         | 748/50000 [15:22<17:00:30,  1.24s/it]  1%|▏         | 749/50000 [15:23<17:01:26,  1.24s/it]  2%|▏         | 750/50000 [15:24<17:04:26,  1.25s/it]                                                      {'loss': 247.65, 'learning_rate': 5.808e-06, 'epoch': 0.12}
  2%|▏         | 750/50000 [15:24<17:04:26,  1.25s/it]  2%|▏         | 751/50000 [15:26<17:03:53,  1.25s/it]  2%|▏         | 752/50000 [15:27<17:03:50,  1.25s/it]  2%|▏         | 753/50000 [15:28<17:04:00,  1.25s/it]  2%|▏         | 754/50000 [15:29<17:03:59,  1.25s/it]  2%|▏         | 755/50000 [15:31<17:03:26,  1.25s/it]  2%|▏         | 756/50000 [15:32<17:02:18,  1.25s/it]  2%|▏         | 757/50000 [15:33<17:01:26,  1.24s/it]  2%|▏         | 758/50000 [15:34<17:01:02,  1.24s/it]  2%|▏         | 759/50000 [15:36<17:00:38,  1.24s/it]  2%|▏         | 760/50000 [15:37<17:00:16,  1.24s/it]                                                      {'loss': 239.3063, 'learning_rate': 5.888e-06, 'epoch': 0.12}
  2%|▏         | 760/50000 [15:37<17:00:16,  1.24s/it]  2%|▏         | 761/50000 [15:38<17:01:00,  1.24s/it]  2%|▏         | 762/50000 [15:39<17:01:16,  1.24s/it]  2%|▏         | 763/50000 [15:41<17:01:07,  1.24s/it]  2%|▏         | 764/50000 [15:42<17:01:19,  1.24s/it]  2%|▏         | 765/50000 [15:43<17:00:47,  1.24s/it]  2%|▏         | 766/50000 [15:44<17:00:12,  1.24s/it]  2%|▏         | 767/50000 [15:45<16:59:42,  1.24s/it]  2%|▏         | 768/50000 [15:47<16:59:31,  1.24s/it]  2%|▏         | 769/50000 [15:48<16:59:56,  1.24s/it]  2%|▏         | 770/50000 [15:49<16:59:46,  1.24s/it]                                                      {'loss': 256.7219, 'learning_rate': 5.968e-06, 'epoch': 0.12}
  2%|▏         | 770/50000 [15:49<16:59:46,  1.24s/it]  2%|▏         | 771/50000 [15:50<17:00:22,  1.24s/it]  2%|▏         | 772/50000 [15:52<17:00:39,  1.24s/it]  2%|▏         | 773/50000 [15:53<17:00:03,  1.24s/it]  2%|▏         | 774/50000 [15:54<16:59:42,  1.24s/it]  2%|▏         | 775/50000 [15:55<16:59:34,  1.24s/it]  2%|▏         | 776/50000 [15:57<16:59:45,  1.24s/it]  2%|▏         | 777/50000 [15:58<16:59:56,  1.24s/it]  2%|▏         | 778/50000 [15:59<16:59:39,  1.24s/it]  2%|▏         | 779/50000 [16:00<16:59:35,  1.24s/it]  2%|▏         | 780/50000 [16:02<17:00:22,  1.24s/it]                                                      {'loss': 204.8344, 'learning_rate': 6.048e-06, 'epoch': 0.12}
  2%|▏         | 780/50000 [16:02<17:00:22,  1.24s/it]  2%|▏         | 781/50000 [16:03<17:00:11,  1.24s/it]  2%|▏         | 782/50000 [16:04<16:59:46,  1.24s/it]  2%|▏         | 783/50000 [16:05<16:59:13,  1.24s/it]  2%|▏         | 784/50000 [16:07<16:59:01,  1.24s/it]  2%|▏         | 785/50000 [16:08<16:58:52,  1.24s/it]  2%|▏         | 786/50000 [16:09<16:59:04,  1.24s/it]  2%|▏         | 787/50000 [16:10<16:59:26,  1.24s/it]  2%|▏         | 788/50000 [16:12<16:59:10,  1.24s/it]  2%|▏         | 789/50000 [16:13<16:58:54,  1.24s/it]  2%|▏         | 790/50000 [16:14<16:58:59,  1.24s/it]                                                      {'loss': 237.8844, 'learning_rate': 6.1280000000000005e-06, 'epoch': 0.12}
  2%|▏         | 790/50000 [16:14<16:58:59,  1.24s/it]  2%|▏         | 791/50000 [16:15<16:59:02,  1.24s/it]  2%|▏         | 792/50000 [16:17<16:59:09,  1.24s/it]  2%|▏         | 793/50000 [16:18<16:58:54,  1.24s/it]  2%|▏         | 794/50000 [16:19<16:58:47,  1.24s/it]  2%|▏         | 795/50000 [16:20<16:59:23,  1.24s/it]  2%|▏         | 796/50000 [16:22<16:58:47,  1.24s/it]  2%|▏         | 797/50000 [16:23<16:58:42,  1.24s/it]  2%|▏         | 798/50000 [16:24<16:58:47,  1.24s/it]  2%|▏         | 799/50000 [16:25<16:59:22,  1.24s/it]  2%|▏         | 800/50000 [16:26<16:59:33,  1.24s/it]                                                      {'loss': 187.3359, 'learning_rate': 6.2080000000000005e-06, 'epoch': 0.12}
  2%|▏         | 800/50000 [16:26<16:59:33,  1.24s/it]  2%|▏         | 801/50000 [16:28<16:59:29,  1.24s/it]  2%|▏         | 802/50000 [16:29<16:59:46,  1.24s/it]  2%|▏         | 803/50000 [16:30<17:00:03,  1.24s/it]  2%|▏         | 804/50000 [16:31<16:59:27,  1.24s/it]  2%|▏         | 805/50000 [16:33<17:01:08,  1.25s/it]  2%|▏         | 806/50000 [16:34<17:02:00,  1.25s/it]  2%|▏         | 807/50000 [16:35<17:02:29,  1.25s/it]  2%|▏         | 808/50000 [16:36<17:02:29,  1.25s/it]  2%|▏         | 809/50000 [16:38<17:02:59,  1.25s/it]  2%|▏         | 810/50000 [16:39<17:02:38,  1.25s/it]                                                      {'loss': 228.1984, 'learning_rate': 6.288000000000001e-06, 'epoch': 0.13}
  2%|▏         | 810/50000 [16:39<17:02:38,  1.25s/it]  2%|▏         | 811/50000 [16:40<17:02:36,  1.25s/it]  2%|▏         | 812/50000 [16:41<17:03:43,  1.25s/it]  2%|▏         | 813/50000 [16:43<17:05:02,  1.25s/it]  2%|▏         | 814/50000 [16:44<17:02:56,  1.25s/it]  2%|▏         | 815/50000 [16:45<17:04:59,  1.25s/it]  2%|▏         | 816/50000 [16:46<17:05:04,  1.25s/it]  2%|▏         | 817/50000 [16:48<17:05:55,  1.25s/it]  2%|▏         | 818/50000 [16:49<17:04:54,  1.25s/it]  2%|▏         | 819/50000 [16:50<17:04:30,  1.25s/it]  2%|▏         | 820/50000 [16:51<17:05:57,  1.25s/it]                                                      {'loss': 203.6656, 'learning_rate': 6.368000000000001e-06, 'epoch': 0.13}
  2%|▏         | 820/50000 [16:51<17:05:57,  1.25s/it]  2%|▏         | 821/50000 [16:53<17:06:11,  1.25s/it]  2%|▏         | 822/50000 [16:54<17:04:19,  1.25s/it]  2%|▏         | 823/50000 [16:55<17:03:00,  1.25s/it]  2%|▏         | 824/50000 [16:56<17:01:06,  1.25s/it]  2%|▏         | 825/50000 [16:58<16:59:38,  1.24s/it]  2%|▏         | 826/50000 [16:59<16:59:30,  1.24s/it]  2%|▏         | 827/50000 [17:00<16:59:37,  1.24s/it]  2%|▏         | 828/50000 [17:01<16:58:35,  1.24s/it]  2%|▏         | 829/50000 [17:03<16:58:49,  1.24s/it]  2%|▏         | 830/50000 [17:04<16:58:48,  1.24s/it]                                                      {'loss': 230.7703, 'learning_rate': 6.448000000000001e-06, 'epoch': 0.13}
  2%|▏         | 830/50000 [17:04<16:58:48,  1.24s/it]  2%|▏         | 831/50000 [17:05<16:59:57,  1.24s/it]  2%|▏         | 832/50000 [17:06<17:00:56,  1.25s/it]  2%|▏         | 833/50000 [17:08<17:02:14,  1.25s/it]  2%|▏         | 834/50000 [17:09<17:03:04,  1.25s/it]  2%|▏         | 835/50000 [17:10<17:04:33,  1.25s/it]  2%|▏         | 836/50000 [17:11<17:05:20,  1.25s/it]  2%|▏         | 837/50000 [17:13<17:06:59,  1.25s/it]  2%|▏         | 838/50000 [17:14<17:06:31,  1.25s/it]  2%|▏         | 839/50000 [17:15<17:06:47,  1.25s/it]  2%|▏         | 840/50000 [17:16<17:06:39,  1.25s/it]                                                      {'loss': 179.5344, 'learning_rate': 6.528000000000001e-06, 'epoch': 0.13}
  2%|▏         | 840/50000 [17:16<17:06:39,  1.25s/it]  2%|▏         | 841/50000 [17:18<17:06:16,  1.25s/it]  2%|▏         | 842/50000 [17:19<17:06:09,  1.25s/it]  2%|▏         | 843/50000 [17:20<17:05:20,  1.25s/it]  2%|▏         | 844/50000 [17:21<17:03:43,  1.25s/it]  2%|▏         | 845/50000 [17:23<17:03:12,  1.25s/it]  2%|▏         | 846/50000 [17:24<17:01:28,  1.25s/it]  2%|▏         | 847/50000 [17:25<17:00:32,  1.25s/it]  2%|▏         | 848/50000 [17:26<17:00:13,  1.25s/it]  2%|▏         | 849/50000 [17:28<16:59:40,  1.24s/it]  2%|▏         | 850/50000 [17:29<17:00:22,  1.25s/it]                                                      {'loss': 244.9328, 'learning_rate': 6.608000000000001e-06, 'epoch': 0.13}
  2%|▏         | 850/50000 [17:29<17:00:22,  1.25s/it]  2%|▏         | 851/50000 [17:30<17:00:55,  1.25s/it]  2%|▏         | 852/50000 [17:31<17:00:07,  1.25s/it]  2%|▏         | 853/50000 [17:33<17:00:03,  1.25s/it]  2%|▏         | 854/50000 [17:34<16:59:49,  1.25s/it]  2%|▏         | 855/50000 [17:35<16:59:17,  1.24s/it]  2%|▏         | 856/50000 [17:36<16:59:11,  1.24s/it]  2%|▏         | 857/50000 [17:38<16:59:25,  1.24s/it]  2%|▏         | 858/50000 [17:39<16:59:58,  1.25s/it]  2%|▏         | 859/50000 [17:40<16:59:42,  1.25s/it]  2%|▏         | 860/50000 [17:41<17:00:05,  1.25s/it]                                                      {'loss': 252.3656, 'learning_rate': 6.688e-06, 'epoch': 0.13}
  2%|▏         | 860/50000 [17:41<17:00:05,  1.25s/it]  2%|▏         | 861/50000 [17:43<17:01:00,  1.25s/it]  2%|▏         | 862/50000 [17:44<16:59:40,  1.25s/it]  2%|▏         | 863/50000 [17:45<16:59:57,  1.25s/it]  2%|▏         | 864/50000 [17:46<16:59:53,  1.25s/it]  2%|▏         | 865/50000 [17:48<16:59:43,  1.25s/it]  2%|▏         | 866/50000 [17:49<17:00:00,  1.25s/it]  2%|▏         | 867/50000 [17:50<16:59:23,  1.24s/it]  2%|▏         | 868/50000 [17:51<16:59:02,  1.24s/it]  2%|▏         | 869/50000 [17:53<16:59:35,  1.25s/it]  2%|▏         | 870/50000 [17:54<16:58:54,  1.24s/it]                                                      {'loss': 189.1188, 'learning_rate': 6.768e-06, 'epoch': 0.13}
  2%|▏         | 870/50000 [17:54<16:58:54,  1.24s/it]  2%|▏         | 871/50000 [17:55<16:58:40,  1.24s/it]  2%|▏         | 872/50000 [17:56<16:58:08,  1.24s/it]  2%|▏         | 873/50000 [17:58<16:58:19,  1.24s/it]  2%|▏         | 874/50000 [17:59<16:58:43,  1.24s/it]  2%|▏         | 875/50000 [18:00<16:57:47,  1.24s/it]  2%|▏         | 876/50000 [18:01<16:58:00,  1.24s/it]  2%|▏         | 877/50000 [18:02<16:57:37,  1.24s/it]  2%|▏         | 878/50000 [18:04<16:57:21,  1.24s/it]  2%|▏         | 879/50000 [18:05<16:57:53,  1.24s/it]  2%|▏         | 880/50000 [18:06<16:57:07,  1.24s/it]                                                      {'loss': 224.2328, 'learning_rate': 6.848e-06, 'epoch': 0.14}
  2%|▏         | 880/50000 [18:06<16:57:07,  1.24s/it]  2%|▏         | 881/50000 [18:07<16:57:11,  1.24s/it]  2%|▏         | 882/50000 [18:09<16:57:26,  1.24s/it]  2%|▏         | 883/50000 [18:10<16:56:59,  1.24s/it]  2%|▏         | 884/50000 [18:11<16:56:16,  1.24s/it]  2%|▏         | 885/50000 [18:12<16:56:17,  1.24s/it]  2%|▏         | 886/50000 [18:14<16:56:43,  1.24s/it]  2%|▏         | 887/50000 [18:15<16:57:17,  1.24s/it]  2%|▏         | 888/50000 [18:16<16:56:54,  1.24s/it]  2%|▏         | 889/50000 [18:17<16:57:15,  1.24s/it]  2%|▏         | 890/50000 [18:19<16:57:14,  1.24s/it]                                                      {'loss': 180.5734, 'learning_rate': 6.928e-06, 'epoch': 0.14}
  2%|▏         | 890/50000 [18:19<16:57:14,  1.24s/it]  2%|▏         | 891/50000 [18:20<16:57:19,  1.24s/it]  2%|▏         | 892/50000 [18:21<16:57:26,  1.24s/it]  2%|▏         | 893/50000 [18:22<16:57:03,  1.24s/it]  2%|▏         | 894/50000 [18:24<16:56:36,  1.24s/it]  2%|▏         | 895/50000 [18:25<16:56:25,  1.24s/it]  2%|▏         | 896/50000 [18:26<16:56:57,  1.24s/it]  2%|▏         | 897/50000 [18:27<16:56:40,  1.24s/it]  2%|▏         | 898/50000 [18:29<16:56:31,  1.24s/it]  2%|▏         | 899/50000 [18:30<16:56:45,  1.24s/it]  2%|▏         | 900/50000 [18:31<16:56:23,  1.24s/it]                                                      {'loss': 187.6828, 'learning_rate': 7.0080000000000005e-06, 'epoch': 0.14}
  2%|▏         | 900/50000 [18:31<16:56:23,  1.24s/it]  2%|▏         | 901/50000 [18:32<16:56:20,  1.24s/it]  2%|▏         | 902/50000 [18:34<16:55:55,  1.24s/it]  2%|▏         | 903/50000 [18:35<16:55:45,  1.24s/it]  2%|▏         | 904/50000 [18:36<16:55:38,  1.24s/it]  2%|▏         | 905/50000 [18:37<16:55:39,  1.24s/it]  2%|▏         | 906/50000 [18:39<16:55:41,  1.24s/it]  2%|▏         | 907/50000 [18:40<16:55:42,  1.24s/it]  2%|▏         | 908/50000 [18:41<16:55:24,  1.24s/it]  2%|▏         | 909/50000 [18:42<16:55:24,  1.24s/it]  2%|▏         | 910/50000 [18:43<16:55:29,  1.24s/it]                                                      {'loss': 189.3141, 'learning_rate': 7.088000000000001e-06, 'epoch': 0.14}
  2%|▏         | 910/50000 [18:43<16:55:29,  1.24s/it]  2%|▏         | 911/50000 [18:45<16:55:33,  1.24s/it]  2%|▏         | 912/50000 [18:46<16:55:20,  1.24s/it]  2%|▏         | 913/50000 [18:47<16:56:06,  1.24s/it]  2%|▏         | 914/50000 [18:48<16:55:53,  1.24s/it]  2%|▏         | 915/50000 [18:50<16:55:42,  1.24s/it]  2%|▏         | 916/50000 [18:51<16:55:34,  1.24s/it]  2%|▏         | 917/50000 [18:52<16:55:40,  1.24s/it]  2%|▏         | 918/50000 [18:53<16:55:46,  1.24s/it]  2%|▏         | 919/50000 [18:55<16:55:48,  1.24s/it]  2%|▏         | 920/50000 [18:56<16:55:57,  1.24s/it]                                                      {'loss': 184.5266, 'learning_rate': 7.168000000000001e-06, 'epoch': 0.14}
  2%|▏         | 920/50000 [18:56<16:55:57,  1.24s/it]  2%|▏         | 921/50000 [18:57<16:56:10,  1.24s/it]  2%|▏         | 922/50000 [18:58<16:56:07,  1.24s/it]  2%|▏         | 923/50000 [19:00<16:56:22,  1.24s/it]  2%|▏         | 924/50000 [19:01<16:56:17,  1.24s/it]  2%|▏         | 925/50000 [19:02<16:56:43,  1.24s/it]  2%|▏         | 926/50000 [19:03<16:57:00,  1.24s/it]  2%|▏         | 927/50000 [19:05<16:57:20,  1.24s/it]  2%|▏         | 928/50000 [19:06<16:57:44,  1.24s/it]  2%|▏         | 929/50000 [19:07<16:58:36,  1.25s/it]  2%|▏         | 930/50000 [19:08<17:01:10,  1.25s/it]                                                      {'loss': 213.8938, 'learning_rate': 7.248000000000001e-06, 'epoch': 0.14}
  2%|▏         | 930/50000 [19:08<17:01:10,  1.25s/it]  2%|▏         | 931/50000 [19:10<17:01:23,  1.25s/it]  2%|▏         | 932/50000 [19:11<17:00:59,  1.25s/it]  2%|▏         | 933/50000 [19:12<16:59:52,  1.25s/it]  2%|▏         | 934/50000 [19:13<16:59:20,  1.25s/it]  2%|▏         | 935/50000 [19:15<16:58:42,  1.25s/it]  2%|▏         | 936/50000 [19:16<16:58:13,  1.25s/it]  2%|▏         | 937/50000 [19:17<16:58:30,  1.25s/it]  2%|▏         | 938/50000 [19:18<16:58:41,  1.25s/it]  2%|▏         | 939/50000 [19:20<16:59:25,  1.25s/it]  2%|▏         | 940/50000 [19:21<16:59:07,  1.25s/it]                                                      {'loss': 196.5344, 'learning_rate': 7.328000000000001e-06, 'epoch': 0.15}
  2%|▏         | 940/50000 [19:21<16:59:07,  1.25s/it]  2%|▏         | 941/50000 [19:22<16:59:10,  1.25s/it]  2%|▏         | 942/50000 [19:23<16:58:29,  1.25s/it]  2%|▏         | 943/50000 [19:25<16:58:14,  1.25s/it]  2%|▏         | 944/50000 [19:26<16:58:19,  1.25s/it]  2%|▏         | 945/50000 [19:27<16:58:20,  1.25s/it]  2%|▏         | 946/50000 [19:28<16:58:39,  1.25s/it]  2%|▏         | 947/50000 [19:30<16:58:29,  1.25s/it]  2%|▏         | 948/50000 [19:31<16:58:22,  1.25s/it]  2%|▏         | 949/50000 [19:32<16:57:58,  1.25s/it]  2%|▏         | 950/50000 [19:33<16:57:37,  1.24s/it]                                                      {'loss': 198.0875, 'learning_rate': 7.408000000000001e-06, 'epoch': 0.15}
  2%|▏         | 950/50000 [19:33<16:57:37,  1.24s/it]  2%|▏         | 951/50000 [19:35<16:57:48,  1.25s/it]  2%|▏         | 952/50000 [19:36<16:57:25,  1.24s/it]  2%|▏         | 953/50000 [19:37<16:57:06,  1.24s/it]  2%|▏         | 954/50000 [19:38<16:57:13,  1.24s/it]  2%|▏         | 955/50000 [19:39<16:57:10,  1.24s/it]  2%|▏         | 956/50000 [19:41<16:57:23,  1.24s/it]  2%|▏         | 957/50000 [19:42<16:57:34,  1.24s/it]  2%|▏         | 958/50000 [19:43<16:57:26,  1.24s/it]  2%|▏         | 959/50000 [19:44<16:57:26,  1.24s/it][2023-07-03 12:19:55,972] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, but hysteresis is 2. Reducing hysteresis to 1
  2%|▏         | 960/50000 [19:45<15:55:30,  1.17s/it]                                                      {'loss': 193.6141, 'learning_rate': 7.48e-06, 'epoch': 0.15}
  2%|▏         | 960/50000 [19:45<15:55:30,  1.17s/it]  2%|▏         | 961/50000 [19:47<16:13:53,  1.19s/it]  2%|▏         | 962/50000 [19:48<16:26:36,  1.21s/it]  2%|▏         | 963/50000 [19:49<16:35:15,  1.22s/it][2023-07-03 12:20:00,697] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  2%|▏         | 964/50000 [19:50<15:40:36,  1.15s/it]  2%|▏         | 965/50000 [19:51<16:03:10,  1.18s/it]  2%|▏         | 966/50000 [19:53<16:19:26,  1.20s/it]  2%|▏         | 967/50000 [19:54<16:30:36,  1.21s/it]  2%|▏         | 968/50000 [19:55<16:38:22,  1.22s/it]  2%|▏         | 969/50000 [19:56<16:43:49,  1.23s/it]  2%|▏         | 970/50000 [19:58<16:48:17,  1.23s/it]                                                      {'loss': 188.9, 'learning_rate': 7.552000000000001e-06, 'epoch': 0.15}
  2%|▏         | 970/50000 [19:58<16:48:17,  1.23s/it]  2%|▏         | 971/50000 [19:59<16:51:44,  1.24s/it]  2%|▏         | 972/50000 [20:00<16:54:34,  1.24s/it]  2%|▏         | 973/50000 [20:01<16:55:41,  1.24s/it]  2%|▏         | 974/50000 [20:03<16:56:42,  1.24s/it]  2%|▏         | 975/50000 [20:04<16:58:01,  1.25s/it]  2%|▏         | 976/50000 [20:05<16:57:53,  1.25s/it]  2%|▏         | 977/50000 [20:06<16:58:05,  1.25s/it]  2%|▏         | 978/50000 [20:08<16:58:08,  1.25s/it]  2%|▏         | 979/50000 [20:09<16:57:53,  1.25s/it]  2%|▏         | 980/50000 [20:10<16:57:54,  1.25s/it]                                                      {'loss': 203.4469, 'learning_rate': 7.632e-06, 'epoch': 0.15}
  2%|▏         | 980/50000 [20:10<16:57:54,  1.25s/it]  2%|▏         | 981/50000 [20:11<16:57:46,  1.25s/it]  2%|▏         | 982/50000 [20:13<16:57:28,  1.25s/it]  2%|▏         | 983/50000 [20:14<16:57:04,  1.24s/it]  2%|▏         | 984/50000 [20:15<16:56:59,  1.24s/it]  2%|▏         | 985/50000 [20:16<16:57:36,  1.25s/it]  2%|▏         | 986/50000 [20:18<16:57:37,  1.25s/it]  2%|▏         | 987/50000 [20:19<16:58:09,  1.25s/it]  2%|▏         | 988/50000 [20:20<16:57:54,  1.25s/it][2023-07-03 12:20:31,595] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
  2%|▏         | 989/50000 [20:21<15:56:28,  1.17s/it]  2%|▏         | 990/50000 [20:22<16:14:23,  1.19s/it]                                                      {'loss': 195.6094, 'learning_rate': 7.704000000000001e-06, 'epoch': 0.15}
  2%|▏         | 990/50000 [20:22<16:14:23,  1.19s/it]  2%|▏         | 991/50000 [20:24<16:27:41,  1.21s/it]  2%|▏         | 992/50000 [20:25<16:36:17,  1.22s/it]  2%|▏         | 993/50000 [20:26<16:41:52,  1.23s/it]  2%|▏         | 994/50000 [20:27<16:49:51,  1.24s/it]  2%|▏         | 995/50000 [20:29<16:51:57,  1.24s/it]  2%|▏         | 996/50000 [20:30<16:54:50,  1.24s/it]  2%|▏         | 997/50000 [20:31<16:55:56,  1.24s/it]  2%|▏         | 998/50000 [20:32<16:57:52,  1.25s/it][2023-07-03 12:20:43,833] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  2%|▏         | 999/50000 [20:33<15:58:46,  1.17s/it]  2%|▏         | 1000/50000 [20:35<16:17:45,  1.20s/it]                                                       {'loss': 203.6172, 'learning_rate': 7.776e-06, 'epoch': 0.16}
  2%|▏         | 1000/50000 [20:35<16:17:45,  1.20s/it][INFO|trainer.py:3129] 2023-07-03 12:20:45,095 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 12:20:45,095 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 12:20:45,095 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.45it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.15it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.73it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.54it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A                                                       
                                             [A{'eval_loss': 195.875, 'eval_accuracy': 0.38950775252145114, 'eval_runtime': 3.1498, 'eval_samples_per_second': 8.255, 'eval_steps_per_second': 2.222, 'epoch': 0.16}
  2%|▏         | 1000/50000 [20:38<16:17:45,  1.20s/it]
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 12:20:48,246 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000
[INFO|trainer.py:2880] 2023-07-03 12:20:48,261 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 12:20:58,880 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 12:20:58,880 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/special_tokens_map.json
[2023-07-03 12:20:58,883] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 12:20:58,907] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2023-07-03 12:20:58,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2023-07-03 12:21:09,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2023-07-03 12:21:09,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 12:21:29,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 12:21:29,182] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 12:21:29,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 12:21:29,317 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-50000] due to args.save_total_limit
  2%|▏         | 1001/50000 [21:29<235:24:23, 17.30s/it]  2%|▏         | 1002/50000 [21:31<169:52:45, 12.48s/it]  2%|▏         | 1003/50000 [21:32<123:59:56,  9.11s/it]  2%|▏         | 1004/50000 [21:33<91:53:30,  6.75s/it]   2%|▏         | 1005/50000 [21:34<69:24:19,  5.10s/it]  2%|▏         | 1006/50000 [21:36<53:40:05,  3.94s/it]  2%|▏         | 1007/50000 [21:37<42:38:54,  3.13s/it]  2%|▏         | 1008/50000 [21:38<34:56:30,  2.57s/it]  2%|▏         | 1009/50000 [21:39<29:32:46,  2.17s/it]  2%|▏         | 1010/50000 [21:41<25:46:48,  1.89s/it]                                                       {'loss': 163.7312, 'learning_rate': 7.856e-06, 'epoch': 0.16}
  2%|▏         | 1010/50000 [21:41<25:46:48,  1.89s/it]  2%|▏         | 1011/50000 [21:42<23:08:46,  1.70s/it]  2%|▏         | 1012/50000 [21:43<21:17:50,  1.57s/it][2023-07-03 12:21:54,655] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  2%|▏         | 1013/50000 [21:44<18:58:30,  1.39s/it]  2%|▏         | 1014/50000 [21:45<18:21:32,  1.35s/it]  2%|▏         | 1015/50000 [21:47<17:55:23,  1.32s/it]  2%|▏         | 1016/50000 [21:48<17:37:02,  1.29s/it]  2%|▏         | 1017/50000 [21:49<17:24:23,  1.28s/it]  2%|▏         | 1018/50000 [21:50<17:15:33,  1.27s/it]  2%|▏         | 1019/50000 [21:52<17:09:30,  1.26s/it]  2%|▏         | 1020/50000 [21:53<17:05:07,  1.26s/it]                                                       {'loss': 178.4656, 'learning_rate': 7.928e-06, 'epoch': 0.16}
  2%|▏         | 1020/50000 [21:53<17:05:07,  1.26s/it]  2%|▏         | 1021/50000 [21:54<17:01:46,  1.25s/it]  2%|▏         | 1022/50000 [21:55<16:59:42,  1.25s/it]  2%|▏         | 1023/50000 [21:57<16:58:06,  1.25s/it]  2%|▏         | 1024/50000 [21:58<16:57:03,  1.25s/it]  2%|▏         | 1025/50000 [21:59<16:56:23,  1.25s/it]  2%|▏         | 1026/50000 [22:00<16:55:59,  1.24s/it]  2%|▏         | 1027/50000 [22:02<16:55:45,  1.24s/it]  2%|▏         | 1028/50000 [22:03<16:55:48,  1.24s/it]  2%|▏         | 1029/50000 [22:04<16:55:07,  1.24s/it]  2%|▏         | 1030/50000 [22:05<16:55:26,  1.24s/it]                                                       {'loss': 188.8422, 'learning_rate': 8.008e-06, 'epoch': 0.16}
  2%|▏         | 1030/50000 [22:05<16:55:26,  1.24s/it]  2%|▏         | 1031/50000 [22:07<16:55:45,  1.24s/it]  2%|▏         | 1032/50000 [22:08<16:56:17,  1.25s/it]  2%|▏         | 1033/50000 [22:09<16:55:56,  1.24s/it]  2%|▏         | 1034/50000 [22:10<16:56:04,  1.25s/it]  2%|▏         | 1035/50000 [22:12<16:55:54,  1.24s/it]  2%|▏         | 1036/50000 [22:13<16:56:30,  1.25s/it]  2%|▏         | 1037/50000 [22:14<16:56:40,  1.25s/it]  2%|▏         | 1038/50000 [22:15<16:57:26,  1.25s/it]  2%|▏         | 1039/50000 [22:16<16:57:33,  1.25s/it]  2%|▏         | 1040/50000 [22:18<16:58:00,  1.25s/it]                                                       {'loss': 150.7859, 'learning_rate': 8.088e-06, 'epoch': 0.16}
  2%|▏         | 1040/50000 [22:18<16:58:00,  1.25s/it]  2%|▏         | 1041/50000 [22:19<16:58:21,  1.25s/it]  2%|▏         | 1042/50000 [22:20<16:58:13,  1.25s/it]  2%|▏         | 1043/50000 [22:21<16:57:41,  1.25s/it]  2%|▏         | 1044/50000 [22:23<16:57:00,  1.25s/it]  2%|▏         | 1045/50000 [22:24<16:57:11,  1.25s/it]  2%|▏         | 1046/50000 [22:25<16:57:20,  1.25s/it]  2%|▏         | 1047/50000 [22:26<16:58:17,  1.25s/it]  2%|▏         | 1048/50000 [22:28<16:57:37,  1.25s/it]  2%|▏         | 1049/50000 [22:29<16:57:28,  1.25s/it]  2%|▏         | 1050/50000 [22:30<16:57:40,  1.25s/it]                                                       {'loss': 184.1344, 'learning_rate': 8.168e-06, 'epoch': 0.16}
  2%|▏         | 1050/50000 [22:30<16:57:40,  1.25s/it]  2%|▏         | 1051/50000 [22:31<16:57:16,  1.25s/it]  2%|▏         | 1052/50000 [22:33<16:56:45,  1.25s/it]  2%|▏         | 1053/50000 [22:34<16:56:16,  1.25s/it]  2%|▏         | 1054/50000 [22:35<16:55:46,  1.25s/it]  2%|▏         | 1055/50000 [22:36<16:55:46,  1.25s/it]  2%|▏         | 1056/50000 [22:38<16:55:54,  1.25s/it]  2%|▏         | 1057/50000 [22:39<16:55:32,  1.24s/it]  2%|▏         | 1058/50000 [22:40<16:54:55,  1.24s/it]  2%|▏         | 1059/50000 [22:41<16:55:25,  1.24s/it]  2%|▏         | 1060/50000 [22:43<16:55:36,  1.25s/it]                                                       {'loss': 178.4516, 'learning_rate': 8.248e-06, 'epoch': 0.16}
  2%|▏         | 1060/50000 [22:43<16:55:36,  1.25s/it]  2%|▏         | 1061/50000 [22:44<16:56:18,  1.25s/it]  2%|▏         | 1062/50000 [22:45<16:55:46,  1.25s/it]  2%|▏         | 1063/50000 [22:46<16:55:44,  1.25s/it]  2%|▏         | 1064/50000 [22:48<16:55:29,  1.25s/it]  2%|▏         | 1065/50000 [22:49<16:55:06,  1.24s/it]  2%|▏         | 1066/50000 [22:50<16:54:59,  1.24s/it]  2%|▏         | 1067/50000 [22:51<16:54:34,  1.24s/it]  2%|▏         | 1068/50000 [22:53<16:54:50,  1.24s/it]  2%|▏         | 1069/50000 [22:54<16:54:27,  1.24s/it]  2%|▏         | 1070/50000 [22:55<16:54:13,  1.24s/it]                                                       {'loss': 201.3445, 'learning_rate': 8.328e-06, 'epoch': 0.17}
  2%|▏         | 1070/50000 [22:55<16:54:13,  1.24s/it]  2%|▏         | 1071/50000 [22:56<16:54:07,  1.24s/it]  2%|▏         | 1072/50000 [22:58<16:53:52,  1.24s/it]  2%|▏         | 1073/50000 [22:59<16:53:53,  1.24s/it]  2%|▏         | 1074/50000 [23:00<16:53:34,  1.24s/it]  2%|▏         | 1075/50000 [23:01<16:53:47,  1.24s/it]  2%|▏         | 1076/50000 [23:03<16:53:42,  1.24s/it]  2%|▏         | 1077/50000 [23:04<16:53:42,  1.24s/it]  2%|▏         | 1078/50000 [23:05<16:54:39,  1.24s/it]  2%|▏         | 1079/50000 [23:06<16:54:54,  1.24s/it]  2%|▏         | 1080/50000 [23:08<16:54:15,  1.24s/it]                                                       {'loss': 149.7953, 'learning_rate': 8.408e-06, 'epoch': 0.17}
  2%|▏         | 1080/50000 [23:08<16:54:15,  1.24s/it]  2%|▏         | 1081/50000 [23:09<16:55:11,  1.25s/it]  2%|▏         | 1082/50000 [23:10<16:54:50,  1.24s/it]  2%|▏         | 1083/50000 [23:11<16:54:55,  1.24s/it]  2%|▏         | 1084/50000 [23:13<16:56:05,  1.25s/it]  2%|▏         | 1085/50000 [23:14<16:56:52,  1.25s/it]  2%|▏         | 1086/50000 [23:15<16:57:20,  1.25s/it]  2%|▏         | 1087/50000 [23:16<16:58:14,  1.25s/it]  2%|▏         | 1088/50000 [23:18<16:59:07,  1.25s/it]  2%|▏         | 1089/50000 [23:19<16:58:14,  1.25s/it]  2%|▏         | 1090/50000 [23:20<16:58:50,  1.25s/it]                                                       {'loss': 160.2906, 'learning_rate': 8.488e-06, 'epoch': 0.17}
  2%|▏         | 1090/50000 [23:20<16:58:50,  1.25s/it]  2%|▏         | 1091/50000 [23:21<16:58:17,  1.25s/it]  2%|▏         | 1092/50000 [23:23<16:57:59,  1.25s/it]  2%|▏         | 1093/50000 [23:24<16:57:40,  1.25s/it]  2%|▏         | 1094/50000 [23:25<16:56:57,  1.25s/it]  2%|▏         | 1095/50000 [23:26<16:55:45,  1.25s/it]  2%|▏         | 1096/50000 [23:28<16:55:05,  1.25s/it]  2%|▏         | 1097/50000 [23:29<16:55:00,  1.25s/it]  2%|▏         | 1098/50000 [23:30<16:54:56,  1.25s/it]  2%|▏         | 1099/50000 [23:31<16:54:34,  1.24s/it]  2%|▏         | 1100/50000 [23:32<16:54:23,  1.24s/it]                                                       {'loss': 170.5281, 'learning_rate': 8.568e-06, 'epoch': 0.17}
  2%|▏         | 1100/50000 [23:32<16:54:23,  1.24s/it]  2%|▏         | 1101/50000 [23:34<16:54:55,  1.25s/it]  2%|▏         | 1102/50000 [23:35<16:54:16,  1.24s/it]  2%|▏         | 1103/50000 [23:36<16:54:01,  1.24s/it]  2%|▏         | 1104/50000 [23:37<16:53:45,  1.24s/it]  2%|▏         | 1105/50000 [23:39<16:53:50,  1.24s/it]  2%|▏         | 1106/50000 [23:40<16:53:25,  1.24s/it]  2%|▏         | 1107/50000 [23:41<16:53:18,  1.24s/it]  2%|▏         | 1108/50000 [23:42<16:53:20,  1.24s/it]  2%|▏         | 1109/50000 [23:44<16:53:13,  1.24s/it]  2%|▏         | 1110/50000 [23:45<16:53:28,  1.24s/it]                                                       {'loss': 164.5469, 'learning_rate': 8.648000000000001e-06, 'epoch': 0.17}
  2%|▏         | 1110/50000 [23:45<16:53:28,  1.24s/it]  2%|▏         | 1111/50000 [23:46<16:53:27,  1.24s/it]  2%|▏         | 1112/50000 [23:47<16:53:32,  1.24s/it]  2%|▏         | 1113/50000 [23:49<16:53:33,  1.24s/it]  2%|▏         | 1114/50000 [23:50<16:53:20,  1.24s/it]  2%|▏         | 1115/50000 [23:51<16:53:07,  1.24s/it]  2%|▏         | 1116/50000 [23:52<16:53:27,  1.24s/it]  2%|▏         | 1117/50000 [23:54<16:53:08,  1.24s/it]  2%|▏         | 1118/50000 [23:55<16:53:14,  1.24s/it]  2%|▏         | 1119/50000 [23:56<16:52:55,  1.24s/it]  2%|▏         | 1120/50000 [23:57<16:52:48,  1.24s/it]                                                       {'loss': 144.3078, 'learning_rate': 8.728e-06, 'epoch': 0.17}
  2%|▏         | 1120/50000 [23:57<16:52:48,  1.24s/it]  2%|▏         | 1121/50000 [23:59<16:53:13,  1.24s/it]  2%|▏         | 1122/50000 [24:00<16:53:05,  1.24s/it]  2%|▏         | 1123/50000 [24:01<16:52:57,  1.24s/it]  2%|▏         | 1124/50000 [24:02<16:52:52,  1.24s/it]  2%|▏         | 1125/50000 [24:04<16:52:40,  1.24s/it]  2%|▏         | 1126/50000 [24:05<16:52:39,  1.24s/it]  2%|▏         | 1127/50000 [24:06<16:52:42,  1.24s/it]  2%|▏         | 1128/50000 [24:07<16:53:40,  1.24s/it]  2%|▏         | 1129/50000 [24:09<16:53:15,  1.24s/it]  2%|▏         | 1130/50000 [24:10<16:52:59,  1.24s/it]                                                       {'loss': 159.7484, 'learning_rate': 8.808000000000001e-06, 'epoch': 0.18}
  2%|▏         | 1130/50000 [24:10<16:52:59,  1.24s/it]  2%|▏         | 1131/50000 [24:11<16:53:54,  1.24s/it]  2%|▏         | 1132/50000 [24:12<16:53:22,  1.24s/it]  2%|▏         | 1133/50000 [24:14<16:53:00,  1.24s/it]  2%|▏         | 1134/50000 [24:15<16:53:25,  1.24s/it]  2%|▏         | 1135/50000 [24:16<16:53:04,  1.24s/it]  2%|▏         | 1136/50000 [24:17<16:52:57,  1.24s/it]  2%|▏         | 1137/50000 [24:19<16:52:43,  1.24s/it]  2%|▏         | 1138/50000 [24:20<16:52:53,  1.24s/it]  2%|▏         | 1139/50000 [24:21<16:52:39,  1.24s/it]  2%|▏         | 1140/50000 [24:22<16:52:29,  1.24s/it]                                                       {'loss': 166.7047, 'learning_rate': 8.888e-06, 'epoch': 0.18}
  2%|▏         | 1140/50000 [24:22<16:52:29,  1.24s/it]  2%|▏         | 1141/50000 [24:23<16:52:42,  1.24s/it]  2%|▏         | 1142/50000 [24:25<16:52:35,  1.24s/it]  2%|▏         | 1143/50000 [24:26<16:52:35,  1.24s/it]  2%|▏         | 1144/50000 [24:27<16:52:25,  1.24s/it]  2%|▏         | 1145/50000 [24:28<16:52:35,  1.24s/it]  2%|▏         | 1146/50000 [24:30<16:52:14,  1.24s/it]  2%|▏         | 1147/50000 [24:31<16:52:20,  1.24s/it]  2%|▏         | 1148/50000 [24:32<16:52:36,  1.24s/it]  2%|▏         | 1149/50000 [24:33<16:52:24,  1.24s/it]  2%|▏         | 1150/50000 [24:35<16:52:16,  1.24s/it]                                                       {'loss': 190.6937, 'learning_rate': 8.968000000000001e-06, 'epoch': 0.18}
  2%|▏         | 1150/50000 [24:35<16:52:16,  1.24s/it]  2%|▏         | 1151/50000 [24:36<16:52:49,  1.24s/it]  2%|▏         | 1152/50000 [24:37<16:52:52,  1.24s/it]  2%|▏         | 1153/50000 [24:38<16:52:35,  1.24s/it]  2%|▏         | 1154/50000 [24:40<16:52:20,  1.24s/it]  2%|▏         | 1155/50000 [24:41<16:52:21,  1.24s/it]  2%|▏         | 1156/50000 [24:42<16:52:25,  1.24s/it]  2%|▏         | 1157/50000 [24:43<16:52:17,  1.24s/it]  2%|▏         | 1158/50000 [24:45<16:52:21,  1.24s/it]  2%|▏         | 1159/50000 [24:46<16:52:19,  1.24s/it]  2%|▏         | 1160/50000 [24:47<16:52:13,  1.24s/it]                                                       {'loss': 217.2656, 'learning_rate': 9.048e-06, 'epoch': 0.18}
  2%|▏         | 1160/50000 [24:47<16:52:13,  1.24s/it]  2%|▏         | 1161/50000 [24:48<16:52:14,  1.24s/it]  2%|▏         | 1162/50000 [24:50<16:51:54,  1.24s/it]  2%|▏         | 1163/50000 [24:51<16:51:55,  1.24s/it]  2%|▏         | 1164/50000 [24:52<16:51:46,  1.24s/it]  2%|▏         | 1165/50000 [24:53<16:51:40,  1.24s/it]  2%|▏         | 1166/50000 [24:55<16:51:29,  1.24s/it]  2%|▏         | 1167/50000 [24:56<16:51:47,  1.24s/it]  2%|▏         | 1168/50000 [24:57<16:51:38,  1.24s/it]  2%|▏         | 1169/50000 [24:58<16:51:41,  1.24s/it]  2%|▏         | 1170/50000 [25:00<16:51:40,  1.24s/it]                                                       {'loss': 185.0781, 'learning_rate': 9.128e-06, 'epoch': 0.18}
  2%|▏         | 1170/50000 [25:00<16:51:40,  1.24s/it]  2%|▏         | 1171/50000 [25:01<16:52:02,  1.24s/it]  2%|▏         | 1172/50000 [25:02<16:51:50,  1.24s/it]  2%|▏         | 1173/50000 [25:03<16:51:30,  1.24s/it]  2%|▏         | 1174/50000 [25:05<16:51:16,  1.24s/it]  2%|▏         | 1175/50000 [25:06<16:51:15,  1.24s/it]  2%|▏         | 1176/50000 [25:07<16:51:44,  1.24s/it]  2%|▏         | 1177/50000 [25:08<16:51:38,  1.24s/it]  2%|▏         | 1178/50000 [25:09<16:51:49,  1.24s/it]  2%|▏         | 1179/50000 [25:11<16:51:44,  1.24s/it]  2%|▏         | 1180/50000 [25:12<16:51:44,  1.24s/it]                                                       {'loss': 164.2766, 'learning_rate': 9.208e-06, 'epoch': 0.18}
  2%|▏         | 1180/50000 [25:12<16:51:44,  1.24s/it]  2%|▏         | 1181/50000 [25:13<16:51:33,  1.24s/it]  2%|▏         | 1182/50000 [25:14<16:51:18,  1.24s/it]  2%|▏         | 1183/50000 [25:16<16:51:16,  1.24s/it]  2%|▏         | 1184/50000 [25:17<16:51:22,  1.24s/it]  2%|▏         | 1185/50000 [25:18<16:51:15,  1.24s/it]  2%|▏         | 1186/50000 [25:19<16:51:53,  1.24s/it]  2%|▏         | 1187/50000 [25:21<16:51:29,  1.24s/it]  2%|▏         | 1188/50000 [25:22<16:51:16,  1.24s/it]  2%|▏         | 1189/50000 [25:23<16:51:26,  1.24s/it]  2%|▏         | 1190/50000 [25:24<16:54:34,  1.25s/it]                                                       {'loss': 182.3219, 'learning_rate': 9.288e-06, 'epoch': 0.18}
  2%|▏         | 1190/50000 [25:24<16:54:34,  1.25s/it]  2%|▏         | 1191/50000 [25:26<16:53:36,  1.25s/it]  2%|▏         | 1192/50000 [25:27<16:52:39,  1.24s/it]  2%|▏         | 1193/50000 [25:28<16:51:52,  1.24s/it]  2%|▏         | 1194/50000 [25:29<16:51:27,  1.24s/it]  2%|▏         | 1195/50000 [25:31<16:51:55,  1.24s/it]  2%|▏         | 1196/50000 [25:32<16:51:22,  1.24s/it]  2%|▏         | 1197/50000 [25:33<16:51:07,  1.24s/it]  2%|▏         | 1198/50000 [25:34<16:51:02,  1.24s/it]  2%|▏         | 1199/50000 [25:36<16:50:41,  1.24s/it]  2%|▏         | 1200/50000 [25:37<16:50:39,  1.24s/it]                                                       {'loss': 181.7625, 'learning_rate': 9.368e-06, 'epoch': 0.19}
  2%|▏         | 1200/50000 [25:37<16:50:39,  1.24s/it]  2%|▏         | 1201/50000 [25:38<16:50:34,  1.24s/it]  2%|▏         | 1202/50000 [25:39<16:50:23,  1.24s/it]  2%|▏         | 1203/50000 [25:41<16:50:34,  1.24s/it]  2%|▏         | 1204/50000 [25:42<16:50:44,  1.24s/it]  2%|▏         | 1205/50000 [25:43<16:50:46,  1.24s/it]  2%|▏         | 1206/50000 [25:44<16:50:44,  1.24s/it]  2%|▏         | 1207/50000 [25:46<16:50:32,  1.24s/it]  2%|▏         | 1208/50000 [25:47<16:51:08,  1.24s/it]  2%|▏         | 1209/50000 [25:48<16:50:47,  1.24s/it]  2%|▏         | 1210/50000 [25:49<16:50:44,  1.24s/it]                                                       {'loss': 148.0484, 'learning_rate': 9.448e-06, 'epoch': 0.19}
  2%|▏         | 1210/50000 [25:49<16:50:44,  1.24s/it]  2%|▏         | 1211/50000 [25:51<16:50:52,  1.24s/it]  2%|▏         | 1212/50000 [25:52<16:50:48,  1.24s/it]  2%|▏         | 1213/50000 [25:53<16:50:35,  1.24s/it]  2%|▏         | 1214/50000 [25:54<16:50:37,  1.24s/it]  2%|▏         | 1215/50000 [25:55<16:50:18,  1.24s/it]  2%|▏         | 1216/50000 [25:57<16:50:23,  1.24s/it]  2%|▏         | 1217/50000 [25:58<16:50:30,  1.24s/it]  2%|▏         | 1218/50000 [25:59<16:50:44,  1.24s/it]  2%|▏         | 1219/50000 [26:00<16:50:38,  1.24s/it]  2%|▏         | 1220/50000 [26:02<16:50:43,  1.24s/it]                                                       {'loss': 168.0094, 'learning_rate': 9.528000000000001e-06, 'epoch': 0.19}
  2%|▏         | 1220/50000 [26:02<16:50:43,  1.24s/it]  2%|▏         | 1221/50000 [26:03<16:50:46,  1.24s/it]  2%|▏         | 1222/50000 [26:04<16:50:35,  1.24s/it]  2%|▏         | 1223/50000 [26:05<16:50:34,  1.24s/it]  2%|▏         | 1224/50000 [26:07<16:50:45,  1.24s/it]  2%|▏         | 1225/50000 [26:08<16:50:47,  1.24s/it]  2%|▏         | 1226/50000 [26:09<16:50:23,  1.24s/it]  2%|▏         | 1227/50000 [26:10<16:50:36,  1.24s/it]  2%|▏         | 1228/50000 [26:12<16:50:11,  1.24s/it]  2%|▏         | 1229/50000 [26:13<16:50:03,  1.24s/it]  2%|▏         | 1230/50000 [26:14<16:50:26,  1.24s/it]                                                       {'loss': 167.95, 'learning_rate': 9.608e-06, 'epoch': 0.19}
  2%|▏         | 1230/50000 [26:14<16:50:26,  1.24s/it]  2%|▏         | 1231/50000 [26:15<16:50:32,  1.24s/it]  2%|▏         | 1232/50000 [26:17<16:50:26,  1.24s/it]  2%|▏         | 1233/50000 [26:18<16:50:54,  1.24s/it]  2%|▏         | 1234/50000 [26:19<16:51:00,  1.24s/it]  2%|▏         | 1235/50000 [26:20<16:50:55,  1.24s/it]  2%|▏         | 1236/50000 [26:22<16:50:52,  1.24s/it]  2%|▏         | 1237/50000 [26:23<16:50:56,  1.24s/it]  2%|▏         | 1238/50000 [26:24<16:50:57,  1.24s/it]  2%|▏         | 1239/50000 [26:25<16:51:31,  1.24s/it]  2%|▏         | 1240/50000 [26:27<16:52:10,  1.25s/it]                                                       {'loss': 157.6438, 'learning_rate': 9.688000000000001e-06, 'epoch': 0.19}
  2%|▏         | 1240/50000 [26:27<16:52:10,  1.25s/it]  2%|▏         | 1241/50000 [26:28<16:52:31,  1.25s/it]  2%|▏         | 1242/50000 [26:29<16:53:16,  1.25s/it]  2%|▏         | 1243/50000 [26:30<16:54:41,  1.25s/it]  2%|▏         | 1244/50000 [26:32<16:53:20,  1.25s/it]  2%|▏         | 1245/50000 [26:33<16:52:35,  1.25s/it]  2%|▏         | 1246/50000 [26:34<16:51:59,  1.25s/it]  2%|▏         | 1247/50000 [26:35<16:51:25,  1.24s/it]  2%|▏         | 1248/50000 [26:37<16:51:38,  1.25s/it]  2%|▏         | 1249/50000 [26:38<16:51:35,  1.25s/it]  2%|▎         | 1250/50000 [26:39<16:52:20,  1.25s/it]                                                       {'loss': 158.9281, 'learning_rate': 9.768e-06, 'epoch': 0.19}
  2%|▎         | 1250/50000 [26:39<16:52:20,  1.25s/it]  3%|▎         | 1251/50000 [26:40<16:52:03,  1.25s/it]  3%|▎         | 1252/50000 [26:42<16:51:48,  1.25s/it]  3%|▎         | 1253/50000 [26:43<16:51:01,  1.24s/it]  3%|▎         | 1254/50000 [26:44<16:51:03,  1.24s/it]  3%|▎         | 1255/50000 [26:45<16:51:31,  1.25s/it]  3%|▎         | 1256/50000 [26:47<16:51:16,  1.24s/it]  3%|▎         | 1257/50000 [26:48<16:51:16,  1.24s/it]  3%|▎         | 1258/50000 [26:49<16:51:14,  1.24s/it]  3%|▎         | 1259/50000 [26:50<16:50:50,  1.24s/it]  3%|▎         | 1260/50000 [26:51<16:50:17,  1.24s/it]                                                       {'loss': 165.625, 'learning_rate': 9.848000000000001e-06, 'epoch': 0.2}
  3%|▎         | 1260/50000 [26:51<16:50:17,  1.24s/it]  3%|▎         | 1261/50000 [26:53<16:50:05,  1.24s/it]  3%|▎         | 1262/50000 [26:54<16:49:58,  1.24s/it]  3%|▎         | 1263/50000 [26:55<16:49:38,  1.24s/it]  3%|▎         | 1264/50000 [26:56<16:49:27,  1.24s/it]  3%|▎         | 1265/50000 [26:58<16:49:13,  1.24s/it]  3%|▎         | 1266/50000 [26:59<16:49:30,  1.24s/it]  3%|▎         | 1267/50000 [27:00<16:49:29,  1.24s/it]  3%|▎         | 1268/50000 [27:01<16:49:18,  1.24s/it]  3%|▎         | 1269/50000 [27:03<16:49:09,  1.24s/it]  3%|▎         | 1270/50000 [27:04<16:49:21,  1.24s/it]                                                       {'loss': 147.3234, 'learning_rate': 9.928e-06, 'epoch': 0.2}
  3%|▎         | 1270/50000 [27:04<16:49:21,  1.24s/it]  3%|▎         | 1271/50000 [27:05<16:49:42,  1.24s/it]  3%|▎         | 1272/50000 [27:06<16:48:58,  1.24s/it]  3%|▎         | 1273/50000 [27:08<16:48:45,  1.24s/it]  3%|▎         | 1274/50000 [27:09<16:48:42,  1.24s/it]  3%|▎         | 1275/50000 [27:10<16:48:39,  1.24s/it]  3%|▎         | 1276/50000 [27:11<16:48:14,  1.24s/it]  3%|▎         | 1277/50000 [27:13<16:48:29,  1.24s/it]  3%|▎         | 1278/50000 [27:14<16:48:46,  1.24s/it]  3%|▎         | 1279/50000 [27:15<16:48:46,  1.24s/it]  3%|▎         | 1280/50000 [27:16<16:48:31,  1.24s/it]                                                       {'loss': 166.0016, 'learning_rate': 1.0008e-05, 'epoch': 0.2}
  3%|▎         | 1280/50000 [27:16<16:48:31,  1.24s/it]  3%|▎         | 1281/50000 [27:18<16:48:48,  1.24s/it]  3%|▎         | 1282/50000 [27:19<16:48:57,  1.24s/it]  3%|▎         | 1283/50000 [27:20<16:48:21,  1.24s/it]  3%|▎         | 1284/50000 [27:21<16:48:21,  1.24s/it]  3%|▎         | 1285/50000 [27:23<16:48:35,  1.24s/it]  3%|▎         | 1286/50000 [27:24<16:49:02,  1.24s/it]  3%|▎         | 1287/50000 [27:25<16:48:26,  1.24s/it]  3%|▎         | 1288/50000 [27:26<16:47:48,  1.24s/it]  3%|▎         | 1289/50000 [27:28<16:47:30,  1.24s/it]  3%|▎         | 1290/50000 [27:29<16:47:33,  1.24s/it]                                                       {'loss': 183.4719, 'learning_rate': 1.0088e-05, 'epoch': 0.2}
  3%|▎         | 1290/50000 [27:29<16:47:33,  1.24s/it]  3%|▎         | 1291/50000 [27:30<16:48:13,  1.24s/it]  3%|▎         | 1292/50000 [27:31<16:48:31,  1.24s/it]  3%|▎         | 1293/50000 [27:32<16:49:47,  1.24s/it]  3%|▎         | 1294/50000 [27:34<16:50:34,  1.24s/it]  3%|▎         | 1295/50000 [27:35<16:49:40,  1.24s/it]  3%|▎         | 1296/50000 [27:36<16:50:04,  1.24s/it]  3%|▎         | 1297/50000 [27:37<16:51:10,  1.25s/it]  3%|▎         | 1298/50000 [27:39<16:50:40,  1.25s/it]  3%|▎         | 1299/50000 [27:40<16:50:41,  1.25s/it]  3%|▎         | 1300/50000 [27:41<16:50:10,  1.24s/it]                                                       {'loss': 156.2172, 'learning_rate': 1.0168e-05, 'epoch': 0.2}
  3%|▎         | 1300/50000 [27:41<16:50:10,  1.24s/it]  3%|▎         | 1301/50000 [27:42<16:50:09,  1.24s/it]  3%|▎         | 1302/50000 [27:44<16:49:36,  1.24s/it]  3%|▎         | 1303/50000 [27:45<16:50:00,  1.24s/it]  3%|▎         | 1304/50000 [27:46<16:49:41,  1.24s/it]  3%|▎         | 1305/50000 [27:47<16:49:18,  1.24s/it]  3%|▎         | 1306/50000 [27:49<16:48:48,  1.24s/it]  3%|▎         | 1307/50000 [27:50<16:48:42,  1.24s/it]  3%|▎         | 1308/50000 [27:51<16:48:19,  1.24s/it]  3%|▎         | 1309/50000 [27:52<16:47:50,  1.24s/it]  3%|▎         | 1310/50000 [27:54<16:47:57,  1.24s/it]                                                       {'loss': 196.2344, 'learning_rate': 1.0248e-05, 'epoch': 0.2}
  3%|▎         | 1310/50000 [27:54<16:47:57,  1.24s/it]  3%|▎         | 1311/50000 [27:55<16:48:03,  1.24s/it]  3%|▎         | 1312/50000 [27:56<16:46:58,  1.24s/it]  3%|▎         | 1313/50000 [27:57<16:46:25,  1.24s/it]  3%|▎         | 1314/50000 [27:59<16:45:59,  1.24s/it]  3%|▎         | 1315/50000 [28:00<16:46:07,  1.24s/it]  3%|▎         | 1316/50000 [28:01<16:46:12,  1.24s/it]  3%|▎         | 1317/50000 [28:02<16:45:59,  1.24s/it]  3%|▎         | 1318/50000 [28:04<16:45:50,  1.24s/it]  3%|▎         | 1319/50000 [28:05<16:45:30,  1.24s/it]  3%|▎         | 1320/50000 [28:06<16:46:19,  1.24s/it]                                                       {'loss': 168.4938, 'learning_rate': 1.0328e-05, 'epoch': 0.2}
  3%|▎         | 1320/50000 [28:06<16:46:19,  1.24s/it]  3%|▎         | 1321/50000 [28:07<16:46:14,  1.24s/it]  3%|▎         | 1322/50000 [28:09<16:46:19,  1.24s/it]  3%|▎         | 1323/50000 [28:10<16:46:05,  1.24s/it]  3%|▎         | 1324/50000 [28:11<16:45:36,  1.24s/it]  3%|▎         | 1325/50000 [28:12<16:45:27,  1.24s/it]  3%|▎         | 1326/50000 [28:13<16:45:35,  1.24s/it]  3%|▎         | 1327/50000 [28:15<16:45:23,  1.24s/it]  3%|▎         | 1328/50000 [28:16<16:45:24,  1.24s/it]  3%|▎         | 1329/50000 [28:17<16:44:57,  1.24s/it]  3%|▎         | 1330/50000 [28:18<16:45:10,  1.24s/it]                                                       {'loss': 135.5133, 'learning_rate': 1.0408000000000001e-05, 'epoch': 0.21}
  3%|▎         | 1330/50000 [28:18<16:45:10,  1.24s/it]  3%|▎         | 1331/50000 [28:20<16:45:00,  1.24s/it]  3%|▎         | 1332/50000 [28:21<16:44:45,  1.24s/it]  3%|▎         | 1333/50000 [28:22<16:44:37,  1.24s/it]  3%|▎         | 1334/50000 [28:23<16:44:37,  1.24s/it]  3%|▎         | 1335/50000 [28:25<16:44:46,  1.24s/it]  3%|▎         | 1336/50000 [28:26<16:45:05,  1.24s/it]  3%|▎         | 1337/50000 [28:27<16:45:14,  1.24s/it]  3%|▎         | 1338/50000 [28:28<16:45:29,  1.24s/it]  3%|▎         | 1339/50000 [28:30<16:45:45,  1.24s/it]  3%|▎         | 1340/50000 [28:31<16:46:15,  1.24s/it]                                                       {'loss': 180.5125, 'learning_rate': 1.0488e-05, 'epoch': 0.21}
  3%|▎         | 1340/50000 [28:31<16:46:15,  1.24s/it]  3%|▎         | 1341/50000 [28:32<16:46:48,  1.24s/it]  3%|▎         | 1342/50000 [28:33<16:46:56,  1.24s/it]  3%|▎         | 1343/50000 [28:35<16:46:48,  1.24s/it][2023-07-03 12:28:46,045] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, but hysteresis is 2. Reducing hysteresis to 1
  3%|▎         | 1344/50000 [28:36<15:46:24,  1.17s/it]  3%|▎         | 1345/50000 [28:37<16:04:35,  1.19s/it]  3%|▎         | 1346/50000 [28:38<16:17:21,  1.21s/it]  3%|▎         | 1347/50000 [28:39<16:26:18,  1.22s/it]  3%|▎         | 1348/50000 [28:40<16:32:46,  1.22s/it]  3%|▎         | 1349/50000 [28:42<16:36:59,  1.23s/it]  3%|▎         | 1350/50000 [28:43<16:40:11,  1.23s/it]                                                       {'loss': 166.1625, 'learning_rate': 1.056e-05, 'epoch': 0.21}
  3%|▎         | 1350/50000 [28:43<16:40:11,  1.23s/it]  3%|▎         | 1351/50000 [28:44<16:42:25,  1.24s/it]  3%|▎         | 1352/50000 [28:45<16:44:25,  1.24s/it]  3%|▎         | 1353/50000 [28:47<16:45:17,  1.24s/it]  3%|▎         | 1354/50000 [28:48<16:45:37,  1.24s/it]  3%|▎         | 1355/50000 [28:49<16:45:59,  1.24s/it]  3%|▎         | 1356/50000 [28:50<16:46:26,  1.24s/it]  3%|▎         | 1357/50000 [28:52<16:46:44,  1.24s/it]  3%|▎         | 1358/50000 [28:53<16:47:36,  1.24s/it][2023-07-03 12:29:04,437] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
  3%|▎         | 1359/50000 [28:54<15:47:01,  1.17s/it]  3%|▎         | 1360/50000 [28:55<16:04:41,  1.19s/it]                                                       {'loss': 144.5469, 'learning_rate': 1.0632000000000001e-05, 'epoch': 0.21}
  3%|▎         | 1360/50000 [28:55<16:04:41,  1.19s/it]  3%|▎         | 1361/50000 [28:56<16:17:39,  1.21s/it]  3%|▎         | 1362/50000 [28:58<16:26:16,  1.22s/it]  3%|▎         | 1363/50000 [28:59<16:32:20,  1.22s/it]  3%|▎         | 1364/50000 [29:00<16:36:32,  1.23s/it]  3%|▎         | 1365/50000 [29:01<16:39:34,  1.23s/it]  3%|▎         | 1366/50000 [29:03<16:41:39,  1.24s/it]  3%|▎         | 1367/50000 [29:04<16:43:47,  1.24s/it]  3%|▎         | 1368/50000 [29:05<16:44:38,  1.24s/it]  3%|▎         | 1369/50000 [29:06<16:45:34,  1.24s/it]  3%|▎         | 1370/50000 [29:08<16:46:03,  1.24s/it]                                                       {'loss': 228.2281, 'learning_rate': 1.0712e-05, 'epoch': 0.21}
  3%|▎         | 1370/50000 [29:08<16:46:03,  1.24s/it]  3%|▎         | 1371/50000 [29:09<16:46:31,  1.24s/it]  3%|▎         | 1372/50000 [29:10<16:46:47,  1.24s/it]  3%|▎         | 1373/50000 [29:11<16:46:52,  1.24s/it]  3%|▎         | 1374/50000 [29:13<16:46:50,  1.24s/it]  3%|▎         | 1375/50000 [29:14<16:46:52,  1.24s/it]  3%|▎         | 1376/50000 [29:15<16:46:51,  1.24s/it]  3%|▎         | 1377/50000 [29:16<16:46:47,  1.24s/it]  3%|▎         | 1378/50000 [29:18<16:46:42,  1.24s/it]  3%|▎         | 1379/50000 [29:19<16:46:40,  1.24s/it]  3%|▎         | 1380/50000 [29:20<16:46:36,  1.24s/it]                                                       {'loss': 208.65, 'learning_rate': 1.0792000000000001e-05, 'epoch': 0.21}
  3%|▎         | 1380/50000 [29:20<16:46:36,  1.24s/it]  3%|▎         | 1381/50000 [29:21<16:47:06,  1.24s/it]  3%|▎         | 1382/50000 [29:22<16:46:47,  1.24s/it]  3%|▎         | 1383/50000 [29:24<16:46:37,  1.24s/it]  3%|▎         | 1384/50000 [29:25<16:46:59,  1.24s/it]  3%|▎         | 1385/50000 [29:26<16:46:38,  1.24s/it]  3%|▎         | 1386/50000 [29:27<16:47:03,  1.24s/it]  3%|▎         | 1387/50000 [29:29<16:47:06,  1.24s/it]  3%|▎         | 1388/50000 [29:30<16:46:44,  1.24s/it]  3%|▎         | 1389/50000 [29:31<16:46:43,  1.24s/it]  3%|▎         | 1390/50000 [29:32<16:46:35,  1.24s/it]                                                       {'loss': 167.1547, 'learning_rate': 1.0872e-05, 'epoch': 0.22}
  3%|▎         | 1390/50000 [29:32<16:46:35,  1.24s/it]  3%|▎         | 1391/50000 [29:34<16:47:29,  1.24s/it]  3%|▎         | 1392/50000 [29:35<16:48:03,  1.24s/it]  3%|▎         | 1393/50000 [29:36<16:47:45,  1.24s/it]  3%|▎         | 1394/50000 [29:37<16:47:40,  1.24s/it]  3%|▎         | 1395/50000 [29:39<16:47:27,  1.24s/it]  3%|▎         | 1396/50000 [29:40<16:47:27,  1.24s/it]  3%|▎         | 1397/50000 [29:41<16:47:04,  1.24s/it]  3%|▎         | 1398/50000 [29:42<16:46:39,  1.24s/it]  3%|▎         | 1399/50000 [29:44<16:46:31,  1.24s/it]  3%|▎         | 1400/50000 [29:45<16:46:14,  1.24s/it]                                                       {'loss': 181.0797, 'learning_rate': 1.0952000000000001e-05, 'epoch': 0.22}
  3%|▎         | 1400/50000 [29:45<16:46:14,  1.24s/it]  3%|▎         | 1401/50000 [29:46<16:46:30,  1.24s/it]  3%|▎         | 1402/50000 [29:47<16:46:21,  1.24s/it]  3%|▎         | 1403/50000 [29:49<16:46:52,  1.24s/it]  3%|▎         | 1404/50000 [29:50<16:46:34,  1.24s/it]  3%|▎         | 1405/50000 [29:51<16:46:19,  1.24s/it]  3%|▎         | 1406/50000 [29:52<16:46:22,  1.24s/it]  3%|▎         | 1407/50000 [29:54<16:46:26,  1.24s/it]  3%|▎         | 1408/50000 [29:55<16:46:38,  1.24s/it]  3%|▎         | 1409/50000 [29:56<16:46:37,  1.24s/it]  3%|▎         | 1410/50000 [29:57<16:46:18,  1.24s/it]                                                       {'loss': 138.1719, 'learning_rate': 1.1032e-05, 'epoch': 0.22}
  3%|▎         | 1410/50000 [29:57<16:46:18,  1.24s/it]  3%|▎         | 1411/50000 [29:59<16:46:15,  1.24s/it]  3%|▎         | 1412/50000 [30:00<16:46:00,  1.24s/it]  3%|▎         | 1413/50000 [30:01<16:46:11,  1.24s/it]  3%|▎         | 1414/50000 [30:02<16:46:00,  1.24s/it]  3%|▎         | 1415/50000 [30:04<16:46:06,  1.24s/it]  3%|▎         | 1416/50000 [30:05<16:45:55,  1.24s/it]  3%|▎         | 1417/50000 [30:06<16:45:48,  1.24s/it]  3%|▎         | 1418/50000 [30:07<16:45:46,  1.24s/it]  3%|▎         | 1419/50000 [30:08<16:45:52,  1.24s/it]  3%|▎         | 1420/50000 [30:10<16:46:04,  1.24s/it]                                                       {'loss': 159.2406, 'learning_rate': 1.1112000000000001e-05, 'epoch': 0.22}
  3%|▎         | 1420/50000 [30:10<16:46:04,  1.24s/it]  3%|▎         | 1421/50000 [30:11<16:46:09,  1.24s/it]  3%|▎         | 1422/50000 [30:12<16:45:51,  1.24s/it]  3%|▎         | 1423/50000 [30:13<16:45:46,  1.24s/it][2023-07-03 12:30:24,960] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  3%|▎         | 1424/50000 [30:14<15:45:23,  1.17s/it]  3%|▎         | 1425/50000 [30:16<16:03:09,  1.19s/it]  3%|▎         | 1426/50000 [30:17<16:15:47,  1.21s/it]  3%|▎         | 1427/50000 [30:18<16:24:45,  1.22s/it]  3%|▎         | 1428/50000 [30:19<16:30:53,  1.22s/it]  3%|▎         | 1429/50000 [30:21<16:35:03,  1.23s/it]  3%|▎         | 1430/50000 [30:22<16:38:17,  1.23s/it]                                                       {'loss': 153.9812, 'learning_rate': 1.1184000000000002e-05, 'epoch': 0.22}
  3%|▎         | 1430/50000 [30:22<16:38:17,  1.23s/it]  3%|▎         | 1431/50000 [30:23<16:40:28,  1.24s/it]  3%|▎         | 1432/50000 [30:24<16:42:08,  1.24s/it]  3%|▎         | 1433/50000 [30:26<16:43:09,  1.24s/it]  3%|▎         | 1434/50000 [30:27<16:43:43,  1.24s/it]  3%|▎         | 1435/50000 [30:28<16:44:58,  1.24s/it]  3%|▎         | 1436/50000 [30:29<16:45:20,  1.24s/it]  3%|▎         | 1437/50000 [30:31<16:45:08,  1.24s/it]  3%|▎         | 1438/50000 [30:32<16:46:00,  1.24s/it]  3%|▎         | 1439/50000 [30:33<16:45:36,  1.24s/it]  3%|▎         | 1440/50000 [30:34<16:45:43,  1.24s/it]                                                       {'loss': 169.8656, 'learning_rate': 1.1264000000000001e-05, 'epoch': 0.22}
  3%|▎         | 1440/50000 [30:34<16:45:43,  1.24s/it]  3%|▎         | 1441/50000 [30:36<16:46:06,  1.24s/it]  3%|▎         | 1442/50000 [30:37<16:46:12,  1.24s/it]  3%|▎         | 1443/50000 [30:38<16:46:03,  1.24s/it]  3%|▎         | 1444/50000 [30:39<16:45:56,  1.24s/it]  3%|▎         | 1445/50000 [30:41<16:46:12,  1.24s/it]  3%|▎         | 1446/50000 [30:42<16:45:42,  1.24s/it]  3%|▎         | 1447/50000 [30:43<16:45:20,  1.24s/it]  3%|▎         | 1448/50000 [30:44<16:45:06,  1.24s/it]  3%|▎         | 1449/50000 [30:46<16:45:03,  1.24s/it]  3%|▎         | 1450/50000 [30:47<16:44:46,  1.24s/it]                                                       {'loss': 139.0062, 'learning_rate': 1.1344000000000002e-05, 'epoch': 0.22}
  3%|▎         | 1450/50000 [30:47<16:44:46,  1.24s/it]  3%|▎         | 1451/50000 [30:48<16:44:47,  1.24s/it]  3%|▎         | 1452/50000 [30:49<16:44:38,  1.24s/it]  3%|▎         | 1453/50000 [30:50<16:45:04,  1.24s/it]  3%|▎         | 1454/50000 [30:52<16:46:09,  1.24s/it]  3%|▎         | 1455/50000 [30:53<16:46:04,  1.24s/it]  3%|▎         | 1456/50000 [30:54<16:46:12,  1.24s/it]  3%|▎         | 1457/50000 [30:55<16:46:47,  1.24s/it]  3%|▎         | 1458/50000 [30:57<16:46:34,  1.24s/it]  3%|▎         | 1459/50000 [30:58<16:46:19,  1.24s/it]  3%|▎         | 1460/50000 [30:59<16:46:40,  1.24s/it]                                                       {'loss': 155.9062, 'learning_rate': 1.1424000000000001e-05, 'epoch': 0.23}
  3%|▎         | 1460/50000 [30:59<16:46:40,  1.24s/it]  3%|▎         | 1461/50000 [31:00<16:46:48,  1.24s/it]  3%|▎         | 1462/50000 [31:02<16:46:27,  1.24s/it]  3%|▎         | 1463/50000 [31:03<16:46:16,  1.24s/it]  3%|▎         | 1464/50000 [31:04<16:45:49,  1.24s/it]  3%|▎         | 1465/50000 [31:05<16:45:49,  1.24s/it]  3%|▎         | 1466/50000 [31:07<16:45:38,  1.24s/it]  3%|▎         | 1467/50000 [31:08<16:45:29,  1.24s/it]  3%|▎         | 1468/50000 [31:09<16:45:18,  1.24s/it]  3%|▎         | 1469/50000 [31:10<16:46:00,  1.24s/it]  3%|▎         | 1470/50000 [31:12<16:45:31,  1.24s/it]                                                       {'loss': 145.3953, 'learning_rate': 1.1504000000000002e-05, 'epoch': 0.23}
  3%|▎         | 1470/50000 [31:12<16:45:31,  1.24s/it]  3%|▎         | 1471/50000 [31:13<16:45:25,  1.24s/it]  3%|▎         | 1472/50000 [31:14<16:45:14,  1.24s/it]  3%|▎         | 1473/50000 [31:15<16:44:49,  1.24s/it]  3%|▎         | 1474/50000 [31:17<16:44:45,  1.24s/it]  3%|▎         | 1475/50000 [31:18<16:44:28,  1.24s/it]  3%|▎         | 1476/50000 [31:19<16:44:18,  1.24s/it]  3%|▎         | 1477/50000 [31:20<16:44:22,  1.24s/it]  3%|▎         | 1478/50000 [31:22<16:44:25,  1.24s/it]  3%|▎         | 1479/50000 [31:23<16:44:13,  1.24s/it]  3%|▎         | 1480/50000 [31:24<16:44:09,  1.24s/it]                                                       {'loss': 155.3672, 'learning_rate': 1.1584000000000001e-05, 'epoch': 0.23}
  3%|▎         | 1480/50000 [31:24<16:44:09,  1.24s/it]  3%|▎         | 1481/50000 [31:25<16:44:16,  1.24s/it]  3%|▎         | 1482/50000 [31:27<16:44:40,  1.24s/it]  3%|▎         | 1483/50000 [31:28<16:44:37,  1.24s/it]  3%|▎         | 1484/50000 [31:29<16:44:14,  1.24s/it]  3%|▎         | 1485/50000 [31:30<16:44:08,  1.24s/it]  3%|▎         | 1486/50000 [31:31<16:44:00,  1.24s/it]  3%|▎         | 1487/50000 [31:33<16:43:50,  1.24s/it]  3%|▎         | 1488/50000 [31:34<16:43:52,  1.24s/it]  3%|▎         | 1489/50000 [31:35<16:44:05,  1.24s/it]  3%|▎         | 1490/50000 [31:36<16:44:34,  1.24s/it]                                                       {'loss': 130.5328, 'learning_rate': 1.1664000000000002e-05, 'epoch': 0.23}
  3%|▎         | 1490/50000 [31:36<16:44:34,  1.24s/it]  3%|▎         | 1491/50000 [31:38<16:44:27,  1.24s/it]  3%|▎         | 1492/50000 [31:39<16:44:20,  1.24s/it]  3%|▎         | 1493/50000 [31:40<16:44:07,  1.24s/it]  3%|▎         | 1494/50000 [31:41<16:44:06,  1.24s/it]  3%|▎         | 1495/50000 [31:43<16:44:03,  1.24s/it]  3%|▎         | 1496/50000 [31:44<16:44:12,  1.24s/it]  3%|▎         | 1497/50000 [31:45<16:44:05,  1.24s/it]  3%|▎         | 1498/50000 [31:46<16:44:09,  1.24s/it]  3%|▎         | 1499/50000 [31:48<16:44:01,  1.24s/it]  3%|▎         | 1500/50000 [31:49<16:44:01,  1.24s/it]                                                       {'loss': 136.6172, 'learning_rate': 1.1744000000000001e-05, 'epoch': 0.23}
  3%|▎         | 1500/50000 [31:49<16:44:01,  1.24s/it]  3%|▎         | 1501/50000 [31:50<16:44:01,  1.24s/it]  3%|▎         | 1502/50000 [31:51<16:44:05,  1.24s/it]  3%|▎         | 1503/50000 [31:53<16:44:09,  1.24s/it]  3%|▎         | 1504/50000 [31:54<16:44:08,  1.24s/it]  3%|▎         | 1505/50000 [31:55<16:44:05,  1.24s/it]  3%|▎         | 1506/50000 [31:56<16:44:25,  1.24s/it]  3%|▎         | 1507/50000 [31:58<16:44:16,  1.24s/it]  3%|▎         | 1508/50000 [31:59<16:44:00,  1.24s/it]  3%|▎         | 1509/50000 [32:00<16:43:54,  1.24s/it]  3%|▎         | 1510/50000 [32:01<16:44:01,  1.24s/it]                                                       {'loss': 153.6797, 'learning_rate': 1.1824e-05, 'epoch': 0.23}
  3%|▎         | 1510/50000 [32:01<16:44:01,  1.24s/it]  3%|▎         | 1511/50000 [32:03<16:43:53,  1.24s/it]  3%|▎         | 1512/50000 [32:04<16:43:59,  1.24s/it]  3%|▎         | 1513/50000 [32:05<16:43:46,  1.24s/it]  3%|▎         | 1514/50000 [32:06<16:43:50,  1.24s/it]  3%|▎         | 1515/50000 [32:08<16:43:27,  1.24s/it]  3%|▎         | 1516/50000 [32:09<16:43:13,  1.24s/it]  3%|▎         | 1517/50000 [32:10<16:43:24,  1.24s/it]  3%|▎         | 1518/50000 [32:11<16:43:28,  1.24s/it]  3%|▎         | 1519/50000 [32:12<16:43:39,  1.24s/it]  3%|▎         | 1520/50000 [32:14<16:44:30,  1.24s/it]                                                       {'loss': 127.375, 'learning_rate': 1.1904e-05, 'epoch': 0.24}
  3%|▎         | 1520/50000 [32:14<16:44:30,  1.24s/it]  3%|▎         | 1521/50000 [32:15<16:44:19,  1.24s/it]  3%|▎         | 1522/50000 [32:16<16:44:07,  1.24s/it]  3%|▎         | 1523/50000 [32:17<16:44:00,  1.24s/it]  3%|▎         | 1524/50000 [32:19<16:43:48,  1.24s/it]  3%|▎         | 1525/50000 [32:20<16:43:44,  1.24s/it]  3%|▎         | 1526/50000 [32:21<16:43:36,  1.24s/it]  3%|▎         | 1527/50000 [32:22<16:43:29,  1.24s/it]  3%|▎         | 1528/50000 [32:24<16:43:25,  1.24s/it]  3%|▎         | 1529/50000 [32:25<16:43:20,  1.24s/it]  3%|▎         | 1530/50000 [32:26<16:43:18,  1.24s/it]                                                       {'loss': 155.1875, 'learning_rate': 1.1984e-05, 'epoch': 0.24}
  3%|▎         | 1530/50000 [32:26<16:43:18,  1.24s/it]  3%|▎         | 1531/50000 [32:27<16:43:20,  1.24s/it]  3%|▎         | 1532/50000 [32:29<16:43:20,  1.24s/it]  3%|▎         | 1533/50000 [32:30<16:43:05,  1.24s/it]  3%|▎         | 1534/50000 [32:31<16:42:48,  1.24s/it]  3%|▎         | 1535/50000 [32:32<16:42:53,  1.24s/it]  3%|▎         | 1536/50000 [32:34<16:42:57,  1.24s/it]  3%|▎         | 1537/50000 [32:35<16:43:00,  1.24s/it]  3%|▎         | 1538/50000 [32:36<16:44:13,  1.24s/it]  3%|▎         | 1539/50000 [32:37<16:44:49,  1.24s/it]  3%|▎         | 1540/50000 [32:39<16:44:43,  1.24s/it]                                                       {'loss': 171.6937, 'learning_rate': 1.2064e-05, 'epoch': 0.24}
  3%|▎         | 1540/50000 [32:39<16:44:43,  1.24s/it]  3%|▎         | 1541/50000 [32:40<16:45:47,  1.25s/it]  3%|▎         | 1542/50000 [32:41<16:46:03,  1.25s/it]  3%|▎         | 1543/50000 [32:42<16:46:43,  1.25s/it]  3%|▎         | 1544/50000 [32:44<16:46:36,  1.25s/it]  3%|▎         | 1545/50000 [32:45<16:46:04,  1.25s/it]  3%|▎         | 1546/50000 [32:46<16:45:45,  1.25s/it]  3%|▎         | 1547/50000 [32:47<16:45:45,  1.25s/it]  3%|▎         | 1548/50000 [32:49<16:45:14,  1.24s/it]  3%|▎         | 1549/50000 [32:50<16:45:32,  1.25s/it]  3%|▎         | 1550/50000 [32:51<16:45:08,  1.24s/it]                                                       {'loss': 165.0078, 'learning_rate': 1.2144000000000001e-05, 'epoch': 0.24}
  3%|▎         | 1550/50000 [32:51<16:45:08,  1.24s/it]  3%|▎         | 1551/50000 [32:52<16:45:12,  1.24s/it]  3%|▎         | 1552/50000 [32:54<16:44:19,  1.24s/it]  3%|▎         | 1553/50000 [32:55<16:44:14,  1.24s/it]  3%|▎         | 1554/50000 [32:56<16:43:33,  1.24s/it]  3%|▎         | 1555/50000 [32:57<16:43:32,  1.24s/it]  3%|▎         | 1556/50000 [32:58<16:43:18,  1.24s/it]  3%|▎         | 1557/50000 [33:00<16:42:59,  1.24s/it]  3%|▎         | 1558/50000 [33:01<16:43:40,  1.24s/it]  3%|▎         | 1559/50000 [33:02<16:44:24,  1.24s/it]  3%|▎         | 1560/50000 [33:03<16:44:06,  1.24s/it]                                                       {'loss': 157.2562, 'learning_rate': 1.2224e-05, 'epoch': 0.24}
  3%|▎         | 1560/50000 [33:03<16:44:06,  1.24s/it]  3%|▎         | 1561/50000 [33:05<16:44:06,  1.24s/it]  3%|▎         | 1562/50000 [33:06<16:44:28,  1.24s/it]  3%|▎         | 1563/50000 [33:07<16:44:07,  1.24s/it]  3%|▎         | 1564/50000 [33:08<16:43:35,  1.24s/it]  3%|▎         | 1565/50000 [33:10<16:43:08,  1.24s/it]  3%|▎         | 1566/50000 [33:11<16:42:52,  1.24s/it]  3%|▎         | 1567/50000 [33:12<16:42:33,  1.24s/it]  3%|▎         | 1568/50000 [33:13<16:42:53,  1.24s/it]  3%|▎         | 1569/50000 [33:15<16:42:45,  1.24s/it]  3%|▎         | 1570/50000 [33:16<16:42:34,  1.24s/it]                                                       {'loss': 155.3938, 'learning_rate': 1.2304000000000001e-05, 'epoch': 0.24}
  3%|▎         | 1570/50000 [33:16<16:42:34,  1.24s/it]  3%|▎         | 1571/50000 [33:17<16:42:41,  1.24s/it]  3%|▎         | 1572/50000 [33:18<16:42:52,  1.24s/it]  3%|▎         | 1573/50000 [33:20<16:42:40,  1.24s/it]  3%|▎         | 1574/50000 [33:21<16:42:27,  1.24s/it]  3%|▎         | 1575/50000 [33:22<16:42:20,  1.24s/it]  3%|▎         | 1576/50000 [33:23<16:42:16,  1.24s/it]  3%|▎         | 1577/50000 [33:25<16:42:30,  1.24s/it]  3%|▎         | 1578/50000 [33:26<16:42:22,  1.24s/it]  3%|▎         | 1579/50000 [33:27<16:42:12,  1.24s/it]  3%|▎         | 1580/50000 [33:28<16:42:34,  1.24s/it]                                                       {'loss': 134.6062, 'learning_rate': 1.2384e-05, 'epoch': 0.24}
  3%|▎         | 1580/50000 [33:28<16:42:34,  1.24s/it]  3%|▎         | 1581/50000 [33:30<16:42:55,  1.24s/it]  3%|▎         | 1582/50000 [33:31<16:42:40,  1.24s/it]  3%|▎         | 1583/50000 [33:32<16:42:21,  1.24s/it]  3%|▎         | 1584/50000 [33:33<16:42:08,  1.24s/it]  3%|▎         | 1585/50000 [33:35<16:41:57,  1.24s/it]  3%|▎         | 1586/50000 [33:36<16:41:59,  1.24s/it]  3%|▎         | 1587/50000 [33:37<16:41:55,  1.24s/it]  3%|▎         | 1588/50000 [33:38<16:42:05,  1.24s/it]  3%|▎         | 1589/50000 [33:39<16:41:42,  1.24s/it]  3%|▎         | 1590/50000 [33:41<16:42:00,  1.24s/it]                                                       {'loss': 160.9844, 'learning_rate': 1.2464000000000001e-05, 'epoch': 0.25}
  3%|▎         | 1590/50000 [33:41<16:42:00,  1.24s/it]  3%|▎         | 1591/50000 [33:42<16:42:34,  1.24s/it]  3%|▎         | 1592/50000 [33:43<16:42:24,  1.24s/it]  3%|▎         | 1593/50000 [33:44<16:42:20,  1.24s/it]  3%|▎         | 1594/50000 [33:46<16:42:15,  1.24s/it]  3%|▎         | 1595/50000 [33:47<16:42:11,  1.24s/it]  3%|▎         | 1596/50000 [33:48<16:42:13,  1.24s/it]  3%|▎         | 1597/50000 [33:49<16:42:01,  1.24s/it]  3%|▎         | 1598/50000 [33:51<16:42:44,  1.24s/it]  3%|▎         | 1599/50000 [33:52<16:42:30,  1.24s/it]  3%|▎         | 1600/50000 [33:53<16:42:16,  1.24s/it]                                                       {'loss': 145.2016, 'learning_rate': 1.2544e-05, 'epoch': 0.25}
  3%|▎         | 1600/50000 [33:53<16:42:16,  1.24s/it]  3%|▎         | 1601/50000 [33:54<16:42:33,  1.24s/it]  3%|▎         | 1602/50000 [33:56<16:42:16,  1.24s/it]  3%|▎         | 1603/50000 [33:57<16:42:11,  1.24s/it]  3%|▎         | 1604/50000 [33:58<16:42:13,  1.24s/it]  3%|▎         | 1605/50000 [33:59<16:42:06,  1.24s/it]  3%|▎         | 1606/50000 [34:01<16:41:58,  1.24s/it]  3%|▎         | 1607/50000 [34:02<16:42:12,  1.24s/it]  3%|▎         | 1608/50000 [34:03<16:42:10,  1.24s/it]  3%|▎         | 1609/50000 [34:04<16:42:16,  1.24s/it]  3%|▎         | 1610/50000 [34:06<16:42:05,  1.24s/it]                                                       {'loss': 135.7594, 'learning_rate': 1.2624000000000001e-05, 'epoch': 0.25}
  3%|▎         | 1610/50000 [34:06<16:42:05,  1.24s/it]  3%|▎         | 1611/50000 [34:07<16:42:06,  1.24s/it]  3%|▎         | 1612/50000 [34:08<16:42:01,  1.24s/it]  3%|▎         | 1613/50000 [34:09<16:42:01,  1.24s/it]  3%|▎         | 1614/50000 [34:11<16:42:07,  1.24s/it]  3%|▎         | 1615/50000 [34:12<16:42:04,  1.24s/it]  3%|▎         | 1616/50000 [34:13<16:41:51,  1.24s/it]  3%|▎         | 1617/50000 [34:14<16:41:36,  1.24s/it]  3%|▎         | 1618/50000 [34:16<16:41:36,  1.24s/it]  3%|▎         | 1619/50000 [34:17<16:41:19,  1.24s/it]  3%|▎         | 1620/50000 [34:18<16:41:44,  1.24s/it]                                                       {'loss': 152.6531, 'learning_rate': 1.2704e-05, 'epoch': 0.25}
  3%|▎         | 1620/50000 [34:18<16:41:44,  1.24s/it]  3%|▎         | 1621/50000 [34:19<16:41:38,  1.24s/it]  3%|▎         | 1622/50000 [34:20<16:41:33,  1.24s/it]  3%|▎         | 1623/50000 [34:22<16:41:25,  1.24s/it]  3%|▎         | 1624/50000 [34:23<16:41:12,  1.24s/it][2023-07-03 12:34:34,484] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, but hysteresis is 2. Reducing hysteresis to 1
  3%|▎         | 1625/50000 [34:24<15:41:04,  1.17s/it]  3%|▎         | 1626/50000 [34:25<15:58:52,  1.19s/it]  3%|▎         | 1627/50000 [34:26<16:11:33,  1.21s/it]  3%|▎         | 1628/50000 [34:28<16:20:25,  1.22s/it]  3%|▎         | 1629/50000 [34:29<16:26:43,  1.22s/it]  3%|▎         | 1630/50000 [34:30<16:31:08,  1.23s/it]                                                       {'loss': 182.8891, 'learning_rate': 1.2776000000000001e-05, 'epoch': 0.25}
  3%|▎         | 1630/50000 [34:30<16:31:08,  1.23s/it]  3%|▎         | 1631/50000 [34:31<16:34:42,  1.23s/it]  3%|▎         | 1632/50000 [34:33<16:36:27,  1.24s/it]  3%|▎         | 1633/50000 [34:34<16:37:58,  1.24s/it]  3%|▎         | 1634/50000 [34:35<16:38:48,  1.24s/it]  3%|▎         | 1635/50000 [34:36<16:39:39,  1.24s/it]  3%|▎         | 1636/50000 [34:38<16:40:14,  1.24s/it]  3%|▎         | 1637/50000 [34:39<16:40:21,  1.24s/it]  3%|▎         | 1638/50000 [34:40<16:40:40,  1.24s/it]  3%|▎         | 1639/50000 [34:41<16:40:53,  1.24s/it]  3%|▎         | 1640/50000 [34:43<16:40:49,  1.24s/it]                                                       {'loss': 150.5547, 'learning_rate': 1.2856000000000002e-05, 'epoch': 0.25}
  3%|▎         | 1640/50000 [34:43<16:40:49,  1.24s/it]  3%|▎         | 1641/50000 [34:44<16:41:06,  1.24s/it]  3%|▎         | 1642/50000 [34:45<16:40:57,  1.24s/it]  3%|▎         | 1643/50000 [34:46<16:40:48,  1.24s/it]  3%|▎         | 1644/50000 [34:48<16:40:54,  1.24s/it]  3%|▎         | 1645/50000 [34:49<16:40:46,  1.24s/it]  3%|▎         | 1646/50000 [34:50<16:40:50,  1.24s/it]  3%|▎         | 1647/50000 [34:51<16:40:52,  1.24s/it]  3%|▎         | 1648/50000 [34:53<16:41:02,  1.24s/it]  3%|▎         | 1649/50000 [34:54<16:41:27,  1.24s/it]  3%|▎         | 1650/50000 [34:55<16:41:30,  1.24s/it]                                                       {'loss': 152.1664, 'learning_rate': 1.2936000000000001e-05, 'epoch': 0.26}
  3%|▎         | 1650/50000 [34:55<16:41:30,  1.24s/it]  3%|▎         | 1651/50000 [34:56<16:42:11,  1.24s/it]  3%|▎         | 1652/50000 [34:58<16:41:43,  1.24s/it]  3%|▎         | 1653/50000 [34:59<16:41:30,  1.24s/it]  3%|▎         | 1654/50000 [35:00<16:41:24,  1.24s/it]  3%|▎         | 1655/50000 [35:01<16:41:13,  1.24s/it]  3%|▎         | 1656/50000 [35:02<16:41:14,  1.24s/it]  3%|▎         | 1657/50000 [35:04<16:41:11,  1.24s/it]  3%|▎         | 1658/50000 [35:05<16:41:45,  1.24s/it]  3%|▎         | 1659/50000 [35:06<16:41:27,  1.24s/it]  3%|▎         | 1660/50000 [35:07<16:41:15,  1.24s/it]                                                       {'loss': 150.8266, 'learning_rate': 1.3016000000000002e-05, 'epoch': 0.26}
  3%|▎         | 1660/50000 [35:07<16:41:15,  1.24s/it]  3%|▎         | 1661/50000 [35:09<16:41:09,  1.24s/it]  3%|▎         | 1662/50000 [35:10<16:41:13,  1.24s/it]  3%|▎         | 1663/50000 [35:11<16:41:15,  1.24s/it]  3%|▎         | 1664/50000 [35:12<16:41:03,  1.24s/it]  3%|▎         | 1665/50000 [35:14<16:40:58,  1.24s/it]  3%|▎         | 1666/50000 [35:15<16:41:01,  1.24s/it]  3%|▎         | 1667/50000 [35:16<16:40:52,  1.24s/it]  3%|▎         | 1668/50000 [35:17<16:40:39,  1.24s/it]  3%|▎         | 1669/50000 [35:19<16:40:30,  1.24s/it]  3%|▎         | 1670/50000 [35:20<16:40:55,  1.24s/it]                                                       {'loss': 156.7188, 'learning_rate': 1.3096000000000001e-05, 'epoch': 0.26}
  3%|▎         | 1670/50000 [35:20<16:40:55,  1.24s/it]  3%|▎         | 1671/50000 [35:21<16:41:24,  1.24s/it]  3%|▎         | 1672/50000 [35:22<16:41:14,  1.24s/it]  3%|▎         | 1673/50000 [35:24<16:40:57,  1.24s/it]  3%|▎         | 1674/50000 [35:25<16:43:58,  1.25s/it]  3%|▎         | 1675/50000 [35:26<16:43:24,  1.25s/it]  3%|▎         | 1676/50000 [35:27<16:43:00,  1.25s/it]  3%|▎         | 1677/50000 [35:29<16:43:13,  1.25s/it]  3%|▎         | 1678/50000 [35:30<16:42:17,  1.24s/it]  3%|▎         | 1679/50000 [35:31<16:41:45,  1.24s/it]  3%|▎         | 1680/50000 [35:32<16:41:12,  1.24s/it]                                                       {'loss': 143.3355, 'learning_rate': 1.3176000000000002e-05, 'epoch': 0.26}
  3%|▎         | 1680/50000 [35:32<16:41:12,  1.24s/it]  3%|▎         | 1681/50000 [35:34<16:41:18,  1.24s/it]  3%|▎         | 1682/50000 [35:35<16:41:13,  1.24s/it]  3%|▎         | 1683/50000 [35:36<16:41:03,  1.24s/it]  3%|▎         | 1684/50000 [35:37<16:40:32,  1.24s/it]  3%|▎         | 1685/50000 [35:39<16:40:29,  1.24s/it]  3%|▎         | 1686/50000 [35:40<16:40:18,  1.24s/it]  3%|▎         | 1687/50000 [35:41<16:40:17,  1.24s/it]  3%|▎         | 1688/50000 [35:42<16:39:59,  1.24s/it]  3%|▎         | 1689/50000 [35:44<16:39:50,  1.24s/it]  3%|▎         | 1690/50000 [35:45<16:39:47,  1.24s/it]                                                       {'loss': 147.7141, 'learning_rate': 1.3256e-05, 'epoch': 0.26}
  3%|▎         | 1690/50000 [35:45<16:39:47,  1.24s/it]  3%|▎         | 1691/50000 [35:46<16:40:12,  1.24s/it]  3%|▎         | 1692/50000 [35:47<16:39:58,  1.24s/it]  3%|▎         | 1693/50000 [35:48<16:39:43,  1.24s/it]  3%|▎         | 1694/50000 [35:50<16:39:25,  1.24s/it]  3%|▎         | 1695/50000 [35:51<16:39:59,  1.24s/it]  3%|▎         | 1696/50000 [35:52<16:39:43,  1.24s/it]  3%|▎         | 1697/50000 [35:53<16:39:27,  1.24s/it]  3%|▎         | 1698/50000 [35:55<16:39:14,  1.24s/it]  3%|▎         | 1699/50000 [35:56<16:39:16,  1.24s/it]  3%|▎         | 1700/50000 [35:57<16:39:18,  1.24s/it]                                                       {'loss': 169.675, 'learning_rate': 1.3336e-05, 'epoch': 0.26}
  3%|▎         | 1700/50000 [35:57<16:39:18,  1.24s/it]  3%|▎         | 1701/50000 [35:58<16:39:38,  1.24s/it]  3%|▎         | 1702/50000 [36:00<16:39:31,  1.24s/it]  3%|▎         | 1703/50000 [36:01<16:39:35,  1.24s/it]  3%|▎         | 1704/50000 [36:02<16:39:23,  1.24s/it]  3%|▎         | 1705/50000 [36:03<16:39:14,  1.24s/it]  3%|▎         | 1706/50000 [36:05<16:39:17,  1.24s/it]  3%|▎         | 1707/50000 [36:06<16:39:15,  1.24s/it]  3%|▎         | 1708/50000 [36:07<16:39:12,  1.24s/it]  3%|▎         | 1709/50000 [36:08<16:39:12,  1.24s/it]  3%|▎         | 1710/50000 [36:10<16:39:14,  1.24s/it]                                                       {'loss': 147.2406, 'learning_rate': 1.3416e-05, 'epoch': 0.27}
  3%|▎         | 1710/50000 [36:10<16:39:14,  1.24s/it]  3%|▎         | 1711/50000 [36:11<16:39:34,  1.24s/it]  3%|▎         | 1712/50000 [36:12<16:39:35,  1.24s/it]  3%|▎         | 1713/50000 [36:13<16:39:18,  1.24s/it]  3%|▎         | 1714/50000 [36:15<16:40:23,  1.24s/it]  3%|▎         | 1715/50000 [36:16<16:40:04,  1.24s/it]  3%|▎         | 1716/50000 [36:17<16:39:40,  1.24s/it]  3%|▎         | 1717/50000 [36:18<16:39:29,  1.24s/it]  3%|▎         | 1718/50000 [36:20<16:39:18,  1.24s/it]  3%|▎         | 1719/50000 [36:21<16:39:19,  1.24s/it]  3%|▎         | 1720/50000 [36:22<16:39:24,  1.24s/it]                                                       {'loss': 165.4031, 'learning_rate': 1.3496000000000001e-05, 'epoch': 0.27}
  3%|▎         | 1720/50000 [36:22<16:39:24,  1.24s/it]  3%|▎         | 1721/50000 [36:23<16:39:38,  1.24s/it]  3%|▎         | 1722/50000 [36:24<16:39:32,  1.24s/it]  3%|▎         | 1723/50000 [36:26<16:39:25,  1.24s/it]  3%|▎         | 1724/50000 [36:27<16:39:16,  1.24s/it]  3%|▎         | 1725/50000 [36:28<16:39:14,  1.24s/it]  3%|▎         | 1726/50000 [36:29<16:39:05,  1.24s/it]  3%|▎         | 1727/50000 [36:31<16:39:19,  1.24s/it]  3%|▎         | 1728/50000 [36:32<16:39:08,  1.24s/it]  3%|▎         | 1729/50000 [36:33<16:39:07,  1.24s/it]  3%|▎         | 1730/50000 [36:34<16:38:56,  1.24s/it]                                                       {'loss': 166.3547, 'learning_rate': 1.3576e-05, 'epoch': 0.27}
  3%|▎         | 1730/50000 [36:34<16:38:56,  1.24s/it]  3%|▎         | 1731/50000 [36:36<16:39:07,  1.24s/it]  3%|▎         | 1732/50000 [36:37<16:38:57,  1.24s/it]  3%|▎         | 1733/50000 [36:38<16:38:39,  1.24s/it]  3%|▎         | 1734/50000 [36:39<16:38:32,  1.24s/it]  3%|▎         | 1735/50000 [36:41<16:38:42,  1.24s/it]  3%|▎         | 1736/50000 [36:42<16:38:38,  1.24s/it]  3%|▎         | 1737/50000 [36:43<16:38:38,  1.24s/it]  3%|▎         | 1738/50000 [36:44<16:38:37,  1.24s/it]  3%|▎         | 1739/50000 [36:46<16:38:28,  1.24s/it]  3%|▎         | 1740/50000 [36:47<16:39:01,  1.24s/it]                                                       {'loss': 150.2141, 'learning_rate': 1.3656000000000001e-05, 'epoch': 0.27}
  3%|▎         | 1740/50000 [36:47<16:39:01,  1.24s/it]  3%|▎         | 1741/50000 [36:48<16:39:16,  1.24s/it][2023-07-03 12:36:59,588] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, but hysteresis is 2. Reducing hysteresis to 1
  3%|▎         | 1742/50000 [36:49<15:39:16,  1.17s/it]  3%|▎         | 1743/50000 [36:50<15:56:50,  1.19s/it]  3%|▎         | 1744/50000 [36:52<16:09:42,  1.21s/it]  3%|▎         | 1745/50000 [36:53<16:18:10,  1.22s/it]  3%|▎         | 1746/50000 [36:54<16:24:15,  1.22s/it]  3%|▎         | 1747/50000 [36:55<16:28:31,  1.23s/it]  3%|▎         | 1748/50000 [36:57<16:31:29,  1.23s/it]  3%|▎         | 1749/50000 [36:58<16:33:42,  1.24s/it]  4%|▎         | 1750/50000 [36:59<16:34:57,  1.24s/it]                                                       {'loss': 164.1391, 'learning_rate': 1.3728000000000001e-05, 'epoch': 0.27}
  4%|▎         | 1750/50000 [36:59<16:34:57,  1.24s/it]  4%|▎         | 1751/50000 [37:00<16:36:15,  1.24s/it]  4%|▎         | 1752/50000 [37:01<16:36:54,  1.24s/it]  4%|▎         | 1753/50000 [37:03<16:37:09,  1.24s/it]  4%|▎         | 1754/50000 [37:04<16:37:36,  1.24s/it]  4%|▎         | 1755/50000 [37:05<16:37:39,  1.24s/it]  4%|▎         | 1756/50000 [37:06<16:37:53,  1.24s/it]  4%|▎         | 1757/50000 [37:08<16:37:53,  1.24s/it]  4%|▎         | 1758/50000 [37:09<16:37:54,  1.24s/it]  4%|▎         | 1759/50000 [37:10<16:38:35,  1.24s/it]  4%|▎         | 1760/50000 [37:11<16:38:21,  1.24s/it]                                                       {'loss': 166.95, 'learning_rate': 1.3808e-05, 'epoch': 0.27}
  4%|▎         | 1760/50000 [37:11<16:38:21,  1.24s/it]  4%|▎         | 1761/50000 [37:13<16:38:31,  1.24s/it]  4%|▎         | 1762/50000 [37:14<16:38:32,  1.24s/it]  4%|▎         | 1763/50000 [37:15<16:38:23,  1.24s/it]  4%|▎         | 1764/50000 [37:16<16:38:10,  1.24s/it]  4%|▎         | 1765/50000 [37:18<16:38:17,  1.24s/it]  4%|▎         | 1766/50000 [37:19<16:38:10,  1.24s/it]  4%|▎         | 1767/50000 [37:20<16:38:16,  1.24s/it]  4%|▎         | 1768/50000 [37:21<16:38:11,  1.24s/it]  4%|▎         | 1769/50000 [37:23<16:38:09,  1.24s/it]  4%|▎         | 1770/50000 [37:24<16:38:13,  1.24s/it]                                                       {'loss': 161.7484, 'learning_rate': 1.3888000000000002e-05, 'epoch': 0.27}
  4%|▎         | 1770/50000 [37:24<16:38:13,  1.24s/it]  4%|▎         | 1771/50000 [37:25<16:38:15,  1.24s/it]  4%|▎         | 1772/50000 [37:26<16:38:12,  1.24s/it]  4%|▎         | 1773/50000 [37:28<16:37:58,  1.24s/it]  4%|▎         | 1774/50000 [37:29<16:38:00,  1.24s/it]  4%|▎         | 1775/50000 [37:30<16:38:04,  1.24s/it]  4%|▎         | 1776/50000 [37:31<16:38:14,  1.24s/it]  4%|▎         | 1777/50000 [37:33<16:38:13,  1.24s/it]  4%|▎         | 1778/50000 [37:34<16:37:53,  1.24s/it]  4%|▎         | 1779/50000 [37:35<16:37:50,  1.24s/it]  4%|▎         | 1780/50000 [37:36<16:37:48,  1.24s/it]                                                       {'loss': 155.8734, 'learning_rate': 1.3968e-05, 'epoch': 0.28}
  4%|▎         | 1780/50000 [37:36<16:37:48,  1.24s/it][2023-07-03 12:37:47,768] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  4%|▎         | 1781/50000 [37:37<15:38:04,  1.17s/it]  4%|▎         | 1782/50000 [37:38<15:55:52,  1.19s/it]  4%|▎         | 1783/50000 [37:40<16:08:28,  1.21s/it]  4%|▎         | 1784/50000 [37:41<16:17:16,  1.22s/it]  4%|▎         | 1785/50000 [37:42<16:23:36,  1.22s/it]  4%|▎         | 1786/50000 [37:43<16:27:48,  1.23s/it]  4%|▎         | 1787/50000 [37:45<16:30:46,  1.23s/it]  4%|▎         | 1788/50000 [37:46<16:32:48,  1.24s/it]  4%|▎         | 1789/50000 [37:47<16:34:07,  1.24s/it]  4%|▎         | 1790/50000 [37:48<16:35:01,  1.24s/it]                                                       {'loss': 168.4781, 'learning_rate': 1.4040000000000001e-05, 'epoch': 0.28}
  4%|▎         | 1790/50000 [37:48<16:35:01,  1.24s/it]  4%|▎         | 1791/50000 [37:50<16:36:10,  1.24s/it]  4%|▎         | 1792/50000 [37:51<16:36:38,  1.24s/it]  4%|▎         | 1793/50000 [37:52<16:36:56,  1.24s/it]  4%|▎         | 1794/50000 [37:53<16:36:53,  1.24s/it]  4%|▎         | 1795/50000 [37:55<16:36:50,  1.24s/it]  4%|▎         | 1796/50000 [37:56<16:36:59,  1.24s/it]  4%|▎         | 1797/50000 [37:57<16:37:00,  1.24s/it]  4%|▎         | 1798/50000 [37:58<16:37:03,  1.24s/it]  4%|▎         | 1799/50000 [38:00<16:36:59,  1.24s/it]  4%|▎         | 1800/50000 [38:01<16:37:00,  1.24s/it]                                                       {'loss': 145.6406, 'learning_rate': 1.412e-05, 'epoch': 0.28}
  4%|▎         | 1800/50000 [38:01<16:37:00,  1.24s/it]  4%|▎         | 1801/50000 [38:02<16:37:25,  1.24s/it]  4%|▎         | 1802/50000 [38:03<16:37:27,  1.24s/it]  4%|▎         | 1803/50000 [38:05<16:37:25,  1.24s/it]  4%|▎         | 1804/50000 [38:06<16:37:15,  1.24s/it]  4%|▎         | 1805/50000 [38:07<16:37:14,  1.24s/it]  4%|▎         | 1806/50000 [38:08<16:37:00,  1.24s/it]  4%|▎         | 1807/50000 [38:10<16:37:05,  1.24s/it]  4%|▎         | 1808/50000 [38:11<16:37:10,  1.24s/it]  4%|▎         | 1809/50000 [38:12<16:37:14,  1.24s/it]  4%|▎         | 1810/50000 [38:13<16:37:19,  1.24s/it]                                                       {'loss': 121.8906, 'learning_rate': 1.4200000000000001e-05, 'epoch': 0.28}
  4%|▎         | 1810/50000 [38:13<16:37:19,  1.24s/it]  4%|▎         | 1811/50000 [38:15<16:37:40,  1.24s/it]  4%|▎         | 1812/50000 [38:16<16:37:29,  1.24s/it]  4%|▎         | 1813/50000 [38:17<16:37:25,  1.24s/it]  4%|▎         | 1814/50000 [38:18<16:37:06,  1.24s/it]  4%|▎         | 1815/50000 [38:19<16:37:09,  1.24s/it]  4%|▎         | 1816/50000 [38:21<16:37:06,  1.24s/it]  4%|▎         | 1817/50000 [38:22<16:37:05,  1.24s/it]  4%|▎         | 1818/50000 [38:23<16:37:19,  1.24s/it]  4%|▎         | 1819/50000 [38:24<16:37:12,  1.24s/it]  4%|▎         | 1820/50000 [38:26<16:37:07,  1.24s/it]                                                       {'loss': 155.0172, 'learning_rate': 1.428e-05, 'epoch': 0.28}
  4%|▎         | 1820/50000 [38:26<16:37:07,  1.24s/it]  4%|▎         | 1821/50000 [38:27<16:37:12,  1.24s/it]  4%|▎         | 1822/50000 [38:28<16:37:09,  1.24s/it]  4%|▎         | 1823/50000 [38:29<16:36:58,  1.24s/it]  4%|▎         | 1824/50000 [38:31<16:36:53,  1.24s/it]  4%|▎         | 1825/50000 [38:32<16:37:16,  1.24s/it]  4%|▎         | 1826/50000 [38:33<16:37:15,  1.24s/it]  4%|▎         | 1827/50000 [38:34<16:37:17,  1.24s/it]  4%|▎         | 1828/50000 [38:36<16:36:46,  1.24s/it]  4%|▎         | 1829/50000 [38:37<16:36:42,  1.24s/it]  4%|▎         | 1830/50000 [38:38<16:36:40,  1.24s/it]                                                       {'loss': 147.0984, 'learning_rate': 1.4360000000000001e-05, 'epoch': 0.28}
  4%|▎         | 1830/50000 [38:38<16:36:40,  1.24s/it]  4%|▎         | 1831/50000 [38:39<16:36:48,  1.24s/it]  4%|▎         | 1832/50000 [38:41<16:36:42,  1.24s/it]  4%|▎         | 1833/50000 [38:42<16:36:23,  1.24s/it]  4%|▎         | 1834/50000 [38:43<16:36:38,  1.24s/it]  4%|▎         | 1835/50000 [38:44<16:36:48,  1.24s/it]  4%|▎         | 1836/50000 [38:46<16:36:27,  1.24s/it]  4%|▎         | 1837/50000 [38:47<16:36:29,  1.24s/it]  4%|▎         | 1838/50000 [38:48<16:36:28,  1.24s/it]  4%|▎         | 1839/50000 [38:49<16:36:23,  1.24s/it]  4%|▎         | 1840/50000 [38:51<16:36:15,  1.24s/it]                                                       {'loss': 129.6656, 'learning_rate': 1.444e-05, 'epoch': 0.29}
  4%|▎         | 1840/50000 [38:51<16:36:15,  1.24s/it]  4%|▎         | 1841/50000 [38:52<16:36:33,  1.24s/it]  4%|▎         | 1842/50000 [38:53<16:36:40,  1.24s/it]  4%|▎         | 1843/50000 [38:54<16:36:32,  1.24s/it]  4%|▎         | 1844/50000 [38:55<16:36:22,  1.24s/it]  4%|▎         | 1845/50000 [38:57<16:36:25,  1.24s/it]  4%|▎         | 1846/50000 [38:58<16:36:03,  1.24s/it]  4%|▎         | 1847/50000 [38:59<16:36:07,  1.24s/it]  4%|▎         | 1848/50000 [39:00<16:36:09,  1.24s/it]  4%|▎         | 1849/50000 [39:02<16:35:58,  1.24s/it]  4%|▎         | 1850/50000 [39:03<16:36:25,  1.24s/it]                                                       {'loss': 145.6586, 'learning_rate': 1.4520000000000002e-05, 'epoch': 0.29}
  4%|▎         | 1850/50000 [39:03<16:36:25,  1.24s/it]  4%|▎         | 1851/50000 [39:04<16:36:36,  1.24s/it]  4%|▎         | 1852/50000 [39:05<16:36:21,  1.24s/it]  4%|▎         | 1853/50000 [39:07<16:36:19,  1.24s/it]  4%|▎         | 1854/50000 [39:08<16:36:07,  1.24s/it]  4%|▎         | 1855/50000 [39:09<16:36:12,  1.24s/it]  4%|▎         | 1856/50000 [39:10<16:36:09,  1.24s/it]  4%|▎         | 1857/50000 [39:12<16:36:02,  1.24s/it]  4%|▎         | 1858/50000 [39:13<16:35:53,  1.24s/it]  4%|▎         | 1859/50000 [39:14<16:36:00,  1.24s/it]  4%|▎         | 1860/50000 [39:15<16:35:47,  1.24s/it]                                                       {'loss': 148.9355, 'learning_rate': 1.46e-05, 'epoch': 0.29}
  4%|▎         | 1860/50000 [39:15<16:35:47,  1.24s/it]  4%|▎         | 1861/50000 [39:17<16:36:10,  1.24s/it]  4%|▎         | 1862/50000 [39:18<16:36:02,  1.24s/it]  4%|▎         | 1863/50000 [39:19<16:36:07,  1.24s/it]  4%|▎         | 1864/50000 [39:20<16:36:14,  1.24s/it]  4%|▎         | 1865/50000 [39:22<16:36:05,  1.24s/it]  4%|▎         | 1866/50000 [39:23<16:35:49,  1.24s/it]  4%|▎         | 1867/50000 [39:24<16:35:53,  1.24s/it]  4%|▎         | 1868/50000 [39:25<16:35:52,  1.24s/it]  4%|▎         | 1869/50000 [39:27<16:35:46,  1.24s/it]  4%|▎         | 1870/50000 [39:28<16:35:35,  1.24s/it]                                                       {'loss': 153.6953, 'learning_rate': 1.4680000000000002e-05, 'epoch': 0.29}
  4%|▎         | 1870/50000 [39:28<16:35:35,  1.24s/it]  4%|▎         | 1871/50000 [39:29<16:35:58,  1.24s/it]  4%|▎         | 1872/50000 [39:30<16:35:52,  1.24s/it]  4%|▎         | 1873/50000 [39:31<16:35:46,  1.24s/it]  4%|▎         | 1874/50000 [39:33<16:35:51,  1.24s/it]  4%|▍         | 1875/50000 [39:34<16:35:54,  1.24s/it]  4%|▍         | 1876/50000 [39:35<16:36:11,  1.24s/it]  4%|▍         | 1877/50000 [39:36<16:35:52,  1.24s/it]  4%|▍         | 1878/50000 [39:38<16:35:49,  1.24s/it]  4%|▍         | 1879/50000 [39:39<16:35:42,  1.24s/it]  4%|▍         | 1880/50000 [39:40<16:35:30,  1.24s/it]                                                       {'loss': 152.3328, 'learning_rate': 1.4760000000000001e-05, 'epoch': 0.29}
  4%|▍         | 1880/50000 [39:40<16:35:30,  1.24s/it]  4%|▍         | 1881/50000 [39:41<16:35:48,  1.24s/it]  4%|▍         | 1882/50000 [39:43<16:35:37,  1.24s/it]  4%|▍         | 1883/50000 [39:44<16:35:51,  1.24s/it]  4%|▍         | 1884/50000 [39:45<16:35:41,  1.24s/it]  4%|▍         | 1885/50000 [39:46<16:35:35,  1.24s/it]  4%|▍         | 1886/50000 [39:48<16:35:34,  1.24s/it]  4%|▍         | 1887/50000 [39:49<16:35:21,  1.24s/it]  4%|▍         | 1888/50000 [39:50<16:35:32,  1.24s/it]  4%|▍         | 1889/50000 [39:51<16:35:47,  1.24s/it]  4%|▍         | 1890/50000 [39:53<16:35:43,  1.24s/it]                                                       {'loss': 178.5484, 'learning_rate': 1.4840000000000002e-05, 'epoch': 0.29}
  4%|▍         | 1890/50000 [39:53<16:35:43,  1.24s/it]  4%|▍         | 1891/50000 [39:54<16:35:49,  1.24s/it]  4%|▍         | 1892/50000 [39:55<16:35:36,  1.24s/it]  4%|▍         | 1893/50000 [39:56<16:35:45,  1.24s/it]  4%|▍         | 1894/50000 [39:58<16:35:33,  1.24s/it]  4%|▍         | 1895/50000 [39:59<16:35:23,  1.24s/it]  4%|▍         | 1896/50000 [40:00<16:35:22,  1.24s/it][2023-07-03 12:40:11,542] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, but hysteresis is 2. Reducing hysteresis to 1
  4%|▍         | 1897/50000 [40:01<15:35:42,  1.17s/it]  4%|▍         | 1898/50000 [40:02<15:53:23,  1.19s/it][2023-07-03 12:40:13,777] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  4%|▍         | 1899/50000 [40:03<15:06:34,  1.13s/it]  4%|▍         | 1900/50000 [40:05<15:33:03,  1.16s/it]                                                       {'loss': 153.8719, 'learning_rate': 1.4904e-05, 'epoch': 0.29}
  4%|▍         | 1900/50000 [40:05<15:33:03,  1.16s/it]  4%|▍         | 1901/50000 [40:06<15:52:01,  1.19s/it]  4%|▍         | 1902/50000 [40:07<16:05:09,  1.20s/it]  4%|▍         | 1903/50000 [40:08<16:14:04,  1.22s/it]  4%|▍         | 1904/50000 [40:09<16:20:14,  1.22s/it]  4%|▍         | 1905/50000 [40:11<16:24:33,  1.23s/it]  4%|▍         | 1906/50000 [40:12<16:28:03,  1.23s/it]  4%|▍         | 1907/50000 [40:13<16:30:13,  1.24s/it]  4%|▍         | 1908/50000 [40:14<16:31:59,  1.24s/it]  4%|▍         | 1909/50000 [40:16<16:32:56,  1.24s/it]  4%|▍         | 1910/50000 [40:17<16:33:28,  1.24s/it]                                                       {'loss': 253.6516, 'learning_rate': 1.4984000000000001e-05, 'epoch': 0.3}
  4%|▍         | 1910/50000 [40:17<16:33:28,  1.24s/it]  4%|▍         | 1911/50000 [40:18<16:34:16,  1.24s/it]  4%|▍         | 1912/50000 [40:19<16:34:46,  1.24s/it]  4%|▍         | 1913/50000 [40:21<16:34:34,  1.24s/it]  4%|▍         | 1914/50000 [40:22<16:34:37,  1.24s/it]  4%|▍         | 1915/50000 [40:23<16:34:44,  1.24s/it]  4%|▍         | 1916/50000 [40:24<16:34:33,  1.24s/it]  4%|▍         | 1917/50000 [40:26<16:34:27,  1.24s/it]  4%|▍         | 1918/50000 [40:27<16:34:24,  1.24s/it]  4%|▍         | 1919/50000 [40:28<16:34:36,  1.24s/it]  4%|▍         | 1920/50000 [40:29<16:34:30,  1.24s/it]                                                       {'loss': 182.2141, 'learning_rate': 1.5064e-05, 'epoch': 0.3}
  4%|▍         | 1920/50000 [40:29<16:34:30,  1.24s/it]  4%|▍         | 1921/50000 [40:31<16:34:50,  1.24s/it]  4%|▍         | 1922/50000 [40:32<16:35:08,  1.24s/it]  4%|▍         | 1923/50000 [40:33<16:34:49,  1.24s/it]  4%|▍         | 1924/50000 [40:34<16:34:49,  1.24s/it]  4%|▍         | 1925/50000 [40:36<16:34:44,  1.24s/it]  4%|▍         | 1926/50000 [40:37<16:34:38,  1.24s/it]  4%|▍         | 1927/50000 [40:38<16:34:27,  1.24s/it]  4%|▍         | 1928/50000 [40:39<16:34:28,  1.24s/it]  4%|▍         | 1929/50000 [40:41<16:34:27,  1.24s/it]  4%|▍         | 1930/50000 [40:42<16:34:16,  1.24s/it]                                                       {'loss': 184.9734, 'learning_rate': 1.5144000000000001e-05, 'epoch': 0.3}
  4%|▍         | 1930/50000 [40:42<16:34:16,  1.24s/it]  4%|▍         | 1931/50000 [40:43<16:34:59,  1.24s/it]  4%|▍         | 1932/50000 [40:44<16:34:52,  1.24s/it]  4%|▍         | 1933/50000 [40:45<16:34:40,  1.24s/it]  4%|▍         | 1934/50000 [40:47<16:34:33,  1.24s/it]  4%|▍         | 1935/50000 [40:48<16:34:30,  1.24s/it]  4%|▍         | 1936/50000 [40:49<16:34:25,  1.24s/it]  4%|▍         | 1937/50000 [40:50<16:34:16,  1.24s/it]  4%|▍         | 1938/50000 [40:52<16:34:10,  1.24s/it]  4%|▍         | 1939/50000 [40:53<16:34:19,  1.24s/it]  4%|▍         | 1940/50000 [40:54<16:34:42,  1.24s/it]                                                       {'loss': 167.7562, 'learning_rate': 1.5224e-05, 'epoch': 0.3}
  4%|▍         | 1940/50000 [40:54<16:34:42,  1.24s/it]  4%|▍         | 1941/50000 [40:55<16:34:59,  1.24s/it]  4%|▍         | 1942/50000 [40:57<16:34:37,  1.24s/it]  4%|▍         | 1943/50000 [40:58<16:34:40,  1.24s/it]  4%|▍         | 1944/50000 [40:59<16:34:33,  1.24s/it]  4%|▍         | 1945/50000 [41:00<16:34:21,  1.24s/it]  4%|▍         | 1946/50000 [41:02<16:34:06,  1.24s/it]  4%|▍         | 1947/50000 [41:03<16:34:05,  1.24s/it]  4%|▍         | 1948/50000 [41:04<16:33:57,  1.24s/it]  4%|▍         | 1949/50000 [41:05<16:33:56,  1.24s/it]  4%|▍         | 1950/50000 [41:07<16:33:43,  1.24s/it]                                                       {'loss': 156.5641, 'learning_rate': 1.5304e-05, 'epoch': 0.3}
  4%|▍         | 1950/50000 [41:07<16:33:43,  1.24s/it]  4%|▍         | 1951/50000 [41:08<16:34:13,  1.24s/it]  4%|▍         | 1952/50000 [41:09<16:34:22,  1.24s/it]  4%|▍         | 1953/50000 [41:10<16:34:15,  1.24s/it]  4%|▍         | 1954/50000 [41:12<16:34:00,  1.24s/it]  4%|▍         | 1955/50000 [41:13<16:33:53,  1.24s/it]  4%|▍         | 1956/50000 [41:14<16:33:52,  1.24s/it]  4%|▍         | 1957/50000 [41:15<16:34:01,  1.24s/it]  4%|▍         | 1958/50000 [41:17<16:34:00,  1.24s/it][2023-07-03 12:41:28,020] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
  4%|▍         | 1959/50000 [41:18<15:34:21,  1.17s/it]  4%|▍         | 1960/50000 [41:19<15:51:48,  1.19s/it]                                                       {'loss': 171.2234, 'learning_rate': 1.5376000000000003e-05, 'epoch': 0.3}
  4%|▍         | 1960/50000 [41:19<15:51:48,  1.19s/it]  4%|▍         | 1961/50000 [41:20<16:05:43,  1.21s/it]  4%|▍         | 1962/50000 [41:21<16:15:32,  1.22s/it]  4%|▍         | 1963/50000 [41:22<16:20:43,  1.22s/it]  4%|▍         | 1964/50000 [41:24<16:25:56,  1.23s/it]  4%|▍         | 1965/50000 [41:25<16:29:47,  1.24s/it]  4%|▍         | 1966/50000 [41:26<16:32:37,  1.24s/it]  4%|▍         | 1967/50000 [41:27<16:34:27,  1.24s/it]  4%|▍         | 1968/50000 [41:29<16:36:38,  1.24s/it]  4%|▍         | 1969/50000 [41:30<16:37:43,  1.25s/it]  4%|▍         | 1970/50000 [41:31<16:38:53,  1.25s/it]                                                       {'loss': 189.5563, 'learning_rate': 1.5456000000000002e-05, 'epoch': 0.31}
  4%|▍         | 1970/50000 [41:31<16:38:53,  1.25s/it]  4%|▍         | 1971/50000 [41:32<16:39:03,  1.25s/it]  4%|▍         | 1972/50000 [41:34<16:38:26,  1.25s/it]  4%|▍         | 1973/50000 [41:35<16:37:36,  1.25s/it]  4%|▍         | 1974/50000 [41:36<16:36:36,  1.25s/it]  4%|▍         | 1975/50000 [41:37<16:36:04,  1.24s/it]  4%|▍         | 1976/50000 [41:39<16:35:44,  1.24s/it]  4%|▍         | 1977/50000 [41:40<16:35:12,  1.24s/it]  4%|▍         | 1978/50000 [41:41<16:35:32,  1.24s/it]  4%|▍         | 1979/50000 [41:42<16:35:32,  1.24s/it]  4%|▍         | 1980/50000 [41:44<16:35:24,  1.24s/it]                                                       {'loss': 167.8828, 'learning_rate': 1.5536e-05, 'epoch': 0.31}
  4%|▍         | 1980/50000 [41:44<16:35:24,  1.24s/it]  4%|▍         | 1981/50000 [41:45<16:35:21,  1.24s/it]  4%|▍         | 1982/50000 [41:46<16:35:14,  1.24s/it]  4%|▍         | 1983/50000 [41:47<16:35:10,  1.24s/it]  4%|▍         | 1984/50000 [41:49<16:35:10,  1.24s/it]  4%|▍         | 1985/50000 [41:50<16:34:37,  1.24s/it]  4%|▍         | 1986/50000 [41:51<16:34:31,  1.24s/it]  4%|▍         | 1987/50000 [41:52<16:34:07,  1.24s/it]  4%|▍         | 1988/50000 [41:54<16:34:36,  1.24s/it]  4%|▍         | 1989/50000 [41:55<16:33:49,  1.24s/it]  4%|▍         | 1990/50000 [41:56<16:33:56,  1.24s/it]                                                       {'loss': 191.9422, 'learning_rate': 1.5616e-05, 'epoch': 0.31}
  4%|▍         | 1990/50000 [41:56<16:33:56,  1.24s/it]  4%|▍         | 1991/50000 [41:57<16:35:13,  1.24s/it]  4%|▍         | 1992/50000 [41:59<16:34:34,  1.24s/it]  4%|▍         | 1993/50000 [42:00<16:34:03,  1.24s/it]  4%|▍         | 1994/50000 [42:01<16:33:49,  1.24s/it]  4%|▍         | 1995/50000 [42:02<16:33:54,  1.24s/it]  4%|▍         | 1996/50000 [42:04<16:33:54,  1.24s/it]  4%|▍         | 1997/50000 [42:05<16:33:48,  1.24s/it]  4%|▍         | 1998/50000 [42:06<16:33:54,  1.24s/it]  4%|▍         | 1999/50000 [42:07<16:33:45,  1.24s/it][2023-07-03 12:42:18,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=38, lr=[1.5696000000000004e-05], mom=[(0.9, 0.999)]
[2023-07-03 12:42:19,028] [INFO] [timer.py:199:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=3.252369338808237, CurrSamplesPerSec=3.2404187053016487, MemAllocated=47.21GB, MaxMemAllocated=59.8GB
  4%|▍         | 2000/50000 [42:09<16:34:04,  1.24s/it]                                                       {'loss': 152.9875, 'learning_rate': 1.5696000000000004e-05, 'epoch': 0.31}
  4%|▍         | 2000/50000 [42:09<16:34:04,  1.24s/it][INFO|trainer.py:3129] 2023-07-03 12:42:19,036 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 12:42:19,036 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 12:42:19,036 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                       
                                             [A{'eval_loss': 181.375, 'eval_accuracy': 0.3910883636911034, 'eval_runtime': 3.1123, 'eval_samples_per_second': 8.354, 'eval_steps_per_second': 2.249, 'epoch': 0.31}
  4%|▍         | 2000/50000 [42:12<16:34:04,  1.24s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 12:42:22,149 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000
[INFO|trainer.py:2880] 2023-07-03 12:42:22,162 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 12:42:32,755 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 12:42:32,755 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/special_tokens_map.json
[2023-07-03 12:42:32,758] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 12:42:32,782] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt
[2023-07-03 12:42:32,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...
[2023-07-03 12:42:43,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.
[2023-07-03 12:42:43,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 12:43:02,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 12:43:02,264] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 12:43:02,264] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 12:43:02,271 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-1000] due to args.save_total_limit
  4%|▍         | 2001/50000 [43:02<226:57:24, 17.02s/it]  4%|▍         | 2002/50000 [43:04<163:50:28, 12.29s/it]  4%|▍         | 2003/50000 [43:05<119:39:36,  8.98s/it]  4%|▍         | 2004/50000 [43:06<88:43:49,  6.66s/it]   4%|▍         | 2005/50000 [43:07<67:04:13,  5.03s/it]  4%|▍         | 2006/50000 [43:09<51:54:24,  3.89s/it]  4%|▍         | 2007/50000 [43:10<41:17:32,  3.10s/it]  4%|▍         | 2008/50000 [43:11<33:51:27,  2.54s/it]  4%|▍         | 2009/50000 [43:12<28:38:54,  2.15s/it]  4%|▍         | 2010/50000 [43:14<25:00:45,  1.88s/it]                                                       {'loss': 181.3359, 'learning_rate': 1.5776e-05, 'epoch': 0.31}
  4%|▍         | 2010/50000 [43:14<25:00:45,  1.88s/it]  4%|▍         | 2011/50000 [43:15<22:27:53,  1.69s/it][2023-07-03 12:43:26,264] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  4%|▍         | 2012/50000 [43:16<19:40:54,  1.48s/it]  4%|▍         | 2013/50000 [43:17<18:43:33,  1.40s/it]  4%|▍         | 2014/50000 [43:18<18:03:23,  1.35s/it]  4%|▍         | 2015/50000 [43:19<17:36:00,  1.32s/it]  4%|▍         | 2016/50000 [43:21<17:16:04,  1.30s/it]  4%|▍         | 2017/50000 [43:22<17:02:08,  1.28s/it]  4%|▍         | 2018/50000 [43:23<16:52:44,  1.27s/it]  4%|▍         | 2019/50000 [43:24<16:46:01,  1.26s/it]  4%|▍         | 2020/50000 [43:26<16:41:05,  1.25s/it]                                                       {'loss': 160.6047, 'learning_rate': 1.5848e-05, 'epoch': 0.31}
  4%|▍         | 2020/50000 [43:26<16:41:05,  1.25s/it]  4%|▍         | 2021/50000 [43:27<16:38:20,  1.25s/it]  4%|▍         | 2022/50000 [43:28<16:35:42,  1.25s/it]  4%|▍         | 2023/50000 [43:29<16:33:59,  1.24s/it]  4%|▍         | 2024/50000 [43:31<16:32:42,  1.24s/it]  4%|▍         | 2025/50000 [43:32<16:31:39,  1.24s/it]  4%|▍         | 2026/50000 [43:33<16:31:27,  1.24s/it]  4%|▍         | 2027/50000 [43:34<16:30:47,  1.24s/it]  4%|▍         | 2028/50000 [43:36<16:31:57,  1.24s/it]  4%|▍         | 2029/50000 [43:37<16:31:08,  1.24s/it]  4%|▍         | 2030/50000 [43:38<16:30:32,  1.24s/it]                                                       {'loss': 176.9234, 'learning_rate': 1.5928e-05, 'epoch': 0.31}
  4%|▍         | 2030/50000 [43:38<16:30:32,  1.24s/it]  4%|▍         | 2031/50000 [43:39<16:30:32,  1.24s/it]  4%|▍         | 2032/50000 [43:41<16:30:16,  1.24s/it]  4%|▍         | 2033/50000 [43:42<16:31:33,  1.24s/it]  4%|▍         | 2034/50000 [43:43<16:32:10,  1.24s/it]  4%|▍         | 2035/50000 [43:44<16:33:06,  1.24s/it]  4%|▍         | 2036/50000 [43:45<16:32:21,  1.24s/it]  4%|▍         | 2037/50000 [43:47<16:31:54,  1.24s/it]  4%|▍         | 2038/50000 [43:48<16:31:34,  1.24s/it]  4%|▍         | 2039/50000 [43:49<16:31:59,  1.24s/it]  4%|▍         | 2040/50000 [43:50<16:32:03,  1.24s/it]                                                       {'loss': 252.6828, 'learning_rate': 1.6008e-05, 'epoch': 0.32}
  4%|▍         | 2040/50000 [43:50<16:32:03,  1.24s/it]  4%|▍         | 2041/50000 [43:52<16:32:14,  1.24s/it]  4%|▍         | 2042/50000 [43:53<16:32:46,  1.24s/it]  4%|▍         | 2043/50000 [43:54<16:33:00,  1.24s/it]  4%|▍         | 2044/50000 [43:55<16:33:05,  1.24s/it]  4%|▍         | 2045/50000 [43:57<16:32:57,  1.24s/it]  4%|▍         | 2046/50000 [43:58<16:32:41,  1.24s/it]  4%|▍         | 2047/50000 [43:59<16:33:01,  1.24s/it]  4%|▍         | 2048/50000 [44:00<16:33:13,  1.24s/it]  4%|▍         | 2049/50000 [44:02<16:33:04,  1.24s/it]  4%|▍         | 2050/50000 [44:03<16:33:23,  1.24s/it]                                                       {'loss': 160.3766, 'learning_rate': 1.6088000000000002e-05, 'epoch': 0.32}
  4%|▍         | 2050/50000 [44:03<16:33:23,  1.24s/it]  4%|▍         | 2051/50000 [44:04<16:33:48,  1.24s/it]  4%|▍         | 2052/50000 [44:05<16:33:05,  1.24s/it]  4%|▍         | 2053/50000 [44:07<16:33:49,  1.24s/it]  4%|▍         | 2054/50000 [44:08<16:33:36,  1.24s/it]  4%|▍         | 2055/50000 [44:09<16:33:22,  1.24s/it]  4%|▍         | 2056/50000 [44:10<16:32:56,  1.24s/it]  4%|▍         | 2057/50000 [44:12<16:32:51,  1.24s/it]  4%|▍         | 2058/50000 [44:13<16:32:39,  1.24s/it]  4%|▍         | 2059/50000 [44:14<16:32:43,  1.24s/it]  4%|▍         | 2060/50000 [44:15<16:32:59,  1.24s/it]                                                       {'loss': 177.2969, 'learning_rate': 1.6168000000000002e-05, 'epoch': 0.32}
  4%|▍         | 2060/50000 [44:15<16:32:59,  1.24s/it]  4%|▍         | 2061/50000 [44:17<16:32:45,  1.24s/it]  4%|▍         | 2062/50000 [44:18<16:32:29,  1.24s/it]  4%|▍         | 2063/50000 [44:19<16:32:52,  1.24s/it]  4%|▍         | 2064/50000 [44:20<16:32:25,  1.24s/it]  4%|▍         | 2065/50000 [44:22<16:32:28,  1.24s/it]  4%|▍         | 2066/50000 [44:23<16:32:25,  1.24s/it]  4%|▍         | 2067/50000 [44:24<16:32:24,  1.24s/it]  4%|▍         | 2068/50000 [44:25<16:33:11,  1.24s/it]  4%|▍         | 2069/50000 [44:26<16:32:44,  1.24s/it]  4%|▍         | 2070/50000 [44:28<16:33:19,  1.24s/it]                                                       {'loss': 180.1813, 'learning_rate': 1.6248e-05, 'epoch': 0.32}
  4%|▍         | 2070/50000 [44:28<16:33:19,  1.24s/it]  4%|▍         | 2071/50000 [44:29<16:33:15,  1.24s/it]  4%|▍         | 2072/50000 [44:30<16:33:04,  1.24s/it]  4%|▍         | 2073/50000 [44:31<16:32:55,  1.24s/it]  4%|▍         | 2074/50000 [44:33<16:33:21,  1.24s/it]  4%|▍         | 2075/50000 [44:34<16:34:28,  1.25s/it]  4%|▍         | 2076/50000 [44:35<16:35:35,  1.25s/it]  4%|▍         | 2077/50000 [44:36<16:35:14,  1.25s/it]  4%|▍         | 2078/50000 [44:38<16:34:42,  1.25s/it]  4%|▍         | 2079/50000 [44:39<16:34:11,  1.24s/it]  4%|▍         | 2080/50000 [44:40<16:33:42,  1.24s/it]                                                       {'loss': 160.5844, 'learning_rate': 1.6328e-05, 'epoch': 0.32}
  4%|▍         | 2080/50000 [44:40<16:33:42,  1.24s/it]  4%|▍         | 2081/50000 [44:41<16:33:54,  1.24s/it]  4%|▍         | 2082/50000 [44:43<16:33:31,  1.24s/it]  4%|▍         | 2083/50000 [44:44<16:33:51,  1.24s/it]  4%|▍         | 2084/50000 [44:45<16:33:50,  1.24s/it]  4%|▍         | 2085/50000 [44:46<16:33:41,  1.24s/it]  4%|▍         | 2086/50000 [44:48<16:33:04,  1.24s/it]  4%|▍         | 2087/50000 [44:49<16:33:20,  1.24s/it]  4%|▍         | 2088/50000 [44:50<16:34:29,  1.25s/it]  4%|▍         | 2089/50000 [44:51<16:34:15,  1.25s/it]  4%|▍         | 2090/50000 [44:53<16:34:07,  1.24s/it]                                                       {'loss': 162.6781, 'learning_rate': 1.6408000000000003e-05, 'epoch': 0.32}
  4%|▍         | 2090/50000 [44:53<16:34:07,  1.24s/it]  4%|▍         | 2091/50000 [44:54<16:34:14,  1.25s/it]  4%|▍         | 2092/50000 [44:55<16:35:36,  1.25s/it]  4%|▍         | 2093/50000 [44:56<16:35:46,  1.25s/it]  4%|▍         | 2094/50000 [44:58<16:35:48,  1.25s/it]  4%|▍         | 2095/50000 [44:59<16:35:14,  1.25s/it]  4%|▍         | 2096/50000 [45:00<16:34:14,  1.25s/it]  4%|▍         | 2097/50000 [45:01<16:33:46,  1.24s/it]  4%|▍         | 2098/50000 [45:03<16:34:09,  1.25s/it]  4%|▍         | 2099/50000 [45:04<16:35:14,  1.25s/it]  4%|▍         | 2100/50000 [45:05<16:34:39,  1.25s/it]                                                       {'loss': 196.1, 'learning_rate': 1.6488000000000002e-05, 'epoch': 0.33}
  4%|▍         | 2100/50000 [45:05<16:34:39,  1.25s/it]  4%|▍         | 2101/50000 [45:06<16:34:24,  1.25s/it]  4%|▍         | 2102/50000 [45:08<16:34:55,  1.25s/it]  4%|▍         | 2103/50000 [45:09<16:35:33,  1.25s/it]  4%|▍         | 2104/50000 [45:10<16:35:15,  1.25s/it]  4%|▍         | 2105/50000 [45:11<16:34:09,  1.25s/it]  4%|▍         | 2106/50000 [45:13<16:33:21,  1.24s/it]  4%|▍         | 2107/50000 [45:14<16:33:51,  1.25s/it]  4%|▍         | 2108/50000 [45:15<16:33:10,  1.24s/it]  4%|▍         | 2109/50000 [45:16<16:32:52,  1.24s/it]  4%|▍         | 2110/50000 [45:18<16:32:34,  1.24s/it]                                                       {'loss': 222.1359, 'learning_rate': 1.6568e-05, 'epoch': 0.33}
  4%|▍         | 2110/50000 [45:18<16:32:34,  1.24s/it]  4%|▍         | 2111/50000 [45:19<16:32:57,  1.24s/it]  4%|▍         | 2112/50000 [45:20<16:34:32,  1.25s/it]  4%|▍         | 2113/50000 [45:21<16:33:59,  1.25s/it]  4%|▍         | 2114/50000 [45:23<16:33:36,  1.24s/it]  4%|▍         | 2115/50000 [45:24<16:32:50,  1.24s/it]  4%|▍         | 2116/50000 [45:25<16:34:47,  1.25s/it]  4%|▍         | 2117/50000 [45:26<16:34:07,  1.25s/it]  4%|▍         | 2118/50000 [45:28<16:33:42,  1.25s/it]  4%|▍         | 2119/50000 [45:29<16:33:36,  1.25s/it]  4%|▍         | 2120/50000 [45:30<16:32:36,  1.24s/it]                                                       {'loss': 187.1969, 'learning_rate': 1.6648e-05, 'epoch': 0.33}
  4%|▍         | 2120/50000 [45:30<16:32:36,  1.24s/it]  4%|▍         | 2121/50000 [45:31<16:33:12,  1.24s/it]  4%|▍         | 2122/50000 [45:32<16:35:23,  1.25s/it]  4%|▍         | 2123/50000 [45:34<16:35:38,  1.25s/it]  4%|▍         | 2124/50000 [45:35<16:35:35,  1.25s/it]  4%|▍         | 2125/50000 [45:36<16:34:53,  1.25s/it]  4%|▍         | 2126/50000 [45:37<16:33:34,  1.25s/it]  4%|▍         | 2127/50000 [45:39<16:33:05,  1.24s/it]  4%|▍         | 2128/50000 [45:40<16:32:36,  1.24s/it]  4%|▍         | 2129/50000 [45:41<16:32:35,  1.24s/it]  4%|▍         | 2130/50000 [45:42<16:31:56,  1.24s/it]                                                       {'loss': 169.4422, 'learning_rate': 1.6728000000000003e-05, 'epoch': 0.33}
  4%|▍         | 2130/50000 [45:42<16:31:56,  1.24s/it]  4%|▍         | 2131/50000 [45:44<16:32:01,  1.24s/it]  4%|▍         | 2132/50000 [45:45<16:32:50,  1.24s/it]  4%|▍         | 2133/50000 [45:46<16:32:39,  1.24s/it]  4%|▍         | 2134/50000 [45:47<16:32:06,  1.24s/it]  4%|▍         | 2135/50000 [45:49<16:31:41,  1.24s/it]  4%|▍         | 2136/50000 [45:50<16:31:34,  1.24s/it]  4%|▍         | 2137/50000 [45:51<16:31:38,  1.24s/it]  4%|▍         | 2138/50000 [45:52<16:31:44,  1.24s/it]  4%|▍         | 2139/50000 [45:54<16:31:18,  1.24s/it]  4%|▍         | 2140/50000 [45:55<16:31:40,  1.24s/it]                                                       {'loss': 188.6016, 'learning_rate': 1.6808000000000002e-05, 'epoch': 0.33}
  4%|▍         | 2140/50000 [45:55<16:31:40,  1.24s/it]  4%|▍         | 2141/50000 [45:56<16:31:33,  1.24s/it]  4%|▍         | 2142/50000 [45:57<16:31:06,  1.24s/it]  4%|▍         | 2143/50000 [45:59<16:31:16,  1.24s/it]  4%|▍         | 2144/50000 [46:00<16:30:45,  1.24s/it]  4%|▍         | 2145/50000 [46:01<16:30:48,  1.24s/it]  4%|▍         | 2146/50000 [46:02<16:30:43,  1.24s/it]  4%|▍         | 2147/50000 [46:04<16:31:07,  1.24s/it]  4%|▍         | 2148/50000 [46:05<16:31:48,  1.24s/it]  4%|▍         | 2149/50000 [46:06<16:31:20,  1.24s/it]  4%|▍         | 2150/50000 [46:07<16:31:33,  1.24s/it]                                                       {'loss': 206.2984, 'learning_rate': 1.6888e-05, 'epoch': 0.33}
  4%|▍         | 2150/50000 [46:07<16:31:33,  1.24s/it]  4%|▍         | 2151/50000 [46:09<16:31:17,  1.24s/it]  4%|▍         | 2152/50000 [46:10<16:30:50,  1.24s/it]  4%|▍         | 2153/50000 [46:11<16:31:02,  1.24s/it]  4%|▍         | 2154/50000 [46:12<16:30:31,  1.24s/it]  4%|▍         | 2155/50000 [46:14<16:30:10,  1.24s/it]  4%|▍         | 2156/50000 [46:15<16:30:00,  1.24s/it][2023-07-03 12:46:26,265] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, but hysteresis is 2. Reducing hysteresis to 1
  4%|▍         | 2157/50000 [46:16<15:30:08,  1.17s/it]  4%|▍         | 2158/50000 [46:17<15:48:22,  1.19s/it]  4%|▍         | 2159/50000 [46:18<16:00:43,  1.20s/it]  4%|▍         | 2160/50000 [46:19<16:09:16,  1.22s/it]                                                       {'loss': 212.6891, 'learning_rate': 1.696e-05, 'epoch': 0.33}
  4%|▍         | 2160/50000 [46:19<16:09:16,  1.22s/it]  4%|▍         | 2161/50000 [46:21<16:15:33,  1.22s/it]  4%|▍         | 2162/50000 [46:22<16:19:25,  1.23s/it]  4%|▍         | 2163/50000 [46:23<16:22:41,  1.23s/it]  4%|▍         | 2164/50000 [46:24<16:25:17,  1.24s/it]  4%|▍         | 2165/50000 [46:26<16:26:57,  1.24s/it]  4%|▍         | 2166/50000 [46:27<16:27:46,  1.24s/it]  4%|▍         | 2167/50000 [46:28<16:28:06,  1.24s/it]  4%|▍         | 2168/50000 [46:29<16:28:54,  1.24s/it]  4%|▍         | 2169/50000 [46:31<16:29:38,  1.24s/it]  4%|▍         | 2170/50000 [46:32<16:29:27,  1.24s/it]                                                       {'loss': 145.8156, 'learning_rate': 1.704e-05, 'epoch': 0.34}
  4%|▍         | 2170/50000 [46:32<16:29:27,  1.24s/it]  4%|▍         | 2171/50000 [46:33<16:29:40,  1.24s/it]  4%|▍         | 2172/50000 [46:34<16:29:41,  1.24s/it]  4%|▍         | 2173/50000 [46:36<16:29:30,  1.24s/it]  4%|▍         | 2174/50000 [46:37<16:29:20,  1.24s/it]  4%|▍         | 2175/50000 [46:38<16:29:21,  1.24s/it]  4%|▍         | 2176/50000 [46:39<16:29:32,  1.24s/it]  4%|▍         | 2177/50000 [46:41<16:29:43,  1.24s/it]  4%|▍         | 2178/50000 [46:42<16:29:57,  1.24s/it]  4%|▍         | 2179/50000 [46:43<16:29:41,  1.24s/it]  4%|▍         | 2180/50000 [46:44<16:29:51,  1.24s/it]                                                       {'loss': 188.6813, 'learning_rate': 1.7120000000000002e-05, 'epoch': 0.34}
  4%|▍         | 2180/50000 [46:44<16:29:51,  1.24s/it]  4%|▍         | 2181/50000 [46:46<16:29:50,  1.24s/it]  4%|▍         | 2182/50000 [46:47<16:29:41,  1.24s/it]  4%|▍         | 2183/50000 [46:48<16:29:28,  1.24s/it]  4%|▍         | 2184/50000 [46:49<16:29:17,  1.24s/it]  4%|▍         | 2185/50000 [46:51<16:29:06,  1.24s/it]  4%|▍         | 2186/50000 [46:52<16:29:16,  1.24s/it]  4%|▍         | 2187/50000 [46:53<16:29:06,  1.24s/it]  4%|▍         | 2188/50000 [46:54<16:29:20,  1.24s/it]  4%|▍         | 2189/50000 [46:55<16:29:15,  1.24s/it]  4%|▍         | 2190/50000 [46:57<16:29:35,  1.24s/it]                                                       {'loss': 142.8328, 'learning_rate': 1.72e-05, 'epoch': 0.34}
  4%|▍         | 2190/50000 [46:57<16:29:35,  1.24s/it]  4%|▍         | 2191/50000 [46:58<16:29:34,  1.24s/it]  4%|▍         | 2192/50000 [46:59<16:29:23,  1.24s/it]  4%|▍         | 2193/50000 [47:00<16:29:23,  1.24s/it]  4%|▍         | 2194/50000 [47:02<16:29:20,  1.24s/it]  4%|▍         | 2195/50000 [47:03<16:29:13,  1.24s/it]  4%|▍         | 2196/50000 [47:04<16:29:26,  1.24s/it]  4%|▍         | 2197/50000 [47:05<16:29:22,  1.24s/it]  4%|▍         | 2198/50000 [47:07<16:29:25,  1.24s/it]  4%|▍         | 2199/50000 [47:08<16:29:16,  1.24s/it]  4%|▍         | 2200/50000 [47:09<16:29:24,  1.24s/it]                                                       {'loss': 143.8031, 'learning_rate': 1.728e-05, 'epoch': 0.34}
  4%|▍         | 2200/50000 [47:09<16:29:24,  1.24s/it]  4%|▍         | 2201/50000 [47:10<16:30:13,  1.24s/it]  4%|▍         | 2202/50000 [47:12<16:30:43,  1.24s/it]  4%|▍         | 2203/50000 [47:13<16:30:33,  1.24s/it]  4%|▍         | 2204/50000 [47:14<16:30:12,  1.24s/it]  4%|▍         | 2205/50000 [47:15<16:30:18,  1.24s/it]  4%|▍         | 2206/50000 [47:17<16:31:05,  1.24s/it]  4%|▍         | 2207/50000 [47:18<16:30:30,  1.24s/it]  4%|▍         | 2208/50000 [47:19<16:30:44,  1.24s/it]  4%|▍         | 2209/50000 [47:20<16:31:53,  1.25s/it]  4%|▍         | 2210/50000 [47:22<16:32:00,  1.25s/it]                                                       {'loss': 167.7781, 'learning_rate': 1.736e-05, 'epoch': 0.34}
  4%|▍         | 2210/50000 [47:22<16:32:00,  1.25s/it]  4%|▍         | 2211/50000 [47:23<16:31:21,  1.24s/it]  4%|▍         | 2212/50000 [47:24<16:30:19,  1.24s/it]  4%|▍         | 2213/50000 [47:25<16:29:31,  1.24s/it]  4%|▍         | 2214/50000 [47:27<16:29:19,  1.24s/it]  4%|▍         | 2215/50000 [47:28<16:28:41,  1.24s/it]  4%|▍         | 2216/50000 [47:29<16:28:14,  1.24s/it]  4%|▍         | 2217/50000 [47:30<16:28:07,  1.24s/it]  4%|▍         | 2218/50000 [47:32<16:28:18,  1.24s/it]  4%|▍         | 2219/50000 [47:33<16:27:47,  1.24s/it]  4%|▍         | 2220/50000 [47:34<16:28:03,  1.24s/it]                                                       {'loss': 175.4844, 'learning_rate': 1.7440000000000002e-05, 'epoch': 0.34}
  4%|▍         | 2220/50000 [47:34<16:28:03,  1.24s/it]  4%|▍         | 2221/50000 [47:35<16:28:13,  1.24s/it]  4%|▍         | 2222/50000 [47:36<16:28:45,  1.24s/it]  4%|▍         | 2223/50000 [47:38<16:28:27,  1.24s/it]  4%|▍         | 2224/50000 [47:39<16:27:59,  1.24s/it]  4%|▍         | 2225/50000 [47:40<16:27:45,  1.24s/it]  4%|▍         | 2226/50000 [47:41<16:28:04,  1.24s/it][2023-07-03 12:47:52,950] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  4%|▍         | 2227/50000 [47:42<15:28:02,  1.17s/it]  4%|▍         | 2228/50000 [47:44<15:45:44,  1.19s/it]  4%|▍         | 2229/50000 [47:45<15:58:20,  1.20s/it]  4%|▍         | 2230/50000 [47:46<16:06:45,  1.21s/it]                                                       {'loss': 149.8297, 'learning_rate': 1.7512e-05, 'epoch': 0.35}
  4%|▍         | 2230/50000 [47:46<16:06:45,  1.21s/it]  4%|▍         | 2231/50000 [47:47<16:13:15,  1.22s/it]  4%|▍         | 2232/50000 [47:49<16:17:20,  1.23s/it]  4%|▍         | 2233/50000 [47:50<16:20:15,  1.23s/it]  4%|▍         | 2234/50000 [47:51<16:22:24,  1.23s/it]  4%|▍         | 2235/50000 [47:52<16:23:48,  1.24s/it]  4%|▍         | 2236/50000 [47:54<16:24:48,  1.24s/it]  4%|▍         | 2237/50000 [47:55<16:25:46,  1.24s/it]  4%|▍         | 2238/50000 [47:56<16:26:16,  1.24s/it]  4%|▍         | 2239/50000 [47:57<16:26:37,  1.24s/it]  4%|▍         | 2240/50000 [47:59<16:26:41,  1.24s/it]                                                       {'loss': 158.4781, 'learning_rate': 1.7592000000000004e-05, 'epoch': 0.35}
  4%|▍         | 2240/50000 [47:59<16:26:41,  1.24s/it]  4%|▍         | 2241/50000 [48:00<16:27:01,  1.24s/it]  4%|▍         | 2242/50000 [48:01<16:26:47,  1.24s/it]  4%|▍         | 2243/50000 [48:02<16:26:38,  1.24s/it]  4%|▍         | 2244/50000 [48:04<16:26:43,  1.24s/it]  4%|▍         | 2245/50000 [48:05<16:26:41,  1.24s/it]  4%|▍         | 2246/50000 [48:06<16:26:37,  1.24s/it]  4%|▍         | 2247/50000 [48:07<16:26:53,  1.24s/it]  4%|▍         | 2248/50000 [48:08<16:26:54,  1.24s/it]  4%|▍         | 2249/50000 [48:10<16:27:05,  1.24s/it]  4%|▍         | 2250/50000 [48:11<16:27:36,  1.24s/it]                                                       {'loss': 131.1844, 'learning_rate': 1.7672000000000003e-05, 'epoch': 0.35}
  4%|▍         | 2250/50000 [48:11<16:27:36,  1.24s/it]  5%|▍         | 2251/50000 [48:12<16:27:37,  1.24s/it]  5%|▍         | 2252/50000 [48:13<16:27:31,  1.24s/it]  5%|▍         | 2253/50000 [48:15<16:27:16,  1.24s/it]  5%|▍         | 2254/50000 [48:16<16:26:56,  1.24s/it]  5%|▍         | 2255/50000 [48:17<16:27:10,  1.24s/it]  5%|▍         | 2256/50000 [48:18<16:27:00,  1.24s/it]  5%|▍         | 2257/50000 [48:20<16:26:57,  1.24s/it]  5%|▍         | 2258/50000 [48:21<16:26:48,  1.24s/it]  5%|▍         | 2259/50000 [48:22<16:26:41,  1.24s/it]  5%|▍         | 2260/50000 [48:23<16:26:35,  1.24s/it]                                                       {'loss': 140.9094, 'learning_rate': 1.7752e-05, 'epoch': 0.35}
  5%|▍         | 2260/50000 [48:23<16:26:35,  1.24s/it]  5%|▍         | 2261/50000 [48:25<16:27:11,  1.24s/it]  5%|▍         | 2262/50000 [48:26<16:27:02,  1.24s/it]  5%|▍         | 2263/50000 [48:27<16:26:56,  1.24s/it]  5%|▍         | 2264/50000 [48:28<16:27:19,  1.24s/it]  5%|▍         | 2265/50000 [48:30<16:26:47,  1.24s/it]  5%|▍         | 2266/50000 [48:31<16:26:45,  1.24s/it]  5%|▍         | 2267/50000 [48:32<16:26:40,  1.24s/it]  5%|▍         | 2268/50000 [48:33<16:26:57,  1.24s/it]  5%|▍         | 2269/50000 [48:35<16:26:49,  1.24s/it]  5%|▍         | 2270/50000 [48:36<16:26:51,  1.24s/it]                                                       {'loss': 172.5109, 'learning_rate': 1.7832e-05, 'epoch': 0.35}
  5%|▍         | 2270/50000 [48:36<16:26:51,  1.24s/it]  5%|▍         | 2271/50000 [48:37<16:26:58,  1.24s/it]  5%|▍         | 2272/50000 [48:38<16:26:48,  1.24s/it]  5%|▍         | 2273/50000 [48:39<16:26:52,  1.24s/it]  5%|▍         | 2274/50000 [48:41<16:26:43,  1.24s/it]  5%|▍         | 2275/50000 [48:42<16:27:00,  1.24s/it]  5%|▍         | 2276/50000 [48:43<16:26:57,  1.24s/it]  5%|▍         | 2277/50000 [48:44<16:27:01,  1.24s/it]  5%|▍         | 2278/50000 [48:46<16:26:38,  1.24s/it]  5%|▍         | 2279/50000 [48:47<16:26:25,  1.24s/it]  5%|▍         | 2280/50000 [48:48<16:26:35,  1.24s/it]                                                       {'loss': 163.0656, 'learning_rate': 1.7912e-05, 'epoch': 0.35}
  5%|▍         | 2280/50000 [48:48<16:26:35,  1.24s/it]  5%|▍         | 2281/50000 [48:49<16:26:48,  1.24s/it]  5%|▍         | 2282/50000 [48:51<16:26:31,  1.24s/it]  5%|▍         | 2283/50000 [48:52<16:26:21,  1.24s/it]  5%|▍         | 2284/50000 [48:53<16:26:05,  1.24s/it]  5%|▍         | 2285/50000 [48:54<16:26:12,  1.24s/it]  5%|▍         | 2286/50000 [48:56<16:26:14,  1.24s/it]  5%|▍         | 2287/50000 [48:57<16:25:53,  1.24s/it]  5%|▍         | 2288/50000 [48:58<16:26:11,  1.24s/it]  5%|▍         | 2289/50000 [48:59<16:26:01,  1.24s/it]  5%|▍         | 2290/50000 [49:01<16:25:56,  1.24s/it]                                                       {'loss': 151.7734, 'learning_rate': 1.7992e-05, 'epoch': 0.35}
  5%|▍         | 2290/50000 [49:01<16:25:56,  1.24s/it]  5%|▍         | 2291/50000 [49:02<16:26:00,  1.24s/it]  5%|▍         | 2292/50000 [49:03<16:26:00,  1.24s/it]  5%|▍         | 2293/50000 [49:04<16:26:07,  1.24s/it]  5%|▍         | 2294/50000 [49:06<16:26:36,  1.24s/it]  5%|▍         | 2295/50000 [49:07<16:26:10,  1.24s/it]  5%|▍         | 2296/50000 [49:08<16:26:15,  1.24s/it]  5%|▍         | 2297/50000 [49:09<16:26:12,  1.24s/it]  5%|▍         | 2298/50000 [49:11<16:26:19,  1.24s/it]  5%|▍         | 2299/50000 [49:12<16:25:57,  1.24s/it]  5%|▍         | 2300/50000 [49:13<16:25:52,  1.24s/it]                                                       {'loss': 167.9789, 'learning_rate': 1.8072e-05, 'epoch': 0.36}
  5%|▍         | 2300/50000 [49:13<16:25:52,  1.24s/it]  5%|▍         | 2301/50000 [49:14<16:26:08,  1.24s/it]  5%|▍         | 2302/50000 [49:15<16:26:04,  1.24s/it]  5%|▍         | 2303/50000 [49:17<16:25:46,  1.24s/it]  5%|▍         | 2304/50000 [49:18<16:25:49,  1.24s/it]  5%|▍         | 2305/50000 [49:19<16:25:35,  1.24s/it]  5%|▍         | 2306/50000 [49:20<16:26:13,  1.24s/it]  5%|▍         | 2307/50000 [49:22<16:25:40,  1.24s/it]  5%|▍         | 2308/50000 [49:23<16:25:25,  1.24s/it]  5%|▍         | 2309/50000 [49:24<16:25:20,  1.24s/it]  5%|▍         | 2310/50000 [49:25<16:25:21,  1.24s/it]                                                       {'loss': 163.9203, 'learning_rate': 1.8152000000000002e-05, 'epoch': 0.36}
  5%|▍         | 2310/50000 [49:25<16:25:21,  1.24s/it]  5%|▍         | 2311/50000 [49:27<16:25:12,  1.24s/it]  5%|▍         | 2312/50000 [49:28<16:25:35,  1.24s/it]  5%|▍         | 2313/50000 [49:29<16:25:31,  1.24s/it]  5%|▍         | 2314/50000 [49:30<16:25:21,  1.24s/it]  5%|▍         | 2315/50000 [49:32<16:25:20,  1.24s/it]  5%|▍         | 2316/50000 [49:33<16:25:26,  1.24s/it]  5%|▍         | 2317/50000 [49:34<16:25:08,  1.24s/it]  5%|▍         | 2318/50000 [49:35<16:25:06,  1.24s/it]  5%|▍         | 2319/50000 [49:37<16:25:11,  1.24s/it]  5%|▍         | 2320/50000 [49:38<16:25:14,  1.24s/it]                                                       {'loss': 145.0203, 'learning_rate': 1.8232e-05, 'epoch': 0.36}
  5%|▍         | 2320/50000 [49:38<16:25:14,  1.24s/it]  5%|▍         | 2321/50000 [49:39<16:25:41,  1.24s/it]  5%|▍         | 2322/50000 [49:40<16:25:27,  1.24s/it]  5%|▍         | 2323/50000 [49:42<16:25:25,  1.24s/it]  5%|▍         | 2324/50000 [49:43<16:25:31,  1.24s/it]  5%|▍         | 2325/50000 [49:44<16:24:50,  1.24s/it]  5%|▍         | 2326/50000 [49:45<16:24:46,  1.24s/it]  5%|▍         | 2327/50000 [49:46<16:24:55,  1.24s/it]  5%|▍         | 2328/50000 [49:48<16:25:01,  1.24s/it]  5%|▍         | 2329/50000 [49:49<16:24:22,  1.24s/it]  5%|▍         | 2330/50000 [49:50<16:23:21,  1.24s/it]                                                       {'loss': 172.0344, 'learning_rate': 1.8312e-05, 'epoch': 0.36}
  5%|▍         | 2330/50000 [49:50<16:23:21,  1.24s/it]  5%|▍         | 2331/50000 [49:51<16:22:44,  1.24s/it]  5%|▍         | 2332/50000 [49:53<16:22:07,  1.24s/it]  5%|▍         | 2333/50000 [49:54<16:21:55,  1.24s/it][2023-07-03 12:50:05,376] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, but hysteresis is 2. Reducing hysteresis to 1
  5%|▍         | 2334/50000 [49:55<15:22:32,  1.16s/it]  5%|▍         | 2335/50000 [49:56<15:40:20,  1.18s/it]  5%|▍         | 2336/50000 [49:57<15:52:20,  1.20s/it]  5%|▍         | 2337/50000 [49:59<16:01:13,  1.21s/it]  5%|▍         | 2338/50000 [50:00<16:07:03,  1.22s/it]  5%|▍         | 2339/50000 [50:01<16:11:12,  1.22s/it]  5%|▍         | 2340/50000 [50:02<16:15:04,  1.23s/it]                                                       {'loss': 159.3422, 'learning_rate': 1.8384000000000002e-05, 'epoch': 0.36}
  5%|▍         | 2340/50000 [50:02<16:15:04,  1.23s/it]  5%|▍         | 2341/50000 [50:04<16:17:49,  1.23s/it]  5%|▍         | 2342/50000 [50:05<16:19:58,  1.23s/it]  5%|▍         | 2343/50000 [50:06<16:21:21,  1.24s/it]  5%|▍         | 2344/50000 [50:07<16:22:25,  1.24s/it]  5%|▍         | 2345/50000 [50:08<16:22:49,  1.24s/it]  5%|▍         | 2346/50000 [50:10<16:23:22,  1.24s/it]  5%|▍         | 2347/50000 [50:11<16:23:38,  1.24s/it]  5%|▍         | 2348/50000 [50:12<16:23:50,  1.24s/it]  5%|▍         | 2349/50000 [50:13<16:23:50,  1.24s/it]  5%|▍         | 2350/50000 [50:15<16:24:00,  1.24s/it]                                                       {'loss': 210.1937, 'learning_rate': 1.8464e-05, 'epoch': 0.36}
  5%|▍         | 2350/50000 [50:15<16:24:00,  1.24s/it]  5%|▍         | 2351/50000 [50:16<16:24:20,  1.24s/it]  5%|▍         | 2352/50000 [50:17<16:24:24,  1.24s/it]  5%|▍         | 2353/50000 [50:18<16:23:56,  1.24s/it]  5%|▍         | 2354/50000 [50:20<16:22:48,  1.24s/it]  5%|▍         | 2355/50000 [50:21<16:21:47,  1.24s/it]  5%|▍         | 2356/50000 [50:22<16:21:09,  1.24s/it]  5%|▍         | 2357/50000 [50:23<16:20:36,  1.23s/it]  5%|▍         | 2358/50000 [50:25<16:20:04,  1.23s/it]  5%|▍         | 2359/50000 [50:26<16:19:36,  1.23s/it]  5%|▍         | 2360/50000 [50:27<16:19:30,  1.23s/it]                                                       {'loss': 224.4336, 'learning_rate': 1.8544e-05, 'epoch': 0.37}
  5%|▍         | 2360/50000 [50:27<16:19:30,  1.23s/it]  5%|▍         | 2361/50000 [50:28<16:19:35,  1.23s/it]  5%|▍         | 2362/50000 [50:29<16:19:29,  1.23s/it]  5%|▍         | 2363/50000 [50:31<16:19:33,  1.23s/it]  5%|▍         | 2364/50000 [50:32<16:19:20,  1.23s/it]  5%|▍         | 2365/50000 [50:33<16:19:11,  1.23s/it]  5%|▍         | 2366/50000 [50:34<16:19:17,  1.23s/it]  5%|▍         | 2367/50000 [50:36<16:19:09,  1.23s/it]  5%|▍         | 2368/50000 [50:37<16:19:05,  1.23s/it]  5%|▍         | 2369/50000 [50:38<16:19:04,  1.23s/it]  5%|▍         | 2370/50000 [50:39<16:19:23,  1.23s/it]                                                       {'loss': 175.1609, 'learning_rate': 1.8624000000000003e-05, 'epoch': 0.37}
  5%|▍         | 2370/50000 [50:39<16:19:23,  1.23s/it]  5%|▍         | 2371/50000 [50:41<16:19:30,  1.23s/it]  5%|▍         | 2372/50000 [50:42<16:19:35,  1.23s/it]  5%|▍         | 2373/50000 [50:43<16:19:44,  1.23s/it]  5%|▍         | 2374/50000 [50:44<16:19:34,  1.23s/it]  5%|▍         | 2375/50000 [50:46<16:19:36,  1.23s/it]  5%|▍         | 2376/50000 [50:47<16:19:59,  1.23s/it]  5%|▍         | 2377/50000 [50:48<16:20:33,  1.24s/it]  5%|▍         | 2378/50000 [50:49<16:21:32,  1.24s/it]  5%|▍         | 2379/50000 [50:50<16:21:51,  1.24s/it]  5%|▍         | 2380/50000 [50:52<16:21:11,  1.24s/it]                                                       {'loss': 184.7516, 'learning_rate': 1.8704000000000003e-05, 'epoch': 0.37}
  5%|▍         | 2380/50000 [50:52<16:21:11,  1.24s/it]  5%|▍         | 2381/50000 [50:53<16:20:59,  1.24s/it]  5%|▍         | 2382/50000 [50:54<16:20:30,  1.24s/it]  5%|▍         | 2383/50000 [50:55<16:20:08,  1.24s/it]  5%|▍         | 2384/50000 [50:57<16:19:49,  1.23s/it]  5%|▍         | 2385/50000 [50:58<16:20:18,  1.24s/it]  5%|▍         | 2386/50000 [50:59<16:20:40,  1.24s/it]  5%|▍         | 2387/50000 [51:00<16:20:51,  1.24s/it]  5%|▍         | 2388/50000 [51:02<16:22:26,  1.24s/it]  5%|▍         | 2389/50000 [51:03<16:23:01,  1.24s/it]  5%|▍         | 2390/50000 [51:04<16:23:43,  1.24s/it]                                                       {'loss': 176.4266, 'learning_rate': 1.8784000000000002e-05, 'epoch': 0.37}
  5%|▍         | 2390/50000 [51:04<16:23:43,  1.24s/it]  5%|▍         | 2391/50000 [51:05<16:24:05,  1.24s/it]  5%|▍         | 2392/50000 [51:07<16:24:43,  1.24s/it]  5%|▍         | 2393/50000 [51:08<16:24:34,  1.24s/it]  5%|▍         | 2394/50000 [51:09<16:24:50,  1.24s/it]  5%|▍         | 2395/50000 [51:10<16:25:05,  1.24s/it]  5%|▍         | 2396/50000 [51:12<16:25:29,  1.24s/it]  5%|▍         | 2397/50000 [51:13<16:25:17,  1.24s/it]  5%|▍         | 2398/50000 [51:14<16:24:51,  1.24s/it]  5%|▍         | 2399/50000 [51:15<16:24:59,  1.24s/it]  5%|▍         | 2400/50000 [51:17<16:25:07,  1.24s/it]                                                       {'loss': 198.4781, 'learning_rate': 1.8864e-05, 'epoch': 0.37}
  5%|▍         | 2400/50000 [51:17<16:25:07,  1.24s/it]  5%|▍         | 2401/50000 [51:18<16:24:44,  1.24s/it]  5%|▍         | 2402/50000 [51:19<16:26:12,  1.24s/it]  5%|▍         | 2403/50000 [51:20<16:25:33,  1.24s/it]  5%|▍         | 2404/50000 [51:21<16:24:50,  1.24s/it]  5%|▍         | 2405/50000 [51:23<16:24:46,  1.24s/it]  5%|▍         | 2406/50000 [51:24<16:24:09,  1.24s/it]  5%|▍         | 2407/50000 [51:25<16:23:59,  1.24s/it]  5%|▍         | 2408/50000 [51:26<16:23:40,  1.24s/it]  5%|▍         | 2409/50000 [51:28<16:23:27,  1.24s/it]  5%|▍         | 2410/50000 [51:29<16:23:36,  1.24s/it]                                                       {'loss': 202.7125, 'learning_rate': 1.8944000000000004e-05, 'epoch': 0.37}
  5%|▍         | 2410/50000 [51:29<16:23:36,  1.24s/it]  5%|▍         | 2411/50000 [51:30<16:23:37,  1.24s/it]  5%|▍         | 2412/50000 [51:31<16:23:56,  1.24s/it]  5%|▍         | 2413/50000 [51:33<16:23:54,  1.24s/it]  5%|▍         | 2414/50000 [51:34<16:23:38,  1.24s/it]  5%|▍         | 2415/50000 [51:35<16:23:37,  1.24s/it]  5%|▍         | 2416/50000 [51:36<16:23:23,  1.24s/it]  5%|▍         | 2417/50000 [51:38<16:23:21,  1.24s/it]  5%|▍         | 2418/50000 [51:39<16:23:22,  1.24s/it]  5%|▍         | 2419/50000 [51:40<16:24:03,  1.24s/it]  5%|▍         | 2420/50000 [51:41<16:23:41,  1.24s/it]                                                       {'loss': 185.4484, 'learning_rate': 1.9024000000000003e-05, 'epoch': 0.38}
  5%|▍         | 2420/50000 [51:41<16:23:41,  1.24s/it]  5%|▍         | 2421/50000 [51:43<16:24:04,  1.24s/it]  5%|▍         | 2422/50000 [51:44<16:23:59,  1.24s/it]  5%|▍         | 2423/50000 [51:45<16:23:36,  1.24s/it]  5%|▍         | 2424/50000 [51:46<16:23:22,  1.24s/it]  5%|▍         | 2425/50000 [51:48<16:23:19,  1.24s/it]  5%|▍         | 2426/50000 [51:49<16:23:15,  1.24s/it]  5%|▍         | 2427/50000 [51:50<16:23:02,  1.24s/it]  5%|▍         | 2428/50000 [51:51<16:22:56,  1.24s/it]  5%|▍         | 2429/50000 [51:52<16:22:59,  1.24s/it]  5%|▍         | 2430/50000 [51:54<16:22:59,  1.24s/it]                                                       {'loss': 151.3016, 'learning_rate': 1.9104000000000002e-05, 'epoch': 0.38}
  5%|▍         | 2430/50000 [51:54<16:22:59,  1.24s/it]  5%|▍         | 2431/50000 [51:55<16:23:05,  1.24s/it]  5%|▍         | 2432/50000 [51:56<16:23:03,  1.24s/it]  5%|▍         | 2433/50000 [51:57<16:22:47,  1.24s/it]  5%|▍         | 2434/50000 [51:59<16:22:34,  1.24s/it]  5%|▍         | 2435/50000 [52:00<16:22:27,  1.24s/it]  5%|▍         | 2436/50000 [52:01<16:22:40,  1.24s/it]  5%|▍         | 2437/50000 [52:02<16:22:47,  1.24s/it]  5%|▍         | 2438/50000 [52:04<16:22:38,  1.24s/it]  5%|▍         | 2439/50000 [52:05<16:22:29,  1.24s/it]  5%|▍         | 2440/50000 [52:06<16:22:26,  1.24s/it]                                                       {'loss': 151.9734, 'learning_rate': 1.9184e-05, 'epoch': 0.38}
  5%|▍         | 2440/50000 [52:06<16:22:26,  1.24s/it]  5%|▍         | 2441/50000 [52:07<16:22:44,  1.24s/it]  5%|▍         | 2442/50000 [52:09<16:22:48,  1.24s/it]  5%|▍         | 2443/50000 [52:10<16:22:40,  1.24s/it]  5%|▍         | 2444/50000 [52:11<16:23:03,  1.24s/it]  5%|▍         | 2445/50000 [52:12<16:23:09,  1.24s/it]  5%|▍         | 2446/50000 [52:14<16:23:01,  1.24s/it]  5%|▍         | 2447/50000 [52:15<16:22:51,  1.24s/it]  5%|▍         | 2448/50000 [52:16<16:22:43,  1.24s/it]  5%|▍         | 2449/50000 [52:17<16:22:36,  1.24s/it]  5%|▍         | 2450/50000 [52:19<16:22:31,  1.24s/it]                                                       {'loss': 156.5437, 'learning_rate': 1.9264e-05, 'epoch': 0.38}
  5%|▍         | 2450/50000 [52:19<16:22:31,  1.24s/it]  5%|▍         | 2451/50000 [52:20<16:22:44,  1.24s/it]  5%|▍         | 2452/50000 [52:21<16:22:41,  1.24s/it]  5%|▍         | 2453/50000 [52:22<16:22:51,  1.24s/it]  5%|▍         | 2454/50000 [52:23<16:22:37,  1.24s/it]  5%|▍         | 2455/50000 [52:25<16:22:39,  1.24s/it]  5%|▍         | 2456/50000 [52:26<16:22:26,  1.24s/it]  5%|▍         | 2457/50000 [52:27<16:22:27,  1.24s/it]  5%|▍         | 2458/50000 [52:28<16:22:13,  1.24s/it]  5%|▍         | 2459/50000 [52:30<16:22:15,  1.24s/it]  5%|▍         | 2460/50000 [52:31<16:22:17,  1.24s/it]                                                       {'loss': 160.5563, 'learning_rate': 1.9344e-05, 'epoch': 0.38}
  5%|▍         | 2460/50000 [52:31<16:22:17,  1.24s/it]  5%|▍         | 2461/50000 [52:32<16:22:34,  1.24s/it]  5%|▍         | 2462/50000 [52:33<16:22:26,  1.24s/it]  5%|▍         | 2463/50000 [52:35<16:22:37,  1.24s/it]  5%|▍         | 2464/50000 [52:36<16:22:39,  1.24s/it]  5%|▍         | 2465/50000 [52:37<16:22:37,  1.24s/it]  5%|▍         | 2466/50000 [52:38<16:22:26,  1.24s/it]  5%|▍         | 2467/50000 [52:40<16:22:36,  1.24s/it]  5%|▍         | 2468/50000 [52:41<16:22:42,  1.24s/it]  5%|▍         | 2469/50000 [52:42<16:22:33,  1.24s/it]  5%|▍         | 2470/50000 [52:43<16:22:40,  1.24s/it]                                                       {'loss': 144.0016, 'learning_rate': 1.9424e-05, 'epoch': 0.38}
  5%|▍         | 2470/50000 [52:43<16:22:40,  1.24s/it]  5%|▍         | 2471/50000 [52:45<16:23:11,  1.24s/it]  5%|▍         | 2472/50000 [52:46<16:22:57,  1.24s/it]  5%|▍         | 2473/50000 [52:47<16:22:51,  1.24s/it]  5%|▍         | 2474/50000 [52:48<16:22:38,  1.24s/it]  5%|▍         | 2475/50000 [52:50<16:22:35,  1.24s/it]  5%|▍         | 2476/50000 [52:51<16:23:13,  1.24s/it]  5%|▍         | 2477/50000 [52:52<16:23:03,  1.24s/it]  5%|▍         | 2478/50000 [52:53<16:22:51,  1.24s/it]  5%|▍         | 2479/50000 [52:54<16:22:27,  1.24s/it]  5%|▍         | 2480/50000 [52:56<16:22:13,  1.24s/it]                                                       {'loss': 173.0016, 'learning_rate': 1.9504e-05, 'epoch': 0.38}
  5%|▍         | 2480/50000 [52:56<16:22:13,  1.24s/it]  5%|▍         | 2481/50000 [52:57<16:22:30,  1.24s/it]  5%|▍         | 2482/50000 [52:58<16:22:21,  1.24s/it]  5%|▍         | 2483/50000 [52:59<16:22:21,  1.24s/it]  5%|▍         | 2484/50000 [53:01<16:22:13,  1.24s/it]  5%|▍         | 2485/50000 [53:02<16:22:13,  1.24s/it]  5%|▍         | 2486/50000 [53:03<16:22:24,  1.24s/it]  5%|▍         | 2487/50000 [53:04<16:22:08,  1.24s/it]  5%|▍         | 2488/50000 [53:06<16:22:08,  1.24s/it]  5%|▍         | 2489/50000 [53:07<16:22:09,  1.24s/it]  5%|▍         | 2490/50000 [53:08<16:21:59,  1.24s/it]                                                       {'loss': 137.7086, 'learning_rate': 1.9584e-05, 'epoch': 0.39}
  5%|▍         | 2490/50000 [53:08<16:21:59,  1.24s/it]  5%|▍         | 2491/50000 [53:09<16:22:27,  1.24s/it]  5%|▍         | 2492/50000 [53:11<16:22:17,  1.24s/it]  5%|▍         | 2493/50000 [53:12<16:22:35,  1.24s/it]  5%|▍         | 2494/50000 [53:13<16:22:26,  1.24s/it]  5%|▍         | 2495/50000 [53:14<16:22:26,  1.24s/it]  5%|▍         | 2496/50000 [53:16<16:22:10,  1.24s/it]  5%|▍         | 2497/50000 [53:17<16:21:57,  1.24s/it]  5%|▍         | 2498/50000 [53:18<16:21:49,  1.24s/it]  5%|▍         | 2499/50000 [53:19<16:21:32,  1.24s/it]  5%|▌         | 2500/50000 [53:21<16:21:29,  1.24s/it]                                                       {'loss': 140.0281, 'learning_rate': 1.9664e-05, 'epoch': 0.39}
  5%|▌         | 2500/50000 [53:21<16:21:29,  1.24s/it]  5%|▌         | 2501/50000 [53:22<16:22:00,  1.24s/it]  5%|▌         | 2502/50000 [53:23<16:21:57,  1.24s/it]  5%|▌         | 2503/50000 [53:24<16:21:45,  1.24s/it]  5%|▌         | 2504/50000 [53:25<16:22:00,  1.24s/it]  5%|▌         | 2505/50000 [53:27<16:21:38,  1.24s/it]  5%|▌         | 2506/50000 [53:28<16:21:31,  1.24s/it]  5%|▌         | 2507/50000 [53:29<16:21:26,  1.24s/it]  5%|▌         | 2508/50000 [53:30<16:21:17,  1.24s/it]  5%|▌         | 2509/50000 [53:32<16:21:56,  1.24s/it]  5%|▌         | 2510/50000 [53:33<16:21:53,  1.24s/it]                                                       {'loss': 167.4594, 'learning_rate': 1.9744e-05, 'epoch': 0.39}
  5%|▌         | 2510/50000 [53:33<16:21:53,  1.24s/it]  5%|▌         | 2511/50000 [53:34<16:21:56,  1.24s/it]  5%|▌         | 2512/50000 [53:35<16:21:46,  1.24s/it]  5%|▌         | 2513/50000 [53:37<16:21:42,  1.24s/it]  5%|▌         | 2514/50000 [53:38<16:21:35,  1.24s/it]  5%|▌         | 2515/50000 [53:39<16:21:24,  1.24s/it]  5%|▌         | 2516/50000 [53:40<16:21:13,  1.24s/it]  5%|▌         | 2517/50000 [53:42<16:21:01,  1.24s/it]  5%|▌         | 2518/50000 [53:43<16:20:55,  1.24s/it]  5%|▌         | 2519/50000 [53:44<16:21:03,  1.24s/it]  5%|▌         | 2520/50000 [53:45<16:21:03,  1.24s/it]                                                       {'loss': 177.1125, 'learning_rate': 1.9824000000000002e-05, 'epoch': 0.39}
  5%|▌         | 2520/50000 [53:45<16:21:03,  1.24s/it]  5%|▌         | 2521/50000 [53:47<16:21:17,  1.24s/it]  5%|▌         | 2522/50000 [53:48<16:21:07,  1.24s/it]  5%|▌         | 2523/50000 [53:49<16:20:56,  1.24s/it]  5%|▌         | 2524/50000 [53:50<16:20:54,  1.24s/it]  5%|▌         | 2525/50000 [53:52<16:20:54,  1.24s/it]  5%|▌         | 2526/50000 [53:53<16:20:52,  1.24s/it]  5%|▌         | 2527/50000 [53:54<16:20:51,  1.24s/it]  5%|▌         | 2528/50000 [53:55<16:20:53,  1.24s/it]  5%|▌         | 2529/50000 [53:56<16:20:47,  1.24s/it]  5%|▌         | 2530/50000 [53:58<16:20:50,  1.24s/it]                                                       {'loss': 174.0047, 'learning_rate': 1.9904e-05, 'epoch': 0.39}
  5%|▌         | 2530/50000 [53:58<16:20:50,  1.24s/it]  5%|▌         | 2531/50000 [53:59<16:21:11,  1.24s/it]  5%|▌         | 2532/50000 [54:00<16:21:06,  1.24s/it]  5%|▌         | 2533/50000 [54:01<16:21:12,  1.24s/it]  5%|▌         | 2534/50000 [54:03<16:21:29,  1.24s/it]  5%|▌         | 2535/50000 [54:04<16:21:16,  1.24s/it]  5%|▌         | 2536/50000 [54:05<16:21:08,  1.24s/it]  5%|▌         | 2537/50000 [54:06<16:20:52,  1.24s/it]  5%|▌         | 2538/50000 [54:08<16:20:55,  1.24s/it]  5%|▌         | 2539/50000 [54:09<16:20:45,  1.24s/it]  5%|▌         | 2540/50000 [54:10<16:20:37,  1.24s/it]                                                       {'loss': 171.7031, 'learning_rate': 1.9984e-05, 'epoch': 0.39}
  5%|▌         | 2540/50000 [54:10<16:20:37,  1.24s/it]  5%|▌         | 2541/50000 [54:11<16:20:47,  1.24s/it]  5%|▌         | 2542/50000 [54:13<16:20:48,  1.24s/it][2023-07-03 12:54:24,122] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, but hysteresis is 2. Reducing hysteresis to 1
  5%|▌         | 2543/50000 [54:14<15:21:58,  1.17s/it]  5%|▌         | 2544/50000 [54:15<15:39:31,  1.19s/it][2023-07-03 12:54:26,353] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  5%|▌         | 2545/50000 [54:16<14:52:50,  1.13s/it]  5%|▌         | 2546/50000 [54:17<15:19:03,  1.16s/it]  5%|▌         | 2547/50000 [54:18<15:38:24,  1.19s/it]  5%|▌         | 2548/50000 [54:20<15:50:58,  1.20s/it]  5%|▌         | 2549/50000 [54:21<15:59:44,  1.21s/it]  5%|▌         | 2550/50000 [54:22<16:06:48,  1.22s/it]                                                       {'loss': 200.1891, 'learning_rate': 1.999999921261883e-05, 'epoch': 0.4}
  5%|▌         | 2550/50000 [54:22<16:06:48,  1.22s/it]  5%|▌         | 2551/50000 [54:23<16:12:00,  1.23s/it]  5%|▌         | 2552/50000 [54:25<16:14:45,  1.23s/it]  5%|▌         | 2553/50000 [54:26<16:15:49,  1.23s/it]  5%|▌         | 2554/50000 [54:27<16:16:43,  1.24s/it]  5%|▌         | 2555/50000 [54:28<16:17:16,  1.24s/it]  5%|▌         | 2556/50000 [54:29<16:17:24,  1.24s/it]  5%|▌         | 2557/50000 [54:31<16:19:31,  1.24s/it]  5%|▌         | 2558/50000 [54:32<16:19:04,  1.24s/it]  5%|▌         | 2559/50000 [54:33<16:18:53,  1.24s/it]  5%|▌         | 2560/50000 [54:34<16:20:12,  1.24s/it]                                                       {'loss': 159.3531, 'learning_rate': 1.9999994400845452e-05, 'epoch': 0.4}
  5%|▌         | 2560/50000 [54:34<16:20:12,  1.24s/it]  5%|▌         | 2561/50000 [54:36<16:21:08,  1.24s/it]  5%|▌         | 2562/50000 [54:37<16:21:00,  1.24s/it]  5%|▌         | 2563/50000 [54:38<16:20:32,  1.24s/it]  5%|▌         | 2564/50000 [54:39<16:19:46,  1.24s/it]  5%|▌         | 2565/50000 [54:41<16:19:25,  1.24s/it]  5%|▌         | 2566/50000 [54:42<16:18:43,  1.24s/it]  5%|▌         | 2567/50000 [54:43<16:20:13,  1.24s/it]  5%|▌         | 2568/50000 [54:44<16:21:06,  1.24s/it]  5%|▌         | 2569/50000 [54:46<16:23:05,  1.24s/it]  5%|▌         | 2570/50000 [54:47<16:23:03,  1.24s/it]                                                       {'loss': 169.2984, 'learning_rate': 1.999998521473478e-05, 'epoch': 0.4}
  5%|▌         | 2570/50000 [54:47<16:23:03,  1.24s/it]  5%|▌         | 2571/50000 [54:48<16:22:30,  1.24s/it]  5%|▌         | 2572/50000 [54:49<16:21:27,  1.24s/it]  5%|▌         | 2573/50000 [54:51<16:22:18,  1.24s/it]  5%|▌         | 2574/50000 [54:52<16:27:56,  1.25s/it]  5%|▌         | 2575/50000 [54:53<16:29:09,  1.25s/it]  5%|▌         | 2576/50000 [54:54<16:34:04,  1.26s/it]  5%|▌         | 2577/50000 [54:56<16:35:42,  1.26s/it]  5%|▌         | 2578/50000 [54:57<16:38:13,  1.26s/it]  5%|▌         | 2579/50000 [54:58<16:39:28,  1.26s/it]  5%|▌         | 2580/50000 [54:59<16:40:30,  1.27s/it]                                                       {'loss': 217.0594, 'learning_rate': 1.9999971654290836e-05, 'epoch': 0.4}
  5%|▌         | 2580/50000 [54:59<16:40:30,  1.27s/it]  5%|▌         | 2581/50000 [55:01<16:46:05,  1.27s/it]  5%|▌         | 2582/50000 [55:02<16:40:36,  1.27s/it]  5%|▌         | 2583/50000 [55:03<16:37:02,  1.26s/it]  5%|▌         | 2584/50000 [55:05<16:37:01,  1.26s/it]  5%|▌         | 2585/50000 [55:06<16:37:34,  1.26s/it]  5%|▌         | 2586/50000 [55:07<16:35:11,  1.26s/it]  5%|▌         | 2587/50000 [55:08<16:34:54,  1.26s/it]  5%|▌         | 2588/50000 [55:10<16:35:06,  1.26s/it]  5%|▌         | 2589/50000 [55:11<16:33:39,  1.26s/it]  5%|▌         | 2590/50000 [55:12<16:31:00,  1.25s/it]                                                       {'loss': 215.0109, 'learning_rate': 1.999995371951955e-05, 'epoch': 0.4}
  5%|▌         | 2590/50000 [55:12<16:31:00,  1.25s/it]  5%|▌         | 2591/50000 [55:13<16:29:38,  1.25s/it]  5%|▌         | 2592/50000 [55:15<16:32:38,  1.26s/it]  5%|▌         | 2593/50000 [55:16<16:33:06,  1.26s/it]  5%|▌         | 2594/50000 [55:17<16:29:41,  1.25s/it]  5%|▌         | 2595/50000 [55:18<16:27:44,  1.25s/it]  5%|▌         | 2596/50000 [55:20<16:26:24,  1.25s/it]  5%|▌         | 2597/50000 [55:21<16:25:55,  1.25s/it]  5%|▌         | 2598/50000 [55:22<16:24:07,  1.25s/it]  5%|▌         | 2599/50000 [55:23<16:22:34,  1.24s/it]  5%|▌         | 2600/50000 [55:25<16:26:18,  1.25s/it]                                                       {'loss': 189.8391, 'learning_rate': 1.9999931410428767e-05, 'epoch': 0.4}
  5%|▌         | 2600/50000 [55:25<16:26:18,  1.25s/it]  5%|▌         | 2601/50000 [55:26<16:25:07,  1.25s/it]  5%|▌         | 2602/50000 [55:27<16:23:33,  1.25s/it]  5%|▌         | 2603/50000 [55:28<16:22:40,  1.24s/it]  5%|▌         | 2604/50000 [55:29<16:21:40,  1.24s/it]  5%|▌         | 2605/50000 [55:31<16:21:38,  1.24s/it]  5%|▌         | 2606/50000 [55:32<16:20:26,  1.24s/it]  5%|▌         | 2607/50000 [55:33<16:20:00,  1.24s/it]  5%|▌         | 2608/50000 [55:34<16:19:22,  1.24s/it]  5%|▌         | 2609/50000 [55:36<16:18:59,  1.24s/it]  5%|▌         | 2610/50000 [55:37<16:18:45,  1.24s/it]                                                       {'loss': 151.6734, 'learning_rate': 1.9999904727028246e-05, 'epoch': 0.4}
  5%|▌         | 2610/50000 [55:37<16:18:45,  1.24s/it]  5%|▌         | 2611/50000 [55:38<16:18:55,  1.24s/it]  5%|▌         | 2612/50000 [55:39<16:19:43,  1.24s/it]  5%|▌         | 2613/50000 [55:41<16:19:40,  1.24s/it]  5%|▌         | 2614/50000 [55:42<16:19:40,  1.24s/it]  5%|▌         | 2615/50000 [55:43<16:19:55,  1.24s/it]  5%|▌         | 2616/50000 [55:44<16:19:47,  1.24s/it]  5%|▌         | 2617/50000 [55:46<16:19:29,  1.24s/it]  5%|▌         | 2618/50000 [55:47<16:19:23,  1.24s/it]  5%|▌         | 2619/50000 [55:48<16:19:15,  1.24s/it]  5%|▌         | 2620/50000 [55:49<16:19:07,  1.24s/it]                                                       {'loss': 151.4203, 'learning_rate': 1.999987366932966e-05, 'epoch': 0.41}
  5%|▌         | 2620/50000 [55:49<16:19:07,  1.24s/it]  5%|▌         | 2621/50000 [55:51<16:18:43,  1.24s/it]  5%|▌         | 2622/50000 [55:52<16:18:22,  1.24s/it]  5%|▌         | 2623/50000 [55:53<16:18:10,  1.24s/it]  5%|▌         | 2624/50000 [55:54<16:18:27,  1.24s/it]  5%|▌         | 2625/50000 [55:56<16:18:07,  1.24s/it]  5%|▌         | 2626/50000 [55:57<16:18:31,  1.24s/it]  5%|▌         | 2627/50000 [55:58<16:17:57,  1.24s/it]  5%|▌         | 2628/50000 [55:59<16:17:27,  1.24s/it]  5%|▌         | 2629/50000 [56:00<16:17:36,  1.24s/it]  5%|▌         | 2630/50000 [56:02<16:17:13,  1.24s/it]                                                       {'loss': 160.5344, 'learning_rate': 1.9999838237346592e-05, 'epoch': 0.41}
  5%|▌         | 2630/50000 [56:02<16:17:13,  1.24s/it]  5%|▌         | 2631/50000 [56:03<16:17:25,  1.24s/it]  5%|▌         | 2632/50000 [56:04<16:17:17,  1.24s/it]  5%|▌         | 2633/50000 [56:05<16:17:24,  1.24s/it]  5%|▌         | 2634/50000 [56:07<16:17:03,  1.24s/it]  5%|▌         | 2635/50000 [56:08<16:17:06,  1.24s/it]  5%|▌         | 2636/50000 [56:09<16:17:03,  1.24s/it]  5%|▌         | 2637/50000 [56:10<16:17:45,  1.24s/it]  5%|▌         | 2638/50000 [56:12<16:18:18,  1.24s/it]  5%|▌         | 2639/50000 [56:13<16:19:15,  1.24s/it]  5%|▌         | 2640/50000 [56:14<16:20:10,  1.24s/it]                                                       {'loss': 149.7031, 'learning_rate': 1.9999798431094544e-05, 'epoch': 0.41}
  5%|▌         | 2640/50000 [56:14<16:20:10,  1.24s/it]  5%|▌         | 2641/50000 [56:15<16:20:24,  1.24s/it]  5%|▌         | 2642/50000 [56:17<16:20:08,  1.24s/it]  5%|▌         | 2643/50000 [56:18<16:20:18,  1.24s/it]  5%|▌         | 2644/50000 [56:19<16:20:15,  1.24s/it]  5%|▌         | 2645/50000 [56:20<16:20:20,  1.24s/it]  5%|▌         | 2646/50000 [56:22<16:20:23,  1.24s/it]  5%|▌         | 2647/50000 [56:23<16:19:55,  1.24s/it]  5%|▌         | 2648/50000 [56:24<16:19:39,  1.24s/it]  5%|▌         | 2649/50000 [56:25<16:19:17,  1.24s/it]  5%|▌         | 2650/50000 [56:27<16:19:44,  1.24s/it]                                                       {'loss': 206.7719, 'learning_rate': 1.9999754250590932e-05, 'epoch': 0.41}
  5%|▌         | 2650/50000 [56:27<16:19:44,  1.24s/it]  5%|▌         | 2651/50000 [56:28<16:19:19,  1.24s/it]  5%|▌         | 2652/50000 [56:29<16:19:07,  1.24s/it]  5%|▌         | 2653/50000 [56:30<16:18:40,  1.24s/it]  5%|▌         | 2654/50000 [56:31<16:18:45,  1.24s/it]  5%|▌         | 2655/50000 [56:33<16:17:57,  1.24s/it]  5%|▌         | 2656/50000 [56:34<16:17:38,  1.24s/it]  5%|▌         | 2657/50000 [56:35<16:17:17,  1.24s/it]  5%|▌         | 2658/50000 [56:36<16:16:59,  1.24s/it]  5%|▌         | 2659/50000 [56:38<16:16:38,  1.24s/it]  5%|▌         | 2660/50000 [56:39<16:16:44,  1.24s/it]                                                       {'loss': 202.9141, 'learning_rate': 1.999970569585507e-05, 'epoch': 0.41}
  5%|▌         | 2660/50000 [56:39<16:16:44,  1.24s/it]  5%|▌         | 2661/50000 [56:40<16:17:21,  1.24s/it]  5%|▌         | 2662/50000 [56:41<16:17:26,  1.24s/it]  5%|▌         | 2663/50000 [56:43<16:17:01,  1.24s/it]  5%|▌         | 2664/50000 [56:44<16:17:18,  1.24s/it]  5%|▌         | 2665/50000 [56:45<16:17:07,  1.24s/it]  5%|▌         | 2666/50000 [56:46<16:16:47,  1.24s/it]  5%|▌         | 2667/50000 [56:48<16:17:06,  1.24s/it]  5%|▌         | 2668/50000 [56:49<16:17:00,  1.24s/it]  5%|▌         | 2669/50000 [56:50<16:16:58,  1.24s/it]  5%|▌         | 2670/50000 [56:51<16:17:24,  1.24s/it]                                                       {'loss': 172.7766, 'learning_rate': 1.999965276690821e-05, 'epoch': 0.41}
  5%|▌         | 2670/50000 [56:51<16:17:24,  1.24s/it]  5%|▌         | 2671/50000 [56:53<16:17:27,  1.24s/it]  5%|▌         | 2672/50000 [56:54<16:17:15,  1.24s/it]  5%|▌         | 2673/50000 [56:55<16:17:11,  1.24s/it]  5%|▌         | 2674/50000 [56:56<16:17:20,  1.24s/it]  5%|▌         | 2675/50000 [56:57<16:17:17,  1.24s/it]  5%|▌         | 2676/50000 [56:59<16:16:54,  1.24s/it]  5%|▌         | 2677/50000 [57:00<16:17:05,  1.24s/it]  5%|▌         | 2678/50000 [57:01<16:17:24,  1.24s/it]  5%|▌         | 2679/50000 [57:02<16:17:12,  1.24s/it]  5%|▌         | 2680/50000 [57:04<16:17:11,  1.24s/it]                                                       {'loss': 158.6281, 'learning_rate': 1.9999595463773504e-05, 'epoch': 0.42}
  5%|▌         | 2680/50000 [57:04<16:17:11,  1.24s/it]  5%|▌         | 2681/50000 [57:05<16:16:58,  1.24s/it]  5%|▌         | 2682/50000 [57:06<16:17:21,  1.24s/it]  5%|▌         | 2683/50000 [57:07<16:17:08,  1.24s/it]  5%|▌         | 2684/50000 [57:09<16:16:56,  1.24s/it]  5%|▌         | 2685/50000 [57:10<16:16:41,  1.24s/it]  5%|▌         | 2686/50000 [57:11<16:16:39,  1.24s/it]  5%|▌         | 2687/50000 [57:12<16:16:30,  1.24s/it]  5%|▌         | 2688/50000 [57:14<16:16:33,  1.24s/it]  5%|▌         | 2689/50000 [57:15<16:16:28,  1.24s/it]  5%|▌         | 2690/50000 [57:16<16:16:17,  1.24s/it]                                                       {'loss': 151.8562, 'learning_rate': 1.9999533786476008e-05, 'epoch': 0.42}
  5%|▌         | 2690/50000 [57:16<16:16:17,  1.24s/it]  5%|▌         | 2691/50000 [57:17<16:16:11,  1.24s/it]  5%|▌         | 2692/50000 [57:19<16:16:08,  1.24s/it]  5%|▌         | 2693/50000 [57:20<16:15:59,  1.24s/it]  5%|▌         | 2694/50000 [57:21<16:15:58,  1.24s/it]  5%|▌         | 2695/50000 [57:22<16:15:57,  1.24s/it]  5%|▌         | 2696/50000 [57:24<16:15:54,  1.24s/it]  5%|▌         | 2697/50000 [57:25<16:15:47,  1.24s/it]  5%|▌         | 2698/50000 [57:26<16:15:50,  1.24s/it]  5%|▌         | 2699/50000 [57:27<16:15:37,  1.24s/it]  5%|▌         | 2700/50000 [57:28<16:16:03,  1.24s/it]                                                       {'loss': 175.0828, 'learning_rate': 1.999946773504271e-05, 'epoch': 0.42}
  5%|▌         | 2700/50000 [57:28<16:16:03,  1.24s/it]  5%|▌         | 2701/50000 [57:30<16:16:13,  1.24s/it]  5%|▌         | 2702/50000 [57:31<16:16:07,  1.24s/it]  5%|▌         | 2703/50000 [57:32<16:16:07,  1.24s/it]  5%|▌         | 2704/50000 [57:33<16:15:47,  1.24s/it]  5%|▌         | 2705/50000 [57:35<16:15:41,  1.24s/it]  5%|▌         | 2706/50000 [57:36<16:15:35,  1.24s/it]  5%|▌         | 2707/50000 [57:37<16:15:25,  1.24s/it]  5%|▌         | 2708/50000 [57:38<16:15:40,  1.24s/it]  5%|▌         | 2709/50000 [57:40<16:15:57,  1.24s/it]  5%|▌         | 2710/50000 [57:41<16:15:47,  1.24s/it]                                                       {'loss': 146.6719, 'learning_rate': 1.99993973095025e-05, 'epoch': 0.42}
  5%|▌         | 2710/50000 [57:41<16:15:47,  1.24s/it]  5%|▌         | 2711/50000 [57:42<16:16:16,  1.24s/it]  5%|▌         | 2712/50000 [57:43<16:15:55,  1.24s/it]  5%|▌         | 2713/50000 [57:45<16:15:48,  1.24s/it]  5%|▌         | 2714/50000 [57:46<16:15:47,  1.24s/it]  5%|▌         | 2715/50000 [57:47<16:15:46,  1.24s/it]  5%|▌         | 2716/50000 [57:48<16:15:44,  1.24s/it]  5%|▌         | 2717/50000 [57:49<16:15:33,  1.24s/it]  5%|▌         | 2718/50000 [57:51<16:15:29,  1.24s/it]  5%|▌         | 2719/50000 [57:52<16:16:20,  1.24s/it]  5%|▌         | 2720/50000 [57:53<16:16:04,  1.24s/it]                                                       {'loss': 156.8172, 'learning_rate': 1.999932250988619e-05, 'epoch': 0.42}
  5%|▌         | 2720/50000 [57:53<16:16:04,  1.24s/it]  5%|▌         | 2721/50000 [57:54<16:16:08,  1.24s/it]  5%|▌         | 2722/50000 [57:56<16:16:14,  1.24s/it]  5%|▌         | 2723/50000 [57:57<16:15:45,  1.24s/it]  5%|▌         | 2724/50000 [57:58<16:15:39,  1.24s/it]  5%|▌         | 2725/50000 [57:59<16:15:42,  1.24s/it]  5%|▌         | 2726/50000 [58:01<16:15:22,  1.24s/it]  5%|▌         | 2727/50000 [58:02<16:15:23,  1.24s/it]  5%|▌         | 2728/50000 [58:03<16:15:32,  1.24s/it]  5%|▌         | 2729/50000 [58:04<16:15:24,  1.24s/it]  5%|▌         | 2730/50000 [58:06<16:15:12,  1.24s/it]                                                       {'loss': 164.0469, 'learning_rate': 1.9999243336226494e-05, 'epoch': 0.42}
  5%|▌         | 2730/50000 [58:06<16:15:12,  1.24s/it]  5%|▌         | 2731/50000 [58:07<16:15:46,  1.24s/it]  5%|▌         | 2732/50000 [58:08<16:15:36,  1.24s/it]  5%|▌         | 2733/50000 [58:09<16:15:42,  1.24s/it]  5%|▌         | 2734/50000 [58:11<16:16:00,  1.24s/it]  5%|▌         | 2735/50000 [58:12<16:15:59,  1.24s/it]  5%|▌         | 2736/50000 [58:13<16:16:04,  1.24s/it]  5%|▌         | 2737/50000 [58:14<16:15:47,  1.24s/it]  5%|▌         | 2738/50000 [58:16<16:15:42,  1.24s/it]  5%|▌         | 2739/50000 [58:17<16:15:16,  1.24s/it]  5%|▌         | 2740/50000 [58:18<16:15:06,  1.24s/it]                                                       {'loss': 189.5156, 'learning_rate': 1.9999159788558046e-05, 'epoch': 0.42}
  5%|▌         | 2740/50000 [58:18<16:15:06,  1.24s/it]  5%|▌         | 2741/50000 [58:19<16:15:20,  1.24s/it]  5%|▌         | 2742/50000 [58:20<16:15:22,  1.24s/it]  5%|▌         | 2743/50000 [58:22<16:15:32,  1.24s/it]  5%|▌         | 2744/50000 [58:23<16:15:49,  1.24s/it]  5%|▌         | 2745/50000 [58:24<16:15:29,  1.24s/it]  5%|▌         | 2746/50000 [58:25<16:15:11,  1.24s/it]  5%|▌         | 2747/50000 [58:27<16:15:03,  1.24s/it]  5%|▌         | 2748/50000 [58:28<16:14:45,  1.24s/it]  5%|▌         | 2749/50000 [58:29<16:14:36,  1.24s/it]  6%|▌         | 2750/50000 [58:30<16:14:47,  1.24s/it]                                                       {'loss': 207.7078, 'learning_rate': 1.99990718669174e-05, 'epoch': 0.43}
  6%|▌         | 2750/50000 [58:30<16:14:47,  1.24s/it]  6%|▌         | 2751/50000 [58:32<16:14:59,  1.24s/it]  6%|▌         | 2752/50000 [58:33<16:14:50,  1.24s/it]  6%|▌         | 2753/50000 [58:34<16:14:58,  1.24s/it]  6%|▌         | 2754/50000 [58:35<16:15:17,  1.24s/it][2023-07-03 12:58:46,823] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, but hysteresis is 2. Reducing hysteresis to 1
  6%|▌         | 2755/50000 [58:36<15:16:05,  1.16s/it][2023-07-03 12:58:47,811] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
  6%|▌         | 2756/50000 [58:37<14:34:35,  1.11s/it]  6%|▌         | 2757/50000 [58:39<15:04:19,  1.15s/it]  6%|▌         | 2758/50000 [58:40<15:25:06,  1.17s/it]  6%|▌         | 2759/50000 [58:41<15:40:15,  1.19s/it]  6%|▌         | 2760/50000 [58:42<15:51:15,  1.21s/it]                                                       {'loss': 186.425, 'learning_rate': 1.9998998380370673e-05, 'epoch': 0.43}
  6%|▌         | 2760/50000 [58:42<15:51:15,  1.21s/it]  6%|▌         | 2761/50000 [58:43<15:58:42,  1.22s/it]  6%|▌         | 2762/50000 [58:45<16:03:48,  1.22s/it]  6%|▌         | 2763/50000 [58:46<16:07:25,  1.23s/it]  6%|▌         | 2764/50000 [58:47<16:09:59,  1.23s/it]  6%|▌         | 2765/50000 [58:48<16:11:41,  1.23s/it]  6%|▌         | 2766/50000 [58:50<16:12:50,  1.24s/it]  6%|▌         | 2767/50000 [58:51<16:14:04,  1.24s/it]  6%|▌         | 2768/50000 [58:52<16:15:08,  1.24s/it]  6%|▌         | 2769/50000 [58:53<16:15:07,  1.24s/it]  6%|▌         | 2770/50000 [58:55<16:15:22,  1.24s/it]                                                       {'loss': 255.3531, 'learning_rate': 1.9998902585678256e-05, 'epoch': 0.43}
  6%|▌         | 2770/50000 [58:55<16:15:22,  1.24s/it]  6%|▌         | 2771/50000 [58:56<16:35:25,  1.26s/it][2023-07-03 12:59:07,482] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  6%|▌         | 2772/50000 [58:57<15:31:35,  1.18s/it]  6%|▌         | 2773/50000 [58:58<15:45:21,  1.20s/it]  6%|▌         | 2774/50000 [59:00<16:28:34,  1.26s/it]  6%|▌         | 2775/50000 [59:01<16:30:57,  1.26s/it]  6%|▌         | 2776/50000 [59:02<16:28:03,  1.26s/it]  6%|▌         | 2777/50000 [59:03<16:25:58,  1.25s/it]  6%|▌         | 2778/50000 [59:05<16:23:34,  1.25s/it]  6%|▌         | 2779/50000 [59:06<16:21:15,  1.25s/it]  6%|▌         | 2780/50000 [59:07<16:19:58,  1.25s/it]                                                       {'loss': 167.3453, 'learning_rate': 1.999881263080381e-05, 'epoch': 0.43}
  6%|▌         | 2780/50000 [59:07<16:19:58,  1.25s/it]  6%|▌         | 2781/50000 [59:08<16:19:49,  1.25s/it]  6%|▌         | 2782/50000 [59:10<16:18:34,  1.24s/it]  6%|▌         | 2783/50000 [59:11<16:17:50,  1.24s/it]  6%|▌         | 2784/50000 [59:12<16:17:37,  1.24s/it]  6%|▌         | 2785/50000 [59:13<16:17:46,  1.24s/it]  6%|▌         | 2786/50000 [59:15<16:17:30,  1.24s/it]  6%|▌         | 2787/50000 [59:16<16:16:36,  1.24s/it]  6%|▌         | 2788/50000 [59:17<16:15:50,  1.24s/it]  6%|▌         | 2789/50000 [59:18<16:15:24,  1.24s/it]  6%|▌         | 2790/50000 [59:19<16:15:04,  1.24s/it]                                                       {'loss': 155.1391, 'learning_rate': 1.9998708525815384e-05, 'epoch': 0.43}
  6%|▌         | 2790/50000 [59:19<16:15:04,  1.24s/it]  6%|▌         | 2791/50000 [59:21<16:15:09,  1.24s/it]  6%|▌         | 2792/50000 [59:22<16:14:50,  1.24s/it]  6%|▌         | 2793/50000 [59:23<16:14:56,  1.24s/it]  6%|▌         | 2794/50000 [59:24<16:15:16,  1.24s/it]  6%|▌         | 2795/50000 [59:26<16:15:22,  1.24s/it]  6%|▌         | 2796/50000 [59:27<16:15:17,  1.24s/it]  6%|▌         | 2797/50000 [59:28<16:15:09,  1.24s/it]  6%|▌         | 2798/50000 [59:29<16:14:54,  1.24s/it]  6%|▌         | 2799/50000 [59:31<16:14:36,  1.24s/it]  6%|▌         | 2800/50000 [59:32<16:14:22,  1.24s/it]                                                       {'loss': 156.5578, 'learning_rate': 1.9998600047052154e-05, 'epoch': 0.43}
  6%|▌         | 2800/50000 [59:32<16:14:22,  1.24s/it]  6%|▌         | 2801/50000 [59:33<16:14:35,  1.24s/it]  6%|▌         | 2802/50000 [59:34<16:14:49,  1.24s/it]  6%|▌         | 2803/50000 [59:36<16:27:02,  1.25s/it]  6%|▌         | 2804/50000 [59:37<16:23:12,  1.25s/it]  6%|▌         | 2805/50000 [59:38<16:20:35,  1.25s/it]  6%|▌         | 2806/50000 [59:39<16:39:30,  1.27s/it]  6%|▌         | 2807/50000 [59:41<16:33:01,  1.26s/it]  6%|▌         | 2808/50000 [59:42<16:27:23,  1.26s/it]  6%|▌         | 2809/50000 [59:43<16:23:53,  1.25s/it]  6%|▌         | 2810/50000 [59:44<16:21:08,  1.25s/it]                                                       {'loss': 148.4625, 'learning_rate': 1.9998487194561567e-05, 'epoch': 0.44}
  6%|▌         | 2810/50000 [59:44<16:21:08,  1.25s/it]  6%|▌         | 2811/50000 [59:46<16:19:20,  1.25s/it]  6%|▌         | 2812/50000 [59:47<16:17:36,  1.24s/it]  6%|▌         | 2813/50000 [59:48<16:17:45,  1.24s/it]  6%|▌         | 2814/50000 [59:49<16:17:44,  1.24s/it]  6%|▌         | 2815/50000 [59:51<16:17:03,  1.24s/it]  6%|▌         | 2816/50000 [59:52<16:16:22,  1.24s/it]  6%|▌         | 2817/50000 [59:53<16:16:29,  1.24s/it]  6%|▌         | 2818/50000 [59:54<16:16:10,  1.24s/it]  6%|▌         | 2819/50000 [59:56<16:16:10,  1.24s/it]  6%|▌         | 2820/50000 [59:57<16:15:39,  1.24s/it]                                                       {'loss': 165.45, 'learning_rate': 1.9998369968392992e-05, 'epoch': 0.44}
  6%|▌         | 2820/50000 [59:57<16:15:39,  1.24s/it]  6%|▌         | 2821/50000 [59:58<16:15:56,  1.24s/it]  6%|▌         | 2822/50000 [59:59<16:14:49,  1.24s/it]  6%|▌         | 2823/50000 [1:00:01<16:14:23,  1.24s/it]  6%|▌         | 2824/50000 [1:00:02<16:15:18,  1.24s/it]  6%|▌         | 2825/50000 [1:00:03<16:14:55,  1.24s/it]  6%|▌         | 2826/50000 [1:00:04<16:16:24,  1.24s/it]  6%|▌         | 2827/50000 [1:00:06<16:16:15,  1.24s/it]  6%|▌         | 2828/50000 [1:00:07<16:16:15,  1.24s/it]  6%|▌         | 2829/50000 [1:00:08<16:15:59,  1.24s/it]  6%|▌         | 2830/50000 [1:00:09<16:15:33,  1.24s/it]                                                         {'loss': 165.7812, 'learning_rate': 1.999824836859771e-05, 'epoch': 0.44}
  6%|▌         | 2830/50000 [1:00:09<16:15:33,  1.24s/it]  6%|▌         | 2831/50000 [1:00:10<16:15:11,  1.24s/it]  6%|▌         | 2832/50000 [1:00:12<16:15:31,  1.24s/it]  6%|▌         | 2833/50000 [1:00:13<16:15:07,  1.24s/it]  6%|▌         | 2834/50000 [1:00:14<16:14:58,  1.24s/it]  6%|▌         | 2835/50000 [1:00:16<16:47:16,  1.28s/it]  6%|▌         | 2836/50000 [1:00:17<16:39:22,  1.27s/it]  6%|▌         | 2837/50000 [1:00:18<16:32:03,  1.26s/it]  6%|▌         | 2838/50000 [1:00:19<16:26:35,  1.26s/it]  6%|▌         | 2839/50000 [1:00:21<16:22:41,  1.25s/it]  6%|▌         | 2840/50000 [1:00:22<16:20:29,  1.25s/it]                                                         {'loss': 206.8844, 'learning_rate': 1.9998122395228905e-05, 'epoch': 0.44}
  6%|▌         | 2840/50000 [1:00:22<16:20:29,  1.25s/it]  6%|▌         | 2841/50000 [1:00:23<16:18:51,  1.25s/it]  6%|▌         | 2842/50000 [1:00:24<16:17:32,  1.24s/it]  6%|▌         | 2843/50000 [1:00:25<16:17:11,  1.24s/it]  6%|▌         | 2844/50000 [1:00:27<16:16:25,  1.24s/it]  6%|▌         | 2845/50000 [1:00:28<16:15:18,  1.24s/it]  6%|▌         | 2846/50000 [1:00:29<16:14:42,  1.24s/it]  6%|▌         | 2847/50000 [1:00:30<16:14:28,  1.24s/it]  6%|▌         | 2848/50000 [1:00:32<16:14:03,  1.24s/it]  6%|▌         | 2849/50000 [1:00:33<16:13:45,  1.24s/it]  6%|▌         | 2850/50000 [1:00:34<16:13:49,  1.24s/it]                                                         {'loss': 178.6844, 'learning_rate': 1.9997992048341687e-05, 'epoch': 0.44}
  6%|▌         | 2850/50000 [1:00:34<16:13:49,  1.24s/it]  6%|▌         | 2851/50000 [1:00:35<16:24:50,  1.25s/it]  6%|▌         | 2852/50000 [1:00:37<16:23:24,  1.25s/it]  6%|▌         | 2853/50000 [1:00:38<16:21:50,  1.25s/it]  6%|▌         | 2854/50000 [1:00:39<16:21:36,  1.25s/it]  6%|▌         | 2855/50000 [1:00:40<16:20:59,  1.25s/it]  6%|▌         | 2856/50000 [1:00:42<16:19:01,  1.25s/it]  6%|▌         | 2857/50000 [1:00:43<16:18:27,  1.25s/it]  6%|▌         | 2858/50000 [1:00:44<16:17:11,  1.24s/it]  6%|▌         | 2859/50000 [1:00:45<16:17:02,  1.24s/it]  6%|▌         | 2860/50000 [1:00:47<16:16:15,  1.24s/it]                                                         {'loss': 181.0437, 'learning_rate': 1.9997857327993074e-05, 'epoch': 0.44}
  6%|▌         | 2860/50000 [1:00:47<16:16:15,  1.24s/it]  6%|▌         | 2861/50000 [1:00:48<16:15:34,  1.24s/it]  6%|▌         | 2862/50000 [1:00:49<16:15:23,  1.24s/it]  6%|▌         | 2863/50000 [1:00:50<16:14:45,  1.24s/it]  6%|▌         | 2864/50000 [1:00:52<16:15:01,  1.24s/it]  6%|▌         | 2865/50000 [1:00:53<16:14:55,  1.24s/it]  6%|▌         | 2866/50000 [1:00:54<16:14:44,  1.24s/it]  6%|▌         | 2867/50000 [1:00:55<16:14:34,  1.24s/it]  6%|▌         | 2868/50000 [1:00:57<16:14:39,  1.24s/it]  6%|▌         | 2869/50000 [1:00:58<16:14:14,  1.24s/it]  6%|▌         | 2870/50000 [1:00:59<16:14:45,  1.24s/it]                                                         {'loss': 176.9125, 'learning_rate': 1.9997718234242e-05, 'epoch': 0.44}
  6%|▌         | 2870/50000 [1:00:59<16:14:45,  1.24s/it]  6%|▌         | 2871/50000 [1:01:00<16:15:13,  1.24s/it]  6%|▌         | 2872/50000 [1:01:02<16:14:47,  1.24s/it][2023-07-03 13:01:13,042] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, but hysteresis is 2. Reducing hysteresis to 1
  6%|▌         | 2873/50000 [1:01:03<15:15:57,  1.17s/it]  6%|▌         | 2874/50000 [1:01:04<15:33:45,  1.19s/it]  6%|▌         | 2875/50000 [1:01:05<15:46:05,  1.20s/it]  6%|▌         | 2876/50000 [1:01:06<15:54:30,  1.22s/it]  6%|▌         | 2877/50000 [1:01:07<16:00:17,  1.22s/it]  6%|▌         | 2878/50000 [1:01:09<16:05:02,  1.23s/it][2023-07-03 13:01:20,242] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  6%|▌         | 2879/50000 [1:01:10<15:09:40,  1.16s/it]  6%|▌         | 2880/50000 [1:01:11<15:29:12,  1.18s/it]                                                         {'loss': 171.2625, 'learning_rate': 1.9997603810432187e-05, 'epoch': 0.45}
  6%|▌         | 2880/50000 [1:01:11<15:29:12,  1.18s/it]  6%|▌         | 2881/50000 [1:01:12<15:43:11,  1.20s/it]  6%|▌         | 2882/50000 [1:01:13<15:52:18,  1.21s/it]  6%|▌         | 2883/50000 [1:01:15<15:58:36,  1.22s/it]  6%|▌         | 2884/50000 [1:01:16<16:03:21,  1.23s/it]  6%|▌         | 2885/50000 [1:01:17<16:06:30,  1.23s/it]  6%|▌         | 2886/50000 [1:01:18<16:08:51,  1.23s/it]  6%|▌         | 2887/50000 [1:01:20<16:10:23,  1.24s/it]  6%|▌         | 2888/50000 [1:01:21<16:11:32,  1.24s/it]  6%|▌         | 2889/50000 [1:01:22<16:12:04,  1.24s/it]  6%|▌         | 2890/50000 [1:01:23<16:13:12,  1.24s/it]                                                         {'loss': 157.5828, 'learning_rate': 1.999745684471129e-05, 'epoch': 0.45}
  6%|▌         | 2890/50000 [1:01:23<16:13:12,  1.24s/it]  6%|▌         | 2891/50000 [1:01:25<16:13:44,  1.24s/it]  6%|▌         | 2892/50000 [1:01:26<16:14:12,  1.24s/it]  6%|▌         | 2893/50000 [1:01:27<16:14:08,  1.24s/it]  6%|▌         | 2894/50000 [1:01:28<16:14:00,  1.24s/it]  6%|▌         | 2895/50000 [1:01:30<16:13:56,  1.24s/it]  6%|▌         | 2896/50000 [1:01:31<16:13:54,  1.24s/it]  6%|▌         | 2897/50000 [1:01:32<16:13:43,  1.24s/it]  6%|▌         | 2898/50000 [1:01:33<16:13:35,  1.24s/it]  6%|▌         | 2899/50000 [1:01:35<16:13:35,  1.24s/it]  6%|▌         | 2900/50000 [1:01:36<16:13:52,  1.24s/it]                                                         {'loss': 155.5297, 'learning_rate': 1.9997305505763115e-05, 'epoch': 0.45}
  6%|▌         | 2900/50000 [1:01:36<16:13:52,  1.24s/it]  6%|▌         | 2901/50000 [1:01:37<16:13:47,  1.24s/it]  6%|▌         | 2902/50000 [1:01:38<16:13:50,  1.24s/it]  6%|▌         | 2903/50000 [1:01:40<16:13:39,  1.24s/it]  6%|▌         | 2904/50000 [1:01:41<16:13:47,  1.24s/it]  6%|▌         | 2905/50000 [1:01:42<16:13:41,  1.24s/it]  6%|▌         | 2906/50000 [1:01:43<16:13:32,  1.24s/it]  6%|▌         | 2907/50000 [1:01:44<16:13:52,  1.24s/it]  6%|▌         | 2908/50000 [1:01:46<16:13:44,  1.24s/it]  6%|▌         | 2909/50000 [1:01:47<16:14:10,  1.24s/it]  6%|▌         | 2910/50000 [1:01:48<16:13:53,  1.24s/it]                                                         {'loss': 161.2578, 'learning_rate': 1.9997149793653862e-05, 'epoch': 0.45}
  6%|▌         | 2910/50000 [1:01:48<16:13:53,  1.24s/it]  6%|▌         | 2911/50000 [1:01:49<16:13:46,  1.24s/it]  6%|▌         | 2912/50000 [1:01:51<16:13:23,  1.24s/it]  6%|▌         | 2913/50000 [1:01:52<16:13:10,  1.24s/it]  6%|▌         | 2914/50000 [1:01:53<16:12:56,  1.24s/it]  6%|▌         | 2915/50000 [1:01:54<16:12:58,  1.24s/it]  6%|▌         | 2916/50000 [1:01:56<16:13:29,  1.24s/it]  6%|▌         | 2917/50000 [1:01:57<16:13:12,  1.24s/it]  6%|▌         | 2918/50000 [1:01:58<16:12:58,  1.24s/it]  6%|▌         | 2919/50000 [1:01:59<16:13:16,  1.24s/it]  6%|▌         | 2920/50000 [1:02:01<16:13:31,  1.24s/it]                                                         {'loss': 154.0109, 'learning_rate': 1.9996989708451644e-05, 'epoch': 0.45}
  6%|▌         | 2920/50000 [1:02:01<16:13:31,  1.24s/it]  6%|▌         | 2921/50000 [1:02:02<16:13:31,  1.24s/it]  6%|▌         | 2922/50000 [1:02:03<16:13:10,  1.24s/it]  6%|▌         | 2923/50000 [1:02:04<16:13:02,  1.24s/it]  6%|▌         | 2924/50000 [1:02:06<16:13:04,  1.24s/it]  6%|▌         | 2925/50000 [1:02:07<16:12:56,  1.24s/it]  6%|▌         | 2926/50000 [1:02:08<16:12:56,  1.24s/it]  6%|▌         | 2927/50000 [1:02:09<16:13:01,  1.24s/it]  6%|▌         | 2928/50000 [1:02:11<16:13:16,  1.24s/it]  6%|▌         | 2929/50000 [1:02:12<16:13:54,  1.24s/it]  6%|▌         | 2930/50000 [1:02:13<16:34:47,  1.27s/it]                                                         {'loss': 153.7766, 'learning_rate': 1.9996825250226492e-05, 'epoch': 0.45}
  6%|▌         | 2930/50000 [1:02:13<16:34:47,  1.27s/it]  6%|▌         | 2931/50000 [1:02:14<16:28:52,  1.26s/it]  6%|▌         | 2932/50000 [1:02:16<16:25:03,  1.26s/it]  6%|▌         | 2933/50000 [1:02:17<16:21:14,  1.25s/it]  6%|▌         | 2934/50000 [1:02:18<16:18:15,  1.25s/it]  6%|▌         | 2935/50000 [1:02:19<16:16:32,  1.24s/it]  6%|▌         | 2936/50000 [1:02:21<16:15:13,  1.24s/it]  6%|▌         | 2937/50000 [1:02:22<16:14:38,  1.24s/it]  6%|▌         | 2938/50000 [1:02:23<16:14:33,  1.24s/it]  6%|▌         | 2939/50000 [1:02:24<16:14:53,  1.24s/it]  6%|▌         | 2940/50000 [1:02:26<16:14:46,  1.24s/it]                                                         {'loss': 155.4563, 'learning_rate': 1.9996656419050334e-05, 'epoch': 0.46}
  6%|▌         | 2940/50000 [1:02:26<16:14:46,  1.24s/it]  6%|▌         | 2941/50000 [1:02:27<16:15:11,  1.24s/it]  6%|▌         | 2942/50000 [1:02:28<16:15:09,  1.24s/it]  6%|▌         | 2943/50000 [1:02:29<16:15:01,  1.24s/it]  6%|▌         | 2944/50000 [1:02:30<16:14:29,  1.24s/it]  6%|▌         | 2945/50000 [1:02:32<16:14:24,  1.24s/it]  6%|▌         | 2946/50000 [1:02:33<16:13:53,  1.24s/it]  6%|▌         | 2947/50000 [1:02:34<16:13:50,  1.24s/it]  6%|▌         | 2948/50000 [1:02:35<16:13:46,  1.24s/it]  6%|▌         | 2949/50000 [1:02:37<16:13:30,  1.24s/it]  6%|▌         | 2950/50000 [1:02:38<16:13:10,  1.24s/it]                                                         {'loss': 135.4094, 'learning_rate': 1.9996483214997034e-05, 'epoch': 0.46}
  6%|▌         | 2950/50000 [1:02:38<16:13:10,  1.24s/it]  6%|▌         | 2951/50000 [1:02:39<16:13:36,  1.24s/it]  6%|▌         | 2952/50000 [1:02:40<16:13:15,  1.24s/it]  6%|▌         | 2953/50000 [1:02:42<16:12:45,  1.24s/it]  6%|▌         | 2954/50000 [1:02:43<16:12:53,  1.24s/it]  6%|▌         | 2955/50000 [1:02:44<16:12:47,  1.24s/it]  6%|▌         | 2956/50000 [1:02:45<16:12:45,  1.24s/it]  6%|▌         | 2957/50000 [1:02:47<16:13:10,  1.24s/it]  6%|▌         | 2958/50000 [1:02:48<16:12:57,  1.24s/it]  6%|▌         | 2959/50000 [1:02:49<16:12:39,  1.24s/it]  6%|▌         | 2960/50000 [1:02:50<16:12:45,  1.24s/it]                                                         {'loss': 138.8797, 'learning_rate': 1.9996305638142358e-05, 'epoch': 0.46}
  6%|▌         | 2960/50000 [1:02:50<16:12:45,  1.24s/it]  6%|▌         | 2961/50000 [1:02:52<16:12:41,  1.24s/it]  6%|▌         | 2962/50000 [1:02:53<16:12:17,  1.24s/it]  6%|▌         | 2963/50000 [1:02:54<16:12:18,  1.24s/it]  6%|▌         | 2964/50000 [1:02:55<16:12:12,  1.24s/it]  6%|▌         | 2965/50000 [1:02:57<16:11:57,  1.24s/it]  6%|▌         | 2966/50000 [1:02:58<16:12:06,  1.24s/it]  6%|▌         | 2967/50000 [1:02:59<16:11:51,  1.24s/it]  6%|▌         | 2968/50000 [1:03:00<16:11:56,  1.24s/it]  6%|▌         | 2969/50000 [1:03:01<16:12:29,  1.24s/it]  6%|▌         | 2970/50000 [1:03:03<16:12:11,  1.24s/it]                                                         {'loss': 155.4078, 'learning_rate': 1.999612368856398e-05, 'epoch': 0.46}
  6%|▌         | 2970/50000 [1:03:03<16:12:11,  1.24s/it]  6%|▌         | 2971/50000 [1:03:04<16:12:18,  1.24s/it]  6%|▌         | 2972/50000 [1:03:05<16:12:13,  1.24s/it]  6%|▌         | 2973/50000 [1:03:06<16:12:24,  1.24s/it]  6%|▌         | 2974/50000 [1:03:08<16:12:19,  1.24s/it]  6%|▌         | 2975/50000 [1:03:09<16:11:56,  1.24s/it]  6%|▌         | 2976/50000 [1:03:10<16:12:18,  1.24s/it]  6%|▌         | 2977/50000 [1:03:11<16:12:09,  1.24s/it]  6%|▌         | 2978/50000 [1:03:13<16:12:27,  1.24s/it]  6%|▌         | 2979/50000 [1:03:14<16:12:22,  1.24s/it]  6%|▌         | 2980/50000 [1:03:15<16:12:21,  1.24s/it]                                                         {'loss': 160.0531, 'learning_rate': 1.9995937366341486e-05, 'epoch': 0.46}
  6%|▌         | 2980/50000 [1:03:15<16:12:21,  1.24s/it]  6%|▌         | 2981/50000 [1:03:16<16:12:38,  1.24s/it]  6%|▌         | 2982/50000 [1:03:18<16:12:22,  1.24s/it]  6%|▌         | 2983/50000 [1:03:19<16:11:56,  1.24s/it]  6%|▌         | 2984/50000 [1:03:20<16:11:54,  1.24s/it]  6%|▌         | 2985/50000 [1:03:21<16:11:49,  1.24s/it]  6%|▌         | 2986/50000 [1:03:23<16:12:02,  1.24s/it]  6%|▌         | 2987/50000 [1:03:24<16:11:56,  1.24s/it]  6%|▌         | 2988/50000 [1:03:25<16:11:41,  1.24s/it]  6%|▌         | 2989/50000 [1:03:26<16:11:44,  1.24s/it]  6%|▌         | 2990/50000 [1:03:28<16:11:46,  1.24s/it]                                                         {'loss': 156.4156, 'learning_rate': 1.999574667155639e-05, 'epoch': 0.46}
  6%|▌         | 2990/50000 [1:03:28<16:11:46,  1.24s/it]  6%|▌         | 2991/50000 [1:03:29<16:11:59,  1.24s/it]  6%|▌         | 2992/50000 [1:03:30<16:11:41,  1.24s/it]  6%|▌         | 2993/50000 [1:03:31<16:11:40,  1.24s/it]  6%|▌         | 2994/50000 [1:03:33<16:11:37,  1.24s/it]  6%|▌         | 2995/50000 [1:03:34<16:11:28,  1.24s/it]  6%|▌         | 2996/50000 [1:03:35<16:11:50,  1.24s/it]  6%|▌         | 2997/50000 [1:03:36<16:11:50,  1.24s/it]  6%|▌         | 2998/50000 [1:03:37<16:11:48,  1.24s/it]  6%|▌         | 2999/50000 [1:03:39<16:12:07,  1.24s/it]  6%|▌         | 3000/50000 [1:03:40<16:11:39,  1.24s/it]                                                         {'loss': 140.5781, 'learning_rate': 1.9995551604292097e-05, 'epoch': 0.47}
  6%|▌         | 3000/50000 [1:03:40<16:11:39,  1.24s/it][INFO|trainer.py:3129] 2023-07-03 13:03:50,467 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 13:03:50,467 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 13:03:50,468 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                         
                                             [A{'eval_loss': 156.125, 'eval_accuracy': 0.39643233478849915, 'eval_runtime': 3.1061, 'eval_samples_per_second': 8.371, 'eval_steps_per_second': 2.254, 'epoch': 0.47}
  6%|▌         | 3000/50000 [1:03:43<16:11:39,  1.24s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 13:03:53,575 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000
[INFO|trainer.py:2880] 2023-07-03 13:03:53,590 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 13:04:03,643 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 13:04:03,643 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/special_tokens_map.json
[2023-07-03 13:04:03,646] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 13:04:03,674] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt
[2023-07-03 13:04:03,674] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt...
[2023-07-03 13:04:14,061] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt.
[2023-07-03 13:04:14,064] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 13:04:30,664] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 13:04:30,664] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 13:04:30,664] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 13:04:32,347 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-2000] due to args.save_total_limit
  6%|▌         | 3001/50000 [1:04:32<216:58:46, 16.62s/it]  6%|▌         | 3002/50000 [1:04:34<156:44:30, 12.01s/it]  6%|▌         | 3003/50000 [1:04:35<114:34:30,  8.78s/it]  6%|▌         | 3004/50000 [1:04:36<85:34:21,  6.56s/it]   6%|▌         | 3005/50000 [1:04:38<64:45:19,  4.96s/it]  6%|▌         | 3006/50000 [1:04:39<50:09:37,  3.84s/it]  6%|▌         | 3007/50000 [1:04:40<40:16:50,  3.09s/it]  6%|▌         | 3008/50000 [1:04:41<33:02:27,  2.53s/it]  6%|▌         | 3009/50000 [1:04:43<27:57:51,  2.14s/it]  6%|▌         | 3010/50000 [1:04:44<24:24:42,  1.87s/it]                                                         {'loss': 123.7297, 'learning_rate': 1.9995352164633945e-05, 'epoch': 0.47}
  6%|▌         | 3010/50000 [1:04:44<24:24:42,  1.87s/it]  6%|▌         | 3011/50000 [1:04:45<21:55:10,  1.68s/it]  6%|▌         | 3012/50000 [1:04:46<20:10:17,  1.55s/it]  6%|▌         | 3013/50000 [1:04:48<18:56:57,  1.45s/it]  6%|▌         | 3014/50000 [1:04:49<18:05:41,  1.39s/it]  6%|▌         | 3015/50000 [1:04:50<17:30:19,  1.34s/it]  6%|▌         | 3016/50000 [1:04:51<17:04:57,  1.31s/it]  6%|▌         | 3017/50000 [1:04:52<16:47:26,  1.29s/it]  6%|▌         | 3018/50000 [1:04:54<16:35:10,  1.27s/it]  6%|▌         | 3019/50000 [1:04:55<16:26:14,  1.26s/it]  6%|▌         | 3020/50000 [1:04:56<16:20:13,  1.25s/it]                                                         {'loss': 157.8859, 'learning_rate': 1.9995148352669173e-05, 'epoch': 0.47}
  6%|▌         | 3020/50000 [1:04:56<16:20:13,  1.25s/it]  6%|▌         | 3021/50000 [1:04:57<16:15:54,  1.25s/it]  6%|▌         | 3022/50000 [1:04:59<16:12:49,  1.24s/it]  6%|▌         | 3023/50000 [1:05:00<16:10:45,  1.24s/it]  6%|▌         | 3024/50000 [1:05:01<16:09:37,  1.24s/it]  6%|▌         | 3025/50000 [1:05:02<16:08:38,  1.24s/it]  6%|▌         | 3026/50000 [1:05:04<16:07:47,  1.24s/it]  6%|▌         | 3027/50000 [1:05:05<16:07:05,  1.24s/it]  6%|▌         | 3028/50000 [1:05:06<16:07:20,  1.24s/it]  6%|▌         | 3029/50000 [1:05:07<16:06:52,  1.24s/it]  6%|▌         | 3030/50000 [1:05:08<16:06:40,  1.23s/it]                                                         {'loss': 152.4938, 'learning_rate': 1.9994940168486935e-05, 'epoch': 0.47}
  6%|▌         | 3030/50000 [1:05:08<16:06:40,  1.23s/it]  6%|▌         | 3031/50000 [1:05:10<16:06:55,  1.24s/it]  6%|▌         | 3032/50000 [1:05:11<16:06:55,  1.24s/it]  6%|▌         | 3033/50000 [1:05:12<16:06:32,  1.23s/it]  6%|▌         | 3034/50000 [1:05:13<16:06:26,  1.23s/it]  6%|▌         | 3035/50000 [1:05:15<16:06:20,  1.23s/it]  6%|▌         | 3036/50000 [1:05:16<16:32:01,  1.27s/it]  6%|▌         | 3037/50000 [1:05:17<16:24:49,  1.26s/it]  6%|▌         | 3038/50000 [1:05:18<16:19:16,  1.25s/it]  6%|▌         | 3039/50000 [1:05:20<16:16:54,  1.25s/it]  6%|▌         | 3040/50000 [1:05:21<16:15:59,  1.25s/it]                                                         {'loss': 158.6219, 'learning_rate': 1.9994727612178298e-05, 'epoch': 0.47}
  6%|▌         | 3040/50000 [1:05:21<16:15:59,  1.25s/it]  6%|▌         | 3041/50000 [1:05:22<16:16:14,  1.25s/it]  6%|▌         | 3042/50000 [1:05:23<16:15:08,  1.25s/it]  6%|▌         | 3043/50000 [1:05:25<16:18:02,  1.25s/it]  6%|▌         | 3044/50000 [1:05:26<16:16:07,  1.25s/it]  6%|▌         | 3045/50000 [1:05:27<16:14:24,  1.25s/it]  6%|▌         | 3046/50000 [1:05:28<16:13:27,  1.24s/it]  6%|▌         | 3047/50000 [1:05:30<16:13:00,  1.24s/it]  6%|▌         | 3048/50000 [1:05:31<16:12:29,  1.24s/it]  6%|▌         | 3049/50000 [1:05:32<16:12:07,  1.24s/it]  6%|▌         | 3050/50000 [1:05:33<16:11:38,  1.24s/it]                                                         {'loss': 154.4766, 'learning_rate': 1.999451068383624e-05, 'epoch': 0.47}
  6%|▌         | 3050/50000 [1:05:33<16:11:38,  1.24s/it]  6%|▌         | 3051/50000 [1:05:35<16:11:22,  1.24s/it]  6%|▌         | 3052/50000 [1:05:36<16:11:10,  1.24s/it]  6%|▌         | 3053/50000 [1:05:37<16:10:50,  1.24s/it]  6%|▌         | 3054/50000 [1:05:38<16:10:48,  1.24s/it]  6%|▌         | 3055/50000 [1:05:40<16:10:47,  1.24s/it]  6%|▌         | 3056/50000 [1:05:41<16:10:53,  1.24s/it]  6%|▌         | 3057/50000 [1:05:42<16:10:59,  1.24s/it]  6%|▌         | 3058/50000 [1:05:43<16:11:07,  1.24s/it]  6%|▌         | 3059/50000 [1:05:45<16:10:55,  1.24s/it]  6%|▌         | 3060/50000 [1:05:46<16:11:04,  1.24s/it]                                                         {'loss': 152.3187, 'learning_rate': 1.9994289383555657e-05, 'epoch': 0.47}
  6%|▌         | 3060/50000 [1:05:46<16:11:04,  1.24s/it]  6%|▌         | 3061/50000 [1:05:47<16:11:09,  1.24s/it]  6%|▌         | 3062/50000 [1:05:48<16:10:58,  1.24s/it]  6%|▌         | 3063/50000 [1:05:50<16:10:50,  1.24s/it]  6%|▌         | 3064/50000 [1:05:51<16:10:53,  1.24s/it]  6%|▌         | 3065/50000 [1:05:52<16:10:45,  1.24s/it]  6%|▌         | 3066/50000 [1:05:53<16:10:58,  1.24s/it]  6%|▌         | 3067/50000 [1:05:54<16:11:26,  1.24s/it]  6%|▌         | 3068/50000 [1:05:56<16:11:08,  1.24s/it]  6%|▌         | 3069/50000 [1:05:57<16:10:59,  1.24s/it]  6%|▌         | 3070/50000 [1:05:58<16:10:51,  1.24s/it]                                                         {'loss': 165.0016, 'learning_rate': 1.9994063711433348e-05, 'epoch': 0.48}
  6%|▌         | 3070/50000 [1:05:58<16:10:51,  1.24s/it]  6%|▌         | 3071/50000 [1:05:59<16:10:56,  1.24s/it]  6%|▌         | 3072/50000 [1:06:01<16:10:44,  1.24s/it]  6%|▌         | 3073/50000 [1:06:02<16:10:30,  1.24s/it]  6%|▌         | 3074/50000 [1:06:03<16:10:26,  1.24s/it]  6%|▌         | 3075/50000 [1:06:04<16:11:02,  1.24s/it]  6%|▌         | 3076/50000 [1:06:06<16:10:46,  1.24s/it]  6%|▌         | 3077/50000 [1:06:07<16:10:32,  1.24s/it]  6%|▌         | 3078/50000 [1:06:08<16:10:18,  1.24s/it]  6%|▌         | 3079/50000 [1:06:09<16:10:13,  1.24s/it]  6%|▌         | 3080/50000 [1:06:11<16:10:02,  1.24s/it]                                                         {'loss': 182.5797, 'learning_rate': 1.9993833667568033e-05, 'epoch': 0.48}
  6%|▌         | 3080/50000 [1:06:11<16:10:02,  1.24s/it]  6%|▌         | 3081/50000 [1:06:12<16:10:17,  1.24s/it]  6%|▌         | 3082/50000 [1:06:13<16:10:10,  1.24s/it]  6%|▌         | 3083/50000 [1:06:14<16:10:10,  1.24s/it]  6%|▌         | 3084/50000 [1:06:16<16:10:00,  1.24s/it]  6%|▌         | 3085/50000 [1:06:17<16:10:22,  1.24s/it]  6%|▌         | 3086/50000 [1:06:18<16:10:16,  1.24s/it]  6%|▌         | 3087/50000 [1:06:19<16:09:56,  1.24s/it]  6%|▌         | 3088/50000 [1:06:21<16:09:53,  1.24s/it]  6%|▌         | 3089/50000 [1:06:22<16:09:52,  1.24s/it]  6%|▌         | 3090/50000 [1:06:23<16:09:41,  1.24s/it]                                                         {'loss': 156.1133, 'learning_rate': 1.9993599252060337e-05, 'epoch': 0.48}
  6%|▌         | 3090/50000 [1:06:23<16:09:41,  1.24s/it]  6%|▌         | 3091/50000 [1:06:24<16:10:02,  1.24s/it]  6%|▌         | 3092/50000 [1:06:26<16:10:03,  1.24s/it]  6%|▌         | 3093/50000 [1:06:27<16:10:22,  1.24s/it]  6%|▌         | 3094/50000 [1:06:28<16:10:11,  1.24s/it]  6%|▌         | 3095/50000 [1:06:29<16:09:57,  1.24s/it]  6%|▌         | 3096/50000 [1:06:30<16:09:55,  1.24s/it]  6%|▌         | 3097/50000 [1:06:32<16:10:01,  1.24s/it]  6%|▌         | 3098/50000 [1:06:33<16:09:56,  1.24s/it]  6%|▌         | 3099/50000 [1:06:34<16:09:56,  1.24s/it][2023-07-03 13:06:45,712] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, but hysteresis is 2. Reducing hysteresis to 1
  6%|▌         | 3100/50000 [1:06:35<15:11:34,  1.17s/it]                                                         {'loss': 170.768, 'learning_rate': 1.9993384540433892e-05, 'epoch': 0.48}
  6%|▌         | 3100/50000 [1:06:35<15:11:34,  1.17s/it]  6%|▌         | 3101/50000 [1:06:36<15:29:01,  1.19s/it]  6%|▌         | 3102/50000 [1:06:38<15:41:18,  1.20s/it]  6%|▌         | 3103/50000 [1:06:39<15:49:39,  1.21s/it]  6%|▌         | 3104/50000 [1:06:40<15:55:42,  1.22s/it]  6%|▌         | 3105/50000 [1:06:41<15:59:46,  1.23s/it]  6%|▌         | 3106/50000 [1:06:43<16:02:42,  1.23s/it]  6%|▌         | 3107/50000 [1:06:44<16:04:37,  1.23s/it]  6%|▌         | 3108/50000 [1:06:45<16:06:15,  1.24s/it]  6%|▌         | 3109/50000 [1:06:46<16:07:06,  1.24s/it]  6%|▌         | 3110/50000 [1:06:48<16:08:17,  1.24s/it]                                                         {'loss': 145.4484, 'learning_rate': 1.9993141819089766e-05, 'epoch': 0.48}
  6%|▌         | 3110/50000 [1:06:48<16:08:17,  1.24s/it]  6%|▌         | 3111/50000 [1:06:49<16:08:42,  1.24s/it]  6%|▌         | 3112/50000 [1:06:50<16:08:55,  1.24s/it]  6%|▌         | 3113/50000 [1:06:51<16:09:09,  1.24s/it]  6%|▌         | 3114/50000 [1:06:53<16:09:17,  1.24s/it]  6%|▌         | 3115/50000 [1:06:54<16:09:12,  1.24s/it]  6%|▌         | 3116/50000 [1:06:55<16:09:09,  1.24s/it]  6%|▌         | 3117/50000 [1:06:56<16:09:05,  1.24s/it]  6%|▌         | 3118/50000 [1:06:58<16:09:05,  1.24s/it]  6%|▌         | 3119/50000 [1:06:59<16:08:55,  1.24s/it]  6%|▌         | 3120/50000 [1:07:00<16:09:08,  1.24s/it]                                                         {'loss': 135.307, 'learning_rate': 1.9992894726405894e-05, 'epoch': 0.48}
  6%|▌         | 3120/50000 [1:07:00<16:09:08,  1.24s/it]  6%|▌         | 3121/50000 [1:07:01<16:09:25,  1.24s/it]  6%|▌         | 3122/50000 [1:07:02<16:09:12,  1.24s/it]  6%|▌         | 3123/50000 [1:07:04<16:09:18,  1.24s/it]  6%|▌         | 3124/50000 [1:07:05<16:09:11,  1.24s/it]  6%|▋         | 3125/50000 [1:07:06<16:09:04,  1.24s/it]  6%|▋         | 3126/50000 [1:07:07<16:09:02,  1.24s/it]  6%|▋         | 3127/50000 [1:07:09<16:09:25,  1.24s/it]  6%|▋         | 3128/50000 [1:07:10<16:10:00,  1.24s/it]  6%|▋         | 3129/50000 [1:07:11<16:10:01,  1.24s/it]  6%|▋         | 3130/50000 [1:07:12<16:09:49,  1.24s/it]                                                         {'loss': 146.9453, 'learning_rate': 1.999264326249037e-05, 'epoch': 0.49}
  6%|▋         | 3130/50000 [1:07:12<16:09:49,  1.24s/it]  6%|▋         | 3131/50000 [1:07:14<16:09:34,  1.24s/it]  6%|▋         | 3132/50000 [1:07:15<16:09:27,  1.24s/it]  6%|▋         | 3133/50000 [1:07:16<16:09:44,  1.24s/it]  6%|▋         | 3134/50000 [1:07:17<16:09:27,  1.24s/it]  6%|▋         | 3135/50000 [1:07:19<16:09:16,  1.24s/it]  6%|▋         | 3136/50000 [1:07:20<16:09:14,  1.24s/it]  6%|▋         | 3137/50000 [1:07:21<16:08:42,  1.24s/it]  6%|▋         | 3138/50000 [1:07:22<16:08:29,  1.24s/it]  6%|▋         | 3139/50000 [1:07:24<16:08:26,  1.24s/it]  6%|▋         | 3140/50000 [1:07:25<16:08:40,  1.24s/it]                                                         {'loss': 146.6656, 'learning_rate': 1.999238742745319e-05, 'epoch': 0.49}
  6%|▋         | 3140/50000 [1:07:25<16:08:40,  1.24s/it]  6%|▋         | 3141/50000 [1:07:26<16:08:45,  1.24s/it]  6%|▋         | 3142/50000 [1:07:27<16:08:44,  1.24s/it]  6%|▋         | 3143/50000 [1:07:29<16:08:50,  1.24s/it]  6%|▋         | 3144/50000 [1:07:30<16:08:43,  1.24s/it]  6%|▋         | 3145/50000 [1:07:31<16:08:34,  1.24s/it]  6%|▋         | 3146/50000 [1:07:32<16:08:23,  1.24s/it]  6%|▋         | 3147/50000 [1:07:34<16:08:22,  1.24s/it]  6%|▋         | 3148/50000 [1:07:35<16:08:27,  1.24s/it]  6%|▋         | 3149/50000 [1:07:36<16:08:29,  1.24s/it]  6%|▋         | 3150/50000 [1:07:37<16:08:24,  1.24s/it]                                                         {'loss': 133.5531, 'learning_rate': 1.9992127221406276e-05, 'epoch': 0.49}
  6%|▋         | 3150/50000 [1:07:37<16:08:24,  1.24s/it][2023-07-03 13:07:48,738] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
  6%|▋         | 3151/50000 [1:07:38<15:10:47,  1.17s/it]  6%|▋         | 3152/50000 [1:07:39<15:27:57,  1.19s/it]  6%|▋         | 3153/50000 [1:07:41<15:40:19,  1.20s/it]  6%|▋         | 3154/50000 [1:07:42<15:48:36,  1.21s/it]  6%|▋         | 3155/50000 [1:07:43<15:54:38,  1.22s/it]  6%|▋         | 3156/50000 [1:07:44<15:58:26,  1.23s/it]  6%|▋         | 3157/50000 [1:07:46<16:01:17,  1.23s/it]  6%|▋         | 3158/50000 [1:07:47<16:03:29,  1.23s/it]  6%|▋         | 3159/50000 [1:07:48<16:05:24,  1.24s/it]  6%|▋         | 3160/50000 [1:07:49<16:06:16,  1.24s/it]                                                         {'loss': 141.1641, 'learning_rate': 1.9991889298844748e-05, 'epoch': 0.49}
  6%|▋         | 3160/50000 [1:07:49<16:06:16,  1.24s/it]  6%|▋         | 3161/50000 [1:07:51<16:07:06,  1.24s/it]  6%|▋         | 3162/50000 [1:07:52<16:07:33,  1.24s/it]  6%|▋         | 3163/50000 [1:07:53<16:07:38,  1.24s/it]  6%|▋         | 3164/50000 [1:07:54<16:07:51,  1.24s/it]  6%|▋         | 3165/50000 [1:07:56<16:07:55,  1.24s/it][2023-07-03 13:08:07,097] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  6%|▋         | 3166/50000 [1:07:57<15:09:52,  1.17s/it]  6%|▋         | 3167/50000 [1:07:58<15:27:12,  1.19s/it]  6%|▋         | 3168/50000 [1:07:59<15:39:46,  1.20s/it]  6%|▋         | 3169/50000 [1:08:00<15:48:26,  1.22s/it]  6%|▋         | 3170/50000 [1:08:02<15:54:24,  1.22s/it]                                                         {'loss': 167.8562, 'learning_rate': 1.9991647835941805e-05, 'epoch': 0.49}
  6%|▋         | 3170/50000 [1:08:02<15:54:24,  1.22s/it]  6%|▋         | 3171/50000 [1:08:03<15:58:47,  1.23s/it]  6%|▋         | 3172/50000 [1:08:04<16:01:33,  1.23s/it]  6%|▋         | 3173/50000 [1:08:05<16:03:42,  1.23s/it]  6%|▋         | 3174/50000 [1:08:07<16:04:59,  1.24s/it]  6%|▋         | 3175/50000 [1:08:08<16:05:48,  1.24s/it]  6%|▋         | 3176/50000 [1:08:09<16:06:36,  1.24s/it]  6%|▋         | 3177/50000 [1:08:10<16:06:58,  1.24s/it]  6%|▋         | 3178/50000 [1:08:11<16:07:26,  1.24s/it]  6%|▋         | 3179/50000 [1:08:13<16:07:33,  1.24s/it]  6%|▋         | 3180/50000 [1:08:14<16:07:34,  1.24s/it]                                                         {'loss': 178.4766, 'learning_rate': 1.9991375391679244e-05, 'epoch': 0.49}
  6%|▋         | 3180/50000 [1:08:14<16:07:34,  1.24s/it]  6%|▋         | 3181/50000 [1:08:15<16:07:59,  1.24s/it]  6%|▋         | 3182/50000 [1:08:16<16:07:42,  1.24s/it]  6%|▋         | 3183/50000 [1:08:18<16:07:37,  1.24s/it]  6%|▋         | 3184/50000 [1:08:19<16:07:57,  1.24s/it]  6%|▋         | 3185/50000 [1:08:20<16:07:53,  1.24s/it]  6%|▋         | 3186/50000 [1:08:21<16:08:29,  1.24s/it][2023-07-03 13:08:32,905] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
  6%|▋         | 3187/50000 [1:08:22<15:10:07,  1.17s/it]  6%|▋         | 3188/50000 [1:08:24<15:27:15,  1.19s/it]  6%|▋         | 3189/50000 [1:08:25<15:39:29,  1.20s/it]  6%|▋         | 3190/50000 [1:08:26<15:48:29,  1.22s/it]                                                         {'loss': 193.0687, 'learning_rate': 1.999112645500468e-05, 'epoch': 0.49}
  6%|▋         | 3190/50000 [1:08:26<15:48:29,  1.22s/it]  6%|▋         | 3191/50000 [1:08:27<15:54:22,  1.22s/it]  6%|▋         | 3192/50000 [1:08:29<15:58:11,  1.23s/it]  6%|▋         | 3193/50000 [1:08:30<16:01:11,  1.23s/it]  6%|▋         | 3194/50000 [1:08:31<16:03:33,  1.24s/it]  6%|▋         | 3195/50000 [1:08:32<16:04:39,  1.24s/it]  6%|▋         | 3196/50000 [1:08:34<16:05:29,  1.24s/it]  6%|▋         | 3197/50000 [1:08:35<16:06:00,  1.24s/it]  6%|▋         | 3198/50000 [1:08:36<16:06:28,  1.24s/it]  6%|▋         | 3199/50000 [1:08:37<16:06:44,  1.24s/it]  6%|▋         | 3200/50000 [1:08:39<16:07:48,  1.24s/it]                                                         {'loss': 189.9328, 'learning_rate': 1.999084570676821e-05, 'epoch': 0.5}
  6%|▋         | 3200/50000 [1:08:39<16:07:48,  1.24s/it]  6%|▋         | 3201/50000 [1:08:40<16:08:54,  1.24s/it]  6%|▋         | 3202/50000 [1:08:41<16:08:56,  1.24s/it]  6%|▋         | 3203/50000 [1:08:42<16:08:32,  1.24s/it]  6%|▋         | 3204/50000 [1:08:43<16:08:10,  1.24s/it]  6%|▋         | 3205/50000 [1:08:45<16:08:00,  1.24s/it]  6%|▋         | 3206/50000 [1:08:46<16:07:51,  1.24s/it]  6%|▋         | 3207/50000 [1:08:47<16:07:42,  1.24s/it]  6%|▋         | 3208/50000 [1:08:48<16:07:27,  1.24s/it]  6%|▋         | 3209/50000 [1:08:50<16:07:28,  1.24s/it]  6%|▋         | 3210/50000 [1:08:51<16:07:22,  1.24s/it]                                                         {'loss': 175.7484, 'learning_rate': 1.9990560588196398e-05, 'epoch': 0.5}
  6%|▋         | 3210/50000 [1:08:51<16:07:22,  1.24s/it]  6%|▋         | 3211/50000 [1:08:52<16:07:48,  1.24s/it]  6%|▋         | 3212/50000 [1:08:53<16:07:35,  1.24s/it]  6%|▋         | 3213/50000 [1:08:55<16:07:22,  1.24s/it]  6%|▋         | 3214/50000 [1:08:56<16:07:06,  1.24s/it]  6%|▋         | 3215/50000 [1:08:57<16:07:04,  1.24s/it]  6%|▋         | 3216/50000 [1:08:58<16:07:00,  1.24s/it]  6%|▋         | 3217/50000 [1:09:00<16:07:04,  1.24s/it]  6%|▋         | 3218/50000 [1:09:01<16:07:09,  1.24s/it]  6%|▋         | 3219/50000 [1:09:02<16:07:22,  1.24s/it]  6%|▋         | 3220/50000 [1:09:03<16:07:14,  1.24s/it]                                                         {'loss': 148.8844, 'learning_rate': 1.999027109941396e-05, 'epoch': 0.5}
  6%|▋         | 3220/50000 [1:09:03<16:07:14,  1.24s/it]  6%|▋         | 3221/50000 [1:09:05<16:07:19,  1.24s/it]  6%|▋         | 3222/50000 [1:09:06<16:07:09,  1.24s/it]  6%|▋         | 3223/50000 [1:09:07<16:07:02,  1.24s/it]  6%|▋         | 3224/50000 [1:09:08<16:06:51,  1.24s/it]  6%|▋         | 3225/50000 [1:09:10<16:07:08,  1.24s/it]  6%|▋         | 3226/50000 [1:09:11<16:50:22,  1.30s/it]  6%|▋         | 3227/50000 [1:09:12<16:38:08,  1.28s/it]  6%|▋         | 3228/50000 [1:09:13<16:29:19,  1.27s/it]  6%|▋         | 3229/50000 [1:09:15<16:44:00,  1.29s/it]  6%|▋         | 3230/50000 [1:09:16<16:33:36,  1.27s/it]                                                         {'loss': 161.7078, 'learning_rate': 1.998997724054754e-05, 'epoch': 0.5}
  6%|▋         | 3230/50000 [1:09:16<16:33:36,  1.27s/it]  6%|▋         | 3231/50000 [1:09:17<16:28:08,  1.27s/it]  6%|▋         | 3232/50000 [1:09:19<16:35:50,  1.28s/it]  6%|▋         | 3233/50000 [1:09:20<16:30:10,  1.27s/it]  6%|▋         | 3234/50000 [1:09:21<16:25:14,  1.26s/it]  6%|▋         | 3235/50000 [1:09:22<16:21:15,  1.26s/it]  6%|▋         | 3236/50000 [1:09:24<16:19:11,  1.26s/it]  6%|▋         | 3237/50000 [1:09:25<16:17:48,  1.25s/it]  6%|▋         | 3238/50000 [1:09:26<16:16:02,  1.25s/it]  6%|▋         | 3239/50000 [1:09:27<16:13:55,  1.25s/it]  6%|▋         | 3240/50000 [1:09:29<16:12:10,  1.25s/it]                                                         {'loss': 148.0312, 'learning_rate': 1.998967901172567e-05, 'epoch': 0.5}
  6%|▋         | 3240/50000 [1:09:29<16:12:10,  1.25s/it]  6%|▋         | 3241/50000 [1:09:30<16:11:39,  1.25s/it]  6%|▋         | 3242/50000 [1:09:31<16:10:55,  1.25s/it]  6%|▋         | 3243/50000 [1:09:32<16:10:40,  1.25s/it]  6%|▋         | 3244/50000 [1:09:34<16:09:42,  1.24s/it]  6%|▋         | 3245/50000 [1:09:35<16:10:31,  1.25s/it]  6%|▋         | 3246/50000 [1:09:36<16:10:24,  1.25s/it]  6%|▋         | 3247/50000 [1:09:37<16:09:58,  1.24s/it]  6%|▋         | 3248/50000 [1:09:39<16:09:07,  1.24s/it]  6%|▋         | 3249/50000 [1:09:40<16:08:51,  1.24s/it]  6%|▋         | 3250/50000 [1:09:41<16:08:49,  1.24s/it]                                                         {'loss': 152.8219, 'learning_rate': 1.9989376413078808e-05, 'epoch': 0.5}
  6%|▋         | 3250/50000 [1:09:41<16:08:49,  1.24s/it]  7%|▋         | 3251/50000 [1:09:42<16:08:24,  1.24s/it]  7%|▋         | 3252/50000 [1:09:43<16:08:00,  1.24s/it]  7%|▋         | 3253/50000 [1:09:45<16:08:11,  1.24s/it]  7%|▋         | 3254/50000 [1:09:46<16:07:48,  1.24s/it]  7%|▋         | 3255/50000 [1:09:47<16:07:29,  1.24s/it]  7%|▋         | 3256/50000 [1:09:48<16:07:12,  1.24s/it]  7%|▋         | 3257/50000 [1:09:50<16:08:37,  1.24s/it]  7%|▋         | 3258/50000 [1:09:51<16:07:54,  1.24s/it]  7%|▋         | 3259/50000 [1:09:52<16:08:10,  1.24s/it]  7%|▋         | 3260/50000 [1:09:53<16:07:43,  1.24s/it]                                                         {'loss': 155.1656, 'learning_rate': 1.9989069444739325e-05, 'epoch': 0.51}
  7%|▋         | 3260/50000 [1:09:53<16:07:43,  1.24s/it]  7%|▋         | 3261/50000 [1:09:55<16:07:20,  1.24s/it]  7%|▋         | 3262/50000 [1:09:56<16:07:03,  1.24s/it]  7%|▋         | 3263/50000 [1:09:57<16:06:53,  1.24s/it]  7%|▋         | 3264/50000 [1:09:58<16:06:27,  1.24s/it]  7%|▋         | 3265/50000 [1:10:00<16:06:12,  1.24s/it]  7%|▋         | 3266/50000 [1:10:01<16:06:49,  1.24s/it]  7%|▋         | 3267/50000 [1:10:02<16:06:25,  1.24s/it]  7%|▋         | 3268/50000 [1:10:03<16:06:20,  1.24s/it]  7%|▋         | 3269/50000 [1:10:05<16:06:23,  1.24s/it]  7%|▋         | 3270/50000 [1:10:06<16:06:38,  1.24s/it]                                                         {'loss': 167.9828, 'learning_rate': 1.9988758106841496e-05, 'epoch': 0.51}
  7%|▋         | 3270/50000 [1:10:06<16:06:38,  1.24s/it]  7%|▋         | 3271/50000 [1:10:07<16:06:48,  1.24s/it]  7%|▋         | 3272/50000 [1:10:08<16:06:37,  1.24s/it]  7%|▋         | 3273/50000 [1:10:10<16:06:38,  1.24s/it]  7%|▋         | 3274/50000 [1:10:11<16:07:06,  1.24s/it]  7%|▋         | 3275/50000 [1:10:12<16:07:02,  1.24s/it]  7%|▋         | 3276/50000 [1:10:13<16:07:30,  1.24s/it]  7%|▋         | 3277/50000 [1:10:15<16:07:09,  1.24s/it]  7%|▋         | 3278/50000 [1:10:16<16:07:01,  1.24s/it]  7%|▋         | 3279/50000 [1:10:17<16:06:44,  1.24s/it]  7%|▋         | 3280/50000 [1:10:18<16:06:32,  1.24s/it]                                                         {'loss': 144.0, 'learning_rate': 1.9988442399521512e-05, 'epoch': 0.51}
  7%|▋         | 3280/50000 [1:10:18<16:06:32,  1.24s/it]  7%|▋         | 3281/50000 [1:10:19<16:06:42,  1.24s/it]  7%|▋         | 3282/50000 [1:10:21<16:07:13,  1.24s/it]  7%|▋         | 3283/50000 [1:10:22<16:07:10,  1.24s/it]  7%|▋         | 3284/50000 [1:10:23<16:06:59,  1.24s/it]  7%|▋         | 3285/50000 [1:10:24<16:06:49,  1.24s/it]  7%|▋         | 3286/50000 [1:10:26<16:06:22,  1.24s/it]  7%|▋         | 3287/50000 [1:10:27<16:06:05,  1.24s/it]  7%|▋         | 3288/50000 [1:10:28<16:06:34,  1.24s/it]  7%|▋         | 3289/50000 [1:10:29<16:06:11,  1.24s/it]  7%|▋         | 3290/50000 [1:10:31<16:06:47,  1.24s/it]                                                         {'loss': 158.3516, 'learning_rate': 1.9988122322917473e-05, 'epoch': 0.51}
  7%|▋         | 3290/50000 [1:10:31<16:06:47,  1.24s/it]  7%|▋         | 3291/50000 [1:10:32<16:07:07,  1.24s/it]  7%|▋         | 3292/50000 [1:10:33<16:06:40,  1.24s/it]  7%|▋         | 3293/50000 [1:10:34<16:06:26,  1.24s/it]  7%|▋         | 3294/50000 [1:10:36<16:06:11,  1.24s/it]  7%|▋         | 3295/50000 [1:10:37<16:06:11,  1.24s/it]  7%|▋         | 3296/50000 [1:10:38<16:06:11,  1.24s/it]  7%|▋         | 3297/50000 [1:10:39<16:05:55,  1.24s/it]  7%|▋         | 3298/50000 [1:10:41<16:06:51,  1.24s/it]  7%|▋         | 3299/50000 [1:10:42<16:08:02,  1.24s/it]  7%|▋         | 3300/50000 [1:10:43<16:07:57,  1.24s/it]                                                         {'loss': 153.6641, 'learning_rate': 1.9987797877169394e-05, 'epoch': 0.51}
  7%|▋         | 3300/50000 [1:10:43<16:07:57,  1.24s/it]  7%|▋         | 3301/50000 [1:10:44<16:07:31,  1.24s/it]  7%|▋         | 3302/50000 [1:10:46<16:07:02,  1.24s/it]  7%|▋         | 3303/50000 [1:10:47<16:06:27,  1.24s/it]  7%|▋         | 3304/50000 [1:10:48<16:06:54,  1.24s/it]  7%|▋         | 3305/50000 [1:10:49<16:06:45,  1.24s/it]  7%|▋         | 3306/50000 [1:10:51<16:06:42,  1.24s/it]  7%|▋         | 3307/50000 [1:10:52<16:06:45,  1.24s/it]  7%|▋         | 3308/50000 [1:10:53<16:06:21,  1.24s/it]  7%|▋         | 3309/50000 [1:10:54<16:06:04,  1.24s/it]  7%|▋         | 3310/50000 [1:10:56<16:05:55,  1.24s/it]                                                         {'loss': 164.7719, 'learning_rate': 1.9987469062419196e-05, 'epoch': 0.51}
  7%|▋         | 3310/50000 [1:10:56<16:05:55,  1.24s/it]  7%|▋         | 3311/50000 [1:10:57<16:05:49,  1.24s/it]  7%|▋         | 3312/50000 [1:10:58<16:05:44,  1.24s/it]  7%|▋         | 3313/50000 [1:10:59<16:05:40,  1.24s/it]  7%|▋         | 3314/50000 [1:11:00<16:05:40,  1.24s/it]  7%|▋         | 3315/50000 [1:11:02<16:05:44,  1.24s/it]  7%|▋         | 3316/50000 [1:11:03<16:05:45,  1.24s/it]  7%|▋         | 3317/50000 [1:11:04<16:05:57,  1.24s/it]  7%|▋         | 3318/50000 [1:11:05<16:18:15,  1.26s/it]  7%|▋         | 3319/50000 [1:11:07<16:14:09,  1.25s/it]  7%|▋         | 3320/50000 [1:11:08<16:12:44,  1.25s/it]                                                         {'loss': 134.8648, 'learning_rate': 1.9987135878810716e-05, 'epoch': 0.51}
  7%|▋         | 3320/50000 [1:11:08<16:12:44,  1.25s/it]  7%|▋         | 3321/50000 [1:11:09<16:22:18,  1.26s/it]  7%|▋         | 3322/50000 [1:11:11<16:17:42,  1.26s/it]  7%|▋         | 3323/50000 [1:11:12<16:15:36,  1.25s/it]  7%|▋         | 3324/50000 [1:11:13<16:32:04,  1.28s/it]  7%|▋         | 3325/50000 [1:11:14<16:29:04,  1.27s/it]  7%|▋         | 3326/50000 [1:11:16<16:22:16,  1.26s/it]  7%|▋         | 3327/50000 [1:11:17<16:17:25,  1.26s/it]  7%|▋         | 3328/50000 [1:11:18<16:13:56,  1.25s/it]  7%|▋         | 3329/50000 [1:11:19<16:13:53,  1.25s/it]  7%|▋         | 3330/50000 [1:11:21<16:10:55,  1.25s/it]                                                         {'loss': 152.268, 'learning_rate': 1.9986798326489694e-05, 'epoch': 0.52}
  7%|▋         | 3330/50000 [1:11:21<16:10:55,  1.25s/it]  7%|▋         | 3331/50000 [1:11:22<16:11:02,  1.25s/it]  7%|▋         | 3332/50000 [1:11:23<16:09:38,  1.25s/it]  7%|▋         | 3333/50000 [1:11:24<16:08:28,  1.25s/it]  7%|▋         | 3334/50000 [1:11:26<16:08:27,  1.25s/it]  7%|▋         | 3335/50000 [1:11:27<16:07:53,  1.24s/it]  7%|▋         | 3336/50000 [1:11:28<16:07:14,  1.24s/it]  7%|▋         | 3337/50000 [1:11:29<16:06:35,  1.24s/it]  7%|▋         | 3338/50000 [1:11:31<16:06:15,  1.24s/it]  7%|▋         | 3339/50000 [1:11:32<16:06:18,  1.24s/it]  7%|▋         | 3340/50000 [1:11:33<16:05:51,  1.24s/it]                                                         {'loss': 127.8031, 'learning_rate': 1.9986456405603792e-05, 'epoch': 0.52}
  7%|▋         | 3340/50000 [1:11:33<16:05:51,  1.24s/it]  7%|▋         | 3341/50000 [1:11:34<16:05:34,  1.24s/it]  7%|▋         | 3342/50000 [1:11:35<16:05:13,  1.24s/it]  7%|▋         | 3343/50000 [1:11:37<16:04:49,  1.24s/it]  7%|▋         | 3344/50000 [1:11:38<16:04:45,  1.24s/it]  7%|▋         | 3345/50000 [1:11:39<16:04:40,  1.24s/it]  7%|▋         | 3346/50000 [1:11:40<16:04:50,  1.24s/it]  7%|▋         | 3347/50000 [1:11:42<16:05:24,  1.24s/it]  7%|▋         | 3348/50000 [1:11:43<16:05:22,  1.24s/it]  7%|▋         | 3349/50000 [1:11:44<16:05:07,  1.24s/it]  7%|▋         | 3350/50000 [1:11:45<16:04:54,  1.24s/it]                                                         {'loss': 139.7438, 'learning_rate': 1.9986110116302583e-05, 'epoch': 0.52}
  7%|▋         | 3350/50000 [1:11:45<16:04:54,  1.24s/it]  7%|▋         | 3351/50000 [1:11:47<16:04:47,  1.24s/it]  7%|▋         | 3352/50000 [1:11:48<16:04:51,  1.24s/it]  7%|▋         | 3353/50000 [1:11:49<16:04:52,  1.24s/it]  7%|▋         | 3354/50000 [1:11:50<16:05:11,  1.24s/it]  7%|▋         | 3355/50000 [1:11:52<16:06:11,  1.24s/it]  7%|▋         | 3356/50000 [1:11:53<16:06:51,  1.24s/it]  7%|▋         | 3357/50000 [1:11:54<16:08:12,  1.25s/it]  7%|▋         | 3358/50000 [1:11:55<16:10:53,  1.25s/it]  7%|▋         | 3359/50000 [1:11:57<16:09:18,  1.25s/it]  7%|▋         | 3360/50000 [1:11:58<16:08:04,  1.25s/it]                                                         {'loss': 160.3984, 'learning_rate': 1.9985759458737536e-05, 'epoch': 0.52}
  7%|▋         | 3360/50000 [1:11:58<16:08:04,  1.25s/it]  7%|▋         | 3361/50000 [1:11:59<16:07:36,  1.24s/it]  7%|▋         | 3362/50000 [1:12:00<16:07:41,  1.24s/it]  7%|▋         | 3363/50000 [1:12:02<16:07:08,  1.24s/it]  7%|▋         | 3364/50000 [1:12:03<16:06:38,  1.24s/it]  7%|▋         | 3365/50000 [1:12:04<16:07:04,  1.24s/it]  7%|▋         | 3366/50000 [1:12:05<16:07:05,  1.24s/it]  7%|▋         | 3367/50000 [1:12:07<16:07:30,  1.24s/it]  7%|▋         | 3368/50000 [1:12:08<16:06:40,  1.24s/it]  7%|▋         | 3369/50000 [1:12:09<16:15:06,  1.25s/it]  7%|▋         | 3370/50000 [1:12:10<16:12:20,  1.25s/it]                                                         {'loss': 157.6906, 'learning_rate': 1.998540443306204e-05, 'epoch': 0.52}
  7%|▋         | 3370/50000 [1:12:10<16:12:20,  1.25s/it]  7%|▋         | 3371/50000 [1:12:12<16:14:03,  1.25s/it]  7%|▋         | 3372/50000 [1:12:13<16:14:07,  1.25s/it]  7%|▋         | 3373/50000 [1:12:14<16:13:56,  1.25s/it]  7%|▋         | 3374/50000 [1:12:15<16:14:16,  1.25s/it]  7%|▋         | 3375/50000 [1:12:17<16:13:15,  1.25s/it]  7%|▋         | 3376/50000 [1:12:18<16:15:00,  1.25s/it]  7%|▋         | 3377/50000 [1:12:19<16:32:17,  1.28s/it]  7%|▋         | 3378/50000 [1:12:20<16:27:01,  1.27s/it]  7%|▋         | 3379/50000 [1:12:22<16:23:13,  1.27s/it]  7%|▋         | 3380/50000 [1:12:23<16:21:07,  1.26s/it]                                                         {'loss': 155.8516, 'learning_rate': 1.9985045039431402e-05, 'epoch': 0.52}
  7%|▋         | 3380/50000 [1:12:23<16:21:07,  1.26s/it]  7%|▋         | 3381/50000 [1:12:24<16:19:02,  1.26s/it]  7%|▋         | 3382/50000 [1:12:25<16:16:51,  1.26s/it]  7%|▋         | 3383/50000 [1:12:27<16:12:54,  1.25s/it]  7%|▋         | 3384/50000 [1:12:28<16:09:19,  1.25s/it]  7%|▋         | 3385/50000 [1:12:29<16:06:21,  1.24s/it]  7%|▋         | 3386/50000 [1:12:30<16:04:41,  1.24s/it]  7%|▋         | 3387/50000 [1:12:32<16:04:27,  1.24s/it]  7%|▋         | 3388/50000 [1:12:33<16:04:49,  1.24s/it]  7%|▋         | 3389/50000 [1:12:34<16:05:55,  1.24s/it]  7%|▋         | 3390/50000 [1:12:35<16:07:53,  1.25s/it]                                                         {'loss': 139.7594, 'learning_rate': 1.9984681278002834e-05, 'epoch': 0.53}
  7%|▋         | 3390/50000 [1:12:35<16:07:53,  1.25s/it]  7%|▋         | 3391/50000 [1:12:37<16:07:25,  1.25s/it]  7%|▋         | 3392/50000 [1:12:38<16:05:53,  1.24s/it]  7%|▋         | 3393/50000 [1:12:39<16:04:54,  1.24s/it]  7%|▋         | 3394/50000 [1:12:40<16:04:46,  1.24s/it]  7%|▋         | 3395/50000 [1:12:42<16:04:54,  1.24s/it]  7%|▋         | 3396/50000 [1:12:43<16:03:58,  1.24s/it]  7%|▋         | 3397/50000 [1:12:44<16:05:12,  1.24s/it]  7%|▋         | 3398/50000 [1:12:45<16:04:50,  1.24s/it]  7%|▋         | 3399/50000 [1:12:47<16:03:28,  1.24s/it]  7%|▋         | 3400/50000 [1:12:48<16:02:04,  1.24s/it]                                                         {'loss': 139.8297, 'learning_rate': 1.9984313148935453e-05, 'epoch': 0.53}
  7%|▋         | 3400/50000 [1:12:48<16:02:04,  1.24s/it]  7%|▋         | 3401/50000 [1:12:49<16:00:37,  1.24s/it]  7%|▋         | 3402/50000 [1:12:50<15:59:39,  1.24s/it]  7%|▋         | 3403/50000 [1:12:51<15:59:06,  1.23s/it]  7%|▋         | 3404/50000 [1:12:53<15:58:29,  1.23s/it]  7%|▋         | 3405/50000 [1:12:54<15:58:44,  1.23s/it]  7%|▋         | 3406/50000 [1:12:55<15:58:58,  1.23s/it]  7%|▋         | 3407/50000 [1:12:56<15:58:40,  1.23s/it]  7%|▋         | 3408/50000 [1:12:58<15:58:27,  1.23s/it]  7%|▋         | 3409/50000 [1:12:59<15:57:55,  1.23s/it]  7%|▋         | 3410/50000 [1:13:00<15:57:36,  1.23s/it]                                                         {'loss': 138.3359, 'learning_rate': 1.998394065239029e-05, 'epoch': 0.53}
  7%|▋         | 3410/50000 [1:13:00<15:57:36,  1.23s/it]  7%|▋         | 3411/50000 [1:13:01<15:57:50,  1.23s/it]  7%|▋         | 3412/50000 [1:13:03<15:57:55,  1.23s/it]  7%|▋         | 3413/50000 [1:13:04<15:58:16,  1.23s/it]  7%|▋         | 3414/50000 [1:13:05<15:57:55,  1.23s/it]  7%|▋         | 3415/50000 [1:13:06<15:58:20,  1.23s/it]  7%|▋         | 3416/50000 [1:13:08<15:58:13,  1.23s/it]  7%|▋         | 3417/50000 [1:13:09<15:58:04,  1.23s/it]  7%|▋         | 3418/50000 [1:13:10<15:57:39,  1.23s/it]  7%|▋         | 3419/50000 [1:13:11<15:57:59,  1.23s/it]  7%|▋         | 3420/50000 [1:13:12<15:57:41,  1.23s/it]                                                         {'loss': 192.3656, 'learning_rate': 1.9983563788530294e-05, 'epoch': 0.53}
  7%|▋         | 3420/50000 [1:13:12<15:57:41,  1.23s/it]  7%|▋         | 3421/50000 [1:13:14<15:57:44,  1.23s/it]  7%|▋         | 3422/50000 [1:13:15<15:57:24,  1.23s/it]  7%|▋         | 3423/50000 [1:13:16<15:57:18,  1.23s/it]  7%|▋         | 3424/50000 [1:13:17<15:57:13,  1.23s/it]  7%|▋         | 3425/50000 [1:13:19<15:57:22,  1.23s/it]  7%|▋         | 3426/50000 [1:13:20<16:17:53,  1.26s/it]  7%|▋         | 3427/50000 [1:13:21<16:12:37,  1.25s/it]  7%|▋         | 3428/50000 [1:13:22<16:08:42,  1.25s/it]  7%|▋         | 3429/50000 [1:13:24<16:17:20,  1.26s/it]  7%|▋         | 3430/50000 [1:13:25<16:11:31,  1.25s/it]                                                         {'loss': 142.5813, 'learning_rate': 1.998318255752031e-05, 'epoch': 0.53}
  7%|▋         | 3430/50000 [1:13:25<16:11:31,  1.25s/it]  7%|▋         | 3431/50000 [1:13:26<16:07:59,  1.25s/it]  7%|▋         | 3432/50000 [1:13:27<16:05:33,  1.24s/it]  7%|▋         | 3433/50000 [1:13:29<16:03:03,  1.24s/it]  7%|▋         | 3434/50000 [1:13:30<16:02:26,  1.24s/it]  7%|▋         | 3435/50000 [1:13:31<16:01:30,  1.24s/it]  7%|▋         | 3436/50000 [1:13:32<16:00:23,  1.24s/it]  7%|▋         | 3437/50000 [1:13:34<15:59:27,  1.24s/it]  7%|▋         | 3438/50000 [1:13:35<15:59:47,  1.24s/it]  7%|▋         | 3439/50000 [1:13:36<16:01:06,  1.24s/it]  7%|▋         | 3440/50000 [1:13:37<16:03:00,  1.24s/it]                                                         {'loss': 151.8094, 'learning_rate': 1.9982796959527105e-05, 'epoch': 0.53}
  7%|▋         | 3440/50000 [1:13:37<16:03:00,  1.24s/it]  7%|▋         | 3441/50000 [1:13:39<16:03:08,  1.24s/it]  7%|▋         | 3442/50000 [1:13:40<16:06:20,  1.25s/it]  7%|▋         | 3443/50000 [1:13:41<16:05:52,  1.24s/it]  7%|▋         | 3444/50000 [1:13:42<16:04:35,  1.24s/it]  7%|▋         | 3445/50000 [1:13:44<16:03:49,  1.24s/it]  7%|▋         | 3446/50000 [1:13:45<16:03:01,  1.24s/it]  7%|▋         | 3447/50000 [1:13:46<16:02:35,  1.24s/it]  7%|▋         | 3448/50000 [1:13:47<16:02:32,  1.24s/it]  7%|▋         | 3449/50000 [1:13:48<16:02:32,  1.24s/it]  7%|▋         | 3450/50000 [1:13:50<16:02:17,  1.24s/it]                                                         {'loss': 150.0844, 'learning_rate': 1.9982406994719355e-05, 'epoch': 0.53}
  7%|▋         | 3450/50000 [1:13:50<16:02:17,  1.24s/it]  7%|▋         | 3451/50000 [1:13:51<16:02:40,  1.24s/it]  7%|▋         | 3452/50000 [1:13:52<16:02:24,  1.24s/it]  7%|▋         | 3453/50000 [1:13:53<16:02:38,  1.24s/it]  7%|▋         | 3454/50000 [1:13:55<16:02:14,  1.24s/it]  7%|▋         | 3455/50000 [1:13:56<16:02:11,  1.24s/it]  7%|▋         | 3456/50000 [1:13:57<16:01:57,  1.24s/it]  7%|▋         | 3457/50000 [1:13:58<16:02:03,  1.24s/it]  7%|▋         | 3458/50000 [1:14:00<16:01:44,  1.24s/it]  7%|▋         | 3459/50000 [1:14:01<16:01:38,  1.24s/it]  7%|▋         | 3460/50000 [1:14:02<16:01:32,  1.24s/it]                                                         {'loss': 156.0531, 'learning_rate': 1.9982012663267644e-05, 'epoch': 0.54}
  7%|▋         | 3460/50000 [1:14:02<16:01:32,  1.24s/it]  7%|▋         | 3461/50000 [1:14:03<16:01:34,  1.24s/it]  7%|▋         | 3462/50000 [1:14:05<16:01:19,  1.24s/it]  7%|▋         | 3463/50000 [1:14:06<16:01:11,  1.24s/it]  7%|▋         | 3464/50000 [1:14:07<16:01:24,  1.24s/it]  7%|▋         | 3465/50000 [1:14:08<16:01:19,  1.24s/it]  7%|▋         | 3466/50000 [1:14:10<16:01:16,  1.24s/it]  7%|▋         | 3467/50000 [1:14:11<16:01:55,  1.24s/it]  7%|▋         | 3468/50000 [1:14:12<16:01:20,  1.24s/it]  7%|▋         | 3469/50000 [1:14:13<16:01:18,  1.24s/it]  7%|▋         | 3470/50000 [1:14:15<16:01:19,  1.24s/it]                                                         {'loss': 188.3094, 'learning_rate': 1.998161396534446e-05, 'epoch': 0.54}
  7%|▋         | 3470/50000 [1:14:15<16:01:19,  1.24s/it]  7%|▋         | 3471/50000 [1:14:16<16:01:26,  1.24s/it]  7%|▋         | 3472/50000 [1:14:17<16:01:18,  1.24s/it]  7%|▋         | 3473/50000 [1:14:18<16:01:11,  1.24s/it]  7%|▋         | 3474/50000 [1:14:19<16:01:23,  1.24s/it]  7%|▋         | 3475/50000 [1:14:21<16:01:38,  1.24s/it]  7%|▋         | 3476/50000 [1:14:22<16:01:03,  1.24s/it]  7%|▋         | 3477/50000 [1:14:23<16:00:56,  1.24s/it]  7%|▋         | 3478/50000 [1:14:24<16:00:59,  1.24s/it]  7%|▋         | 3479/50000 [1:14:26<16:00:58,  1.24s/it]  7%|▋         | 3480/50000 [1:14:27<16:00:49,  1.24s/it]                                                         {'loss': 166.025, 'learning_rate': 1.9981210901124212e-05, 'epoch': 0.54}
  7%|▋         | 3480/50000 [1:14:27<16:00:49,  1.24s/it]  7%|▋         | 3481/50000 [1:14:28<16:01:06,  1.24s/it]  7%|▋         | 3482/50000 [1:14:29<16:01:05,  1.24s/it]  7%|▋         | 3483/50000 [1:14:31<16:00:55,  1.24s/it]  7%|▋         | 3484/50000 [1:14:32<16:01:02,  1.24s/it]  7%|▋         | 3485/50000 [1:14:33<16:01:10,  1.24s/it]  7%|▋         | 3486/50000 [1:14:34<16:01:08,  1.24s/it]  7%|▋         | 3487/50000 [1:14:36<16:01:08,  1.24s/it]  7%|▋         | 3488/50000 [1:14:37<16:00:59,  1.24s/it]  7%|▋         | 3489/50000 [1:14:38<16:00:50,  1.24s/it]  7%|▋         | 3490/50000 [1:14:39<16:01:04,  1.24s/it]                                                         {'loss': 224.8516, 'learning_rate': 1.998080347078321e-05, 'epoch': 0.54}
  7%|▋         | 3490/50000 [1:14:39<16:01:04,  1.24s/it]  7%|▋         | 3491/50000 [1:14:41<16:01:10,  1.24s/it]  7%|▋         | 3492/50000 [1:14:42<16:00:48,  1.24s/it]  7%|▋         | 3493/50000 [1:14:43<16:00:55,  1.24s/it]  7%|▋         | 3494/50000 [1:14:44<16:01:12,  1.24s/it]  7%|▋         | 3495/50000 [1:14:46<16:00:47,  1.24s/it]  7%|▋         | 3496/50000 [1:14:47<16:00:59,  1.24s/it]  7%|▋         | 3497/50000 [1:14:48<16:01:03,  1.24s/it]  7%|▋         | 3498/50000 [1:14:49<16:01:01,  1.24s/it]  7%|▋         | 3499/50000 [1:14:50<16:00:42,  1.24s/it][2023-07-03 13:15:01,991] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, but hysteresis is 2. Reducing hysteresis to 1
  7%|▋         | 3500/50000 [1:14:51<15:02:58,  1.17s/it]                                                         {'loss': 161.1656, 'learning_rate': 1.9980433050590337e-05, 'epoch': 0.54}
  7%|▋         | 3500/50000 [1:14:51<15:02:58,  1.17s/it]  7%|▋         | 3501/50000 [1:14:53<15:20:16,  1.19s/it]  7%|▋         | 3502/50000 [1:14:54<15:32:08,  1.20s/it]  7%|▋         | 3503/50000 [1:14:55<15:40:35,  1.21s/it]  7%|▋         | 3504/50000 [1:14:56<15:46:31,  1.22s/it]  7%|▋         | 3505/50000 [1:14:58<15:51:08,  1.23s/it]  7%|▋         | 3506/50000 [1:14:59<15:54:04,  1.23s/it]  7%|▋         | 3507/50000 [1:15:00<15:55:54,  1.23s/it]  7%|▋         | 3508/50000 [1:15:01<15:57:11,  1.24s/it]  7%|▋         | 3509/50000 [1:15:03<15:57:54,  1.24s/it]  7%|▋         | 3510/50000 [1:15:04<15:58:39,  1.24s/it]                                                         {'loss': 178.4453, 'learning_rate': 1.9980017325112494e-05, 'epoch': 0.54}
  7%|▋         | 3510/50000 [1:15:04<15:58:39,  1.24s/it]  7%|▋         | 3511/50000 [1:15:05<15:59:23,  1.24s/it]  7%|▋         | 3512/50000 [1:15:06<15:59:41,  1.24s/it]  7%|▋         | 3513/50000 [1:15:08<15:59:50,  1.24s/it]  7%|▋         | 3514/50000 [1:15:09<15:59:49,  1.24s/it]  7%|▋         | 3515/50000 [1:15:10<16:00:15,  1.24s/it]  7%|▋         | 3516/50000 [1:15:11<16:00:20,  1.24s/it]  7%|▋         | 3517/50000 [1:15:13<16:00:17,  1.24s/it]  7%|▋         | 3518/50000 [1:15:14<16:00:06,  1.24s/it]  7%|▋         | 3519/50000 [1:15:15<15:59:59,  1.24s/it]  7%|▋         | 3520/50000 [1:15:16<15:59:54,  1.24s/it]                                                         {'loss': 149.2453, 'learning_rate': 1.9979597234036016e-05, 'epoch': 0.55}
  7%|▋         | 3520/50000 [1:15:16<15:59:54,  1.24s/it]  7%|▋         | 3521/50000 [1:15:18<16:00:09,  1.24s/it]  7%|▋         | 3522/50000 [1:15:19<16:00:00,  1.24s/it]  7%|▋         | 3523/50000 [1:15:20<16:00:06,  1.24s/it]  7%|▋         | 3524/50000 [1:15:21<16:00:04,  1.24s/it]  7%|▋         | 3525/50000 [1:15:22<16:00:13,  1.24s/it][2023-07-03 13:15:33,970] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
  7%|▋         | 3526/50000 [1:15:23<15:02:22,  1.17s/it]  7%|▋         | 3527/50000 [1:15:25<15:23:13,  1.19s/it]  7%|▋         | 3528/50000 [1:15:26<15:34:10,  1.21s/it]  7%|▋         | 3529/50000 [1:15:27<15:41:41,  1.22s/it]  7%|▋         | 3530/50000 [1:15:28<15:47:06,  1.22s/it]                                                         {'loss': 180.6422, 'learning_rate': 1.997921541963218e-05, 'epoch': 0.55}
  7%|▋         | 3530/50000 [1:15:28<15:47:06,  1.22s/it]  7%|▋         | 3531/50000 [1:15:30<15:51:05,  1.23s/it]  7%|▋         | 3532/50000 [1:15:31<15:53:51,  1.23s/it]  7%|▋         | 3533/50000 [1:15:32<15:55:43,  1.23s/it]  7%|▋         | 3534/50000 [1:15:33<15:56:58,  1.24s/it]  7%|▋         | 3535/50000 [1:15:35<15:57:53,  1.24s/it]  7%|▋         | 3536/50000 [1:15:36<15:58:09,  1.24s/it]  7%|▋         | 3537/50000 [1:15:37<15:58:35,  1.24s/it]  7%|▋         | 3538/50000 [1:15:38<15:59:10,  1.24s/it]  7%|▋         | 3539/50000 [1:15:40<15:59:31,  1.24s/it]  7%|▋         | 3540/50000 [1:15:41<15:59:26,  1.24s/it]                                                         {'loss': 143.5969, 'learning_rate': 1.9978787034426126e-05, 'epoch': 0.55}
  7%|▋         | 3540/50000 [1:15:41<15:59:26,  1.24s/it]  7%|▋         | 3541/50000 [1:15:42<15:59:51,  1.24s/it][2023-07-03 13:15:53,572] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  7%|▋         | 3542/50000 [1:15:43<15:02:29,  1.17s/it]  7%|▋         | 3543/50000 [1:15:44<15:20:00,  1.19s/it]  7%|▋         | 3544/50000 [1:15:46<15:31:51,  1.20s/it]  7%|▋         | 3545/50000 [1:15:47<15:40:11,  1.21s/it]  7%|▋         | 3546/50000 [1:15:48<15:46:08,  1.22s/it]  7%|▋         | 3547/50000 [1:15:49<15:50:09,  1.23s/it]  7%|▋         | 3548/50000 [1:15:50<15:52:48,  1.23s/it]  7%|▋         | 3549/50000 [1:15:52<15:54:45,  1.23s/it]  7%|▋         | 3550/50000 [1:15:53<15:56:05,  1.23s/it]                                                         {'loss': 185.8656, 'learning_rate': 1.9978397755608595e-05, 'epoch': 0.55}
  7%|▋         | 3550/50000 [1:15:53<15:56:05,  1.23s/it]  7%|▋         | 3551/50000 [1:15:54<15:57:01,  1.24s/it]  7%|▋         | 3552/50000 [1:15:55<15:57:48,  1.24s/it]  7%|▋         | 3553/50000 [1:15:57<15:58:16,  1.24s/it]  7%|▋         | 3554/50000 [1:15:58<15:58:37,  1.24s/it]  7%|▋         | 3555/50000 [1:15:59<15:58:40,  1.24s/it]  7%|▋         | 3556/50000 [1:16:00<15:58:57,  1.24s/it]  7%|▋         | 3557/50000 [1:16:02<15:59:07,  1.24s/it]  7%|▋         | 3558/50000 [1:16:03<15:59:07,  1.24s/it]  7%|▋         | 3559/50000 [1:16:04<15:59:24,  1.24s/it]  7%|▋         | 3560/50000 [1:16:05<15:59:24,  1.24s/it]                                                         {'loss': 186.6125, 'learning_rate': 1.9977961076949448e-05, 'epoch': 0.55}
  7%|▋         | 3560/50000 [1:16:05<15:59:24,  1.24s/it]  7%|▋         | 3561/50000 [1:16:07<15:59:39,  1.24s/it]  7%|▋         | 3562/50000 [1:16:08<15:59:35,  1.24s/it]  7%|▋         | 3563/50000 [1:16:09<15:59:51,  1.24s/it]  7%|▋         | 3564/50000 [1:16:10<15:59:59,  1.24s/it]  7%|▋         | 3565/50000 [1:16:12<15:59:46,  1.24s/it]  7%|▋         | 3566/50000 [1:16:13<15:59:29,  1.24s/it]  7%|▋         | 3567/50000 [1:16:14<15:59:25,  1.24s/it]  7%|▋         | 3568/50000 [1:16:15<15:59:18,  1.24s/it]  7%|▋         | 3569/50000 [1:16:17<15:59:14,  1.24s/it]  7%|▋         | 3570/50000 [1:16:18<15:59:12,  1.24s/it]                                                         {'loss': 158.5461, 'learning_rate': 1.997752003359113e-05, 'epoch': 0.55}
  7%|▋         | 3570/50000 [1:16:18<15:59:12,  1.24s/it]  7%|▋         | 3571/50000 [1:16:19<15:59:14,  1.24s/it]  7%|▋         | 3572/50000 [1:16:20<15:59:57,  1.24s/it]  7%|▋         | 3573/50000 [1:16:21<16:00:18,  1.24s/it]  7%|▋         | 3574/50000 [1:16:23<16:00:38,  1.24s/it]  7%|▋         | 3575/50000 [1:16:24<16:18:01,  1.26s/it]  7%|▋         | 3576/50000 [1:16:25<16:31:11,  1.28s/it]  7%|▋         | 3577/50000 [1:16:27<16:22:03,  1.27s/it]  7%|▋         | 3578/50000 [1:16:28<16:15:36,  1.26s/it]  7%|▋         | 3579/50000 [1:16:29<16:10:49,  1.25s/it]  7%|▋         | 3580/50000 [1:16:30<16:08:38,  1.25s/it]                                                         {'loss': 163.1, 'learning_rate': 1.997707462572657e-05, 'epoch': 0.55}
  7%|▋         | 3580/50000 [1:16:30<16:08:38,  1.25s/it]  7%|▋         | 3581/50000 [1:16:32<16:24:23,  1.27s/it]  7%|▋         | 3582/50000 [1:16:33<16:28:32,  1.28s/it]  7%|▋         | 3583/50000 [1:16:34<16:19:54,  1.27s/it]  7%|▋         | 3584/50000 [1:16:35<16:14:16,  1.26s/it]  7%|▋         | 3585/50000 [1:16:37<16:11:06,  1.26s/it]  7%|▋         | 3586/50000 [1:16:38<16:08:17,  1.25s/it]  7%|▋         | 3587/50000 [1:16:39<16:05:45,  1.25s/it]  7%|▋         | 3588/50000 [1:16:40<16:04:07,  1.25s/it]  7%|▋         | 3589/50000 [1:16:42<16:02:51,  1.24s/it]  7%|▋         | 3590/50000 [1:16:43<16:01:39,  1.24s/it]                                                         {'loss': 149.3719, 'learning_rate': 1.9976624853550612e-05, 'epoch': 0.56}
  7%|▋         | 3590/50000 [1:16:43<16:01:39,  1.24s/it]  7%|▋         | 3591/50000 [1:16:44<16:00:55,  1.24s/it]  7%|▋         | 3592/50000 [1:16:45<16:00:12,  1.24s/it]  7%|▋         | 3593/50000 [1:16:47<15:59:36,  1.24s/it]  7%|▋         | 3594/50000 [1:16:48<15:59:02,  1.24s/it]  7%|▋         | 3595/50000 [1:16:49<15:58:42,  1.24s/it]  7%|▋         | 3596/50000 [1:16:50<15:58:50,  1.24s/it]  7%|▋         | 3597/50000 [1:16:52<15:58:34,  1.24s/it]  7%|▋         | 3598/50000 [1:16:53<15:58:44,  1.24s/it]  7%|▋         | 3599/50000 [1:16:54<16:00:59,  1.24s/it]  7%|▋         | 3600/50000 [1:16:55<16:00:20,  1.24s/it]                                                         {'loss': 154.5875, 'learning_rate': 1.997617071725999e-05, 'epoch': 0.56}
  7%|▋         | 3600/50000 [1:16:55<16:00:20,  1.24s/it]  7%|▋         | 3601/50000 [1:16:57<16:00:03,  1.24s/it]  7%|▋         | 3602/50000 [1:16:58<15:59:42,  1.24s/it]  7%|▋         | 3603/50000 [1:16:59<15:59:03,  1.24s/it]  7%|▋         | 3604/50000 [1:17:00<15:58:50,  1.24s/it]  7%|▋         | 3605/50000 [1:17:01<15:58:41,  1.24s/it]  7%|▋         | 3606/50000 [1:17:03<15:58:24,  1.24s/it]  7%|▋         | 3607/50000 [1:17:04<15:58:14,  1.24s/it]  7%|▋         | 3608/50000 [1:17:05<15:58:53,  1.24s/it]  7%|▋         | 3609/50000 [1:17:06<15:58:28,  1.24s/it]  7%|▋         | 3610/50000 [1:17:08<15:58:09,  1.24s/it]                                                         {'loss': 166.4094, 'learning_rate': 1.9975712217053367e-05, 'epoch': 0.56}
  7%|▋         | 3610/50000 [1:17:08<15:58:09,  1.24s/it]  7%|▋         | 3611/50000 [1:17:09<15:58:20,  1.24s/it]  7%|▋         | 3612/50000 [1:17:10<15:58:19,  1.24s/it]  7%|▋         | 3613/50000 [1:17:11<15:58:15,  1.24s/it]  7%|▋         | 3614/50000 [1:17:13<15:58:13,  1.24s/it]  7%|▋         | 3615/50000 [1:17:14<15:58:02,  1.24s/it]  7%|▋         | 3616/50000 [1:17:15<15:57:49,  1.24s/it]  7%|▋         | 3617/50000 [1:17:16<15:57:45,  1.24s/it]  7%|▋         | 3618/50000 [1:17:18<15:57:51,  1.24s/it]  7%|▋         | 3619/50000 [1:17:19<15:58:22,  1.24s/it]  7%|▋         | 3620/50000 [1:17:20<15:57:55,  1.24s/it]                                                         {'loss': 143.6937, 'learning_rate': 1.9975249353131304e-05, 'epoch': 0.56}
  7%|▋         | 3620/50000 [1:17:20<15:57:55,  1.24s/it]  7%|▋         | 3621/50000 [1:17:21<15:58:07,  1.24s/it]  7%|▋         | 3622/50000 [1:17:23<15:58:02,  1.24s/it]  7%|▋         | 3623/50000 [1:17:24<15:57:47,  1.24s/it]  7%|▋         | 3624/50000 [1:17:25<15:57:34,  1.24s/it]  7%|▋         | 3625/50000 [1:17:26<15:57:24,  1.24s/it]  7%|▋         | 3626/50000 [1:17:28<15:57:22,  1.24s/it]  7%|▋         | 3627/50000 [1:17:29<15:57:30,  1.24s/it]  7%|▋         | 3628/50000 [1:17:30<15:57:16,  1.24s/it]  7%|▋         | 3629/50000 [1:17:31<15:57:11,  1.24s/it]  7%|▋         | 3630/50000 [1:17:32<15:57:18,  1.24s/it]                                                         {'loss': 162.993, 'learning_rate': 1.9974782125696275e-05, 'epoch': 0.56}
  7%|▋         | 3630/50000 [1:17:32<15:57:18,  1.24s/it]  7%|▋         | 3631/50000 [1:17:34<15:57:34,  1.24s/it]  7%|▋         | 3632/50000 [1:17:35<15:57:34,  1.24s/it]  7%|▋         | 3633/50000 [1:17:36<15:57:26,  1.24s/it]  7%|▋         | 3634/50000 [1:17:37<15:57:31,  1.24s/it]  7%|▋         | 3635/50000 [1:17:39<15:57:15,  1.24s/it]  7%|▋         | 3636/50000 [1:17:40<15:57:06,  1.24s/it]  7%|▋         | 3637/50000 [1:17:41<15:57:12,  1.24s/it]  7%|▋         | 3638/50000 [1:17:42<15:57:10,  1.24s/it]  7%|▋         | 3639/50000 [1:17:44<15:57:23,  1.24s/it]  7%|▋         | 3640/50000 [1:17:45<15:57:31,  1.24s/it]                                                         {'loss': 164.3797, 'learning_rate': 1.9974310534952657e-05, 'epoch': 0.56}
  7%|▋         | 3640/50000 [1:17:45<15:57:31,  1.24s/it]  7%|▋         | 3641/50000 [1:17:46<15:57:26,  1.24s/it]  7%|▋         | 3642/50000 [1:17:47<15:57:15,  1.24s/it]  7%|▋         | 3643/50000 [1:17:49<15:57:16,  1.24s/it]  7%|▋         | 3644/50000 [1:17:50<15:57:07,  1.24s/it]  7%|▋         | 3645/50000 [1:17:51<15:56:54,  1.24s/it]  7%|▋         | 3646/50000 [1:17:52<15:56:49,  1.24s/it]  7%|▋         | 3647/50000 [1:17:54<15:56:44,  1.24s/it]  7%|▋         | 3648/50000 [1:17:55<15:56:41,  1.24s/it]  7%|▋         | 3649/50000 [1:17:56<15:57:31,  1.24s/it]  7%|▋         | 3650/50000 [1:17:57<15:57:14,  1.24s/it]                                                         {'loss': 174.4734, 'learning_rate': 1.997383458110674e-05, 'epoch': 0.57}
  7%|▋         | 3650/50000 [1:17:57<15:57:14,  1.24s/it]  7%|▋         | 3651/50000 [1:17:58<15:57:13,  1.24s/it]  7%|▋         | 3652/50000 [1:18:00<15:57:11,  1.24s/it]  7%|▋         | 3653/50000 [1:18:01<15:57:05,  1.24s/it]  7%|▋         | 3654/50000 [1:18:02<15:56:50,  1.24s/it]  7%|▋         | 3655/50000 [1:18:03<15:56:51,  1.24s/it]  7%|▋         | 3656/50000 [1:18:05<15:56:46,  1.24s/it]  7%|▋         | 3657/50000 [1:18:06<15:56:48,  1.24s/it]  7%|▋         | 3658/50000 [1:18:07<15:57:01,  1.24s/it]  7%|▋         | 3659/50000 [1:18:08<15:57:03,  1.24s/it]  7%|▋         | 3660/50000 [1:18:10<15:56:50,  1.24s/it]                                                         {'loss': 140.5891, 'learning_rate': 1.997335426436673e-05, 'epoch': 0.57}
  7%|▋         | 3660/50000 [1:18:10<15:56:50,  1.24s/it]  7%|▋         | 3661/50000 [1:18:11<15:57:09,  1.24s/it]  7%|▋         | 3662/50000 [1:18:12<15:56:54,  1.24s/it]  7%|▋         | 3663/50000 [1:18:13<15:56:54,  1.24s/it]  7%|▋         | 3664/50000 [1:18:15<15:56:40,  1.24s/it]  7%|▋         | 3665/50000 [1:18:16<15:56:49,  1.24s/it]  7%|▋         | 3666/50000 [1:18:17<15:57:00,  1.24s/it]  7%|▋         | 3667/50000 [1:18:18<15:56:57,  1.24s/it]  7%|▋         | 3668/50000 [1:18:20<15:56:44,  1.24s/it]  7%|▋         | 3669/50000 [1:18:21<15:56:41,  1.24s/it]  7%|▋         | 3670/50000 [1:18:22<15:56:45,  1.24s/it]                                                         {'loss': 136.7531, 'learning_rate': 1.997286958494273e-05, 'epoch': 0.57}
  7%|▋         | 3670/50000 [1:18:22<15:56:45,  1.24s/it]  7%|▋         | 3671/50000 [1:18:23<15:56:51,  1.24s/it]  7%|▋         | 3672/50000 [1:18:25<15:56:53,  1.24s/it]  7%|▋         | 3673/50000 [1:18:26<15:56:56,  1.24s/it]  7%|▋         | 3674/50000 [1:18:27<15:56:47,  1.24s/it]  7%|▋         | 3675/50000 [1:18:28<15:56:52,  1.24s/it]  7%|▋         | 3676/50000 [1:18:29<15:56:42,  1.24s/it]  7%|▋         | 3677/50000 [1:18:31<15:56:31,  1.24s/it]  7%|▋         | 3678/50000 [1:18:32<15:56:45,  1.24s/it]  7%|▋         | 3679/50000 [1:18:33<15:56:55,  1.24s/it]  7%|▋         | 3680/50000 [1:18:34<15:57:02,  1.24s/it]                                                         {'loss': 167.8297, 'learning_rate': 1.997238054304675e-05, 'epoch': 0.57}
  7%|▋         | 3680/50000 [1:18:34<15:57:02,  1.24s/it]  7%|▋         | 3681/50000 [1:18:36<15:57:33,  1.24s/it]  7%|▋         | 3682/50000 [1:18:37<15:57:16,  1.24s/it]  7%|▋         | 3683/50000 [1:18:38<15:57:37,  1.24s/it]  7%|▋         | 3684/50000 [1:18:39<15:57:39,  1.24s/it]  7%|▋         | 3685/50000 [1:18:41<15:57:47,  1.24s/it]  7%|▋         | 3686/50000 [1:18:42<15:57:55,  1.24s/it]  7%|▋         | 3687/50000 [1:18:43<15:57:59,  1.24s/it]  7%|▋         | 3688/50000 [1:18:44<15:57:55,  1.24s/it]  7%|▋         | 3689/50000 [1:18:46<15:57:42,  1.24s/it]  7%|▋         | 3690/50000 [1:18:47<15:57:17,  1.24s/it]                                                         {'loss': 148.0328, 'learning_rate': 1.9971887138892718e-05, 'epoch': 0.57}
  7%|▋         | 3690/50000 [1:18:47<15:57:17,  1.24s/it]  7%|▋         | 3691/50000 [1:18:48<15:57:14,  1.24s/it]  7%|▋         | 3692/50000 [1:18:49<15:56:58,  1.24s/it]  7%|▋         | 3693/50000 [1:18:51<15:56:50,  1.24s/it]  7%|▋         | 3694/50000 [1:18:52<15:57:17,  1.24s/it]  7%|▋         | 3695/50000 [1:18:53<15:57:03,  1.24s/it]  7%|▋         | 3696/50000 [1:18:54<15:57:12,  1.24s/it]  7%|▋         | 3697/50000 [1:18:56<15:57:08,  1.24s/it]  7%|▋         | 3698/50000 [1:18:57<15:57:05,  1.24s/it]  7%|▋         | 3699/50000 [1:18:58<15:56:29,  1.24s/it]  7%|▋         | 3700/50000 [1:18:59<15:56:32,  1.24s/it]                                                         {'loss': 167.5891, 'learning_rate': 1.9971389372696465e-05, 'epoch': 0.57}
  7%|▋         | 3700/50000 [1:18:59<15:56:32,  1.24s/it]  7%|▋         | 3701/50000 [1:19:00<15:57:20,  1.24s/it]  7%|▋         | 3702/50000 [1:19:02<15:57:23,  1.24s/it]  7%|▋         | 3703/50000 [1:19:03<15:57:36,  1.24s/it]  7%|▋         | 3704/50000 [1:19:04<15:57:47,  1.24s/it]  7%|▋         | 3705/50000 [1:19:05<15:57:42,  1.24s/it]  7%|▋         | 3706/50000 [1:19:07<15:57:45,  1.24s/it]  7%|▋         | 3707/50000 [1:19:08<15:57:04,  1.24s/it]  7%|▋         | 3708/50000 [1:19:09<15:57:11,  1.24s/it]  7%|▋         | 3709/50000 [1:19:10<15:57:20,  1.24s/it]  7%|▋         | 3710/50000 [1:19:12<15:57:40,  1.24s/it]                                                         {'loss': 158.0219, 'learning_rate': 1.9970887244675733e-05, 'epoch': 0.58}
  7%|▋         | 3710/50000 [1:19:12<15:57:40,  1.24s/it]  7%|▋         | 3711/50000 [1:19:13<15:57:10,  1.24s/it]  7%|▋         | 3712/50000 [1:19:14<15:56:52,  1.24s/it]  7%|▋         | 3713/50000 [1:19:15<15:56:45,  1.24s/it]  7%|▋         | 3714/50000 [1:19:17<15:56:23,  1.24s/it]  7%|▋         | 3715/50000 [1:19:18<15:56:28,  1.24s/it]  7%|▋         | 3716/50000 [1:19:19<15:56:11,  1.24s/it]  7%|▋         | 3717/50000 [1:19:20<15:55:51,  1.24s/it]  7%|▋         | 3718/50000 [1:19:22<15:55:40,  1.24s/it]  7%|▋         | 3719/50000 [1:19:23<15:55:39,  1.24s/it]  7%|▋         | 3720/50000 [1:19:24<15:55:51,  1.24s/it]                                                         {'loss': 164.4703, 'learning_rate': 1.9970380755050166e-05, 'epoch': 0.58}
  7%|▋         | 3720/50000 [1:19:24<15:55:51,  1.24s/it]  7%|▋         | 3721/50000 [1:19:25<15:55:49,  1.24s/it]  7%|▋         | 3722/50000 [1:19:27<15:55:43,  1.24s/it]  7%|▋         | 3723/50000 [1:19:28<15:55:33,  1.24s/it]  7%|▋         | 3724/50000 [1:19:29<15:55:36,  1.24s/it]  7%|▋         | 3725/50000 [1:19:30<15:55:31,  1.24s/it]  7%|▋         | 3726/50000 [1:19:31<15:55:23,  1.24s/it]  7%|▋         | 3727/50000 [1:19:33<15:55:06,  1.24s/it]  7%|▋         | 3728/50000 [1:19:34<15:55:14,  1.24s/it]  7%|▋         | 3729/50000 [1:19:35<15:55:13,  1.24s/it]  7%|▋         | 3730/50000 [1:19:36<15:55:06,  1.24s/it]                                                         {'loss': 217.2344, 'learning_rate': 1.9969869904041323e-05, 'epoch': 0.58}
  7%|▋         | 3730/50000 [1:19:36<15:55:06,  1.24s/it]  7%|▋         | 3731/50000 [1:19:38<15:55:18,  1.24s/it]  7%|▋         | 3732/50000 [1:19:39<15:55:31,  1.24s/it]  7%|▋         | 3733/50000 [1:19:40<15:55:27,  1.24s/it]  7%|▋         | 3734/50000 [1:19:41<15:55:28,  1.24s/it]  7%|▋         | 3735/50000 [1:19:43<15:55:22,  1.24s/it]  7%|▋         | 3736/50000 [1:19:44<15:55:21,  1.24s/it]  7%|▋         | 3737/50000 [1:19:45<15:55:13,  1.24s/it]  7%|▋         | 3738/50000 [1:19:46<15:55:31,  1.24s/it]  7%|▋         | 3739/50000 [1:19:48<15:55:03,  1.24s/it]  7%|▋         | 3740/50000 [1:19:49<15:55:45,  1.24s/it]                                                         {'loss': 143.0484, 'learning_rate': 1.9969354691872666e-05, 'epoch': 0.58}
  7%|▋         | 3740/50000 [1:19:49<15:55:45,  1.24s/it]  7%|▋         | 3741/50000 [1:19:50<15:55:50,  1.24s/it]  7%|▋         | 3742/50000 [1:19:51<15:55:27,  1.24s/it]  7%|▋         | 3743/50000 [1:19:53<15:55:07,  1.24s/it]  7%|▋         | 3744/50000 [1:19:54<15:55:51,  1.24s/it]  7%|▋         | 3745/50000 [1:19:55<15:55:10,  1.24s/it]  7%|▋         | 3746/50000 [1:19:56<15:55:04,  1.24s/it]  7%|▋         | 3747/50000 [1:19:57<15:54:58,  1.24s/it]  7%|▋         | 3748/50000 [1:19:59<15:54:50,  1.24s/it]  7%|▋         | 3749/50000 [1:20:00<15:54:46,  1.24s/it]  8%|▊         | 3750/50000 [1:20:01<15:54:42,  1.24s/it]                                                         {'loss': 192.4453, 'learning_rate': 1.9968835118769567e-05, 'epoch': 0.58}
  8%|▊         | 3750/50000 [1:20:01<15:54:42,  1.24s/it]  8%|▊         | 3751/50000 [1:20:02<15:54:54,  1.24s/it]  8%|▊         | 3752/50000 [1:20:04<15:54:40,  1.24s/it]  8%|▊         | 3753/50000 [1:20:05<15:54:40,  1.24s/it]  8%|▊         | 3754/50000 [1:20:06<15:55:15,  1.24s/it]  8%|▊         | 3755/50000 [1:20:07<15:54:56,  1.24s/it]  8%|▊         | 3756/50000 [1:20:09<15:54:43,  1.24s/it][2023-07-03 13:20:20,139] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, but hysteresis is 2. Reducing hysteresis to 1
  8%|▊         | 3757/50000 [1:20:10<14:57:20,  1.16s/it]  8%|▊         | 3758/50000 [1:20:11<15:14:55,  1.19s/it]  8%|▊         | 3759/50000 [1:20:12<15:26:59,  1.20s/it]  8%|▊         | 3760/50000 [1:20:13<15:35:23,  1.21s/it]                                                         {'loss': 164.2953, 'learning_rate': 1.9968363774565635e-05, 'epoch': 0.58}
  8%|▊         | 3760/50000 [1:20:13<15:35:23,  1.21s/it]  8%|▊         | 3761/50000 [1:20:15<15:41:21,  1.22s/it]  8%|▊         | 3762/50000 [1:20:16<15:45:18,  1.23s/it]  8%|▊         | 3763/50000 [1:20:17<15:47:55,  1.23s/it]  8%|▊         | 3764/50000 [1:20:18<15:50:09,  1.23s/it]  8%|▊         | 3765/50000 [1:20:20<15:51:23,  1.23s/it]  8%|▊         | 3766/50000 [1:20:21<15:52:17,  1.24s/it]  8%|▊         | 3767/50000 [1:20:22<15:52:57,  1.24s/it]  8%|▊         | 3768/50000 [1:20:23<15:53:22,  1.24s/it]  8%|▊         | 3769/50000 [1:20:24<15:53:32,  1.24s/it]  8%|▊         | 3770/50000 [1:20:26<15:53:39,  1.24s/it]                                                         {'loss': 141.0141, 'learning_rate': 1.9967835916314826e-05, 'epoch': 0.58}
  8%|▊         | 3770/50000 [1:20:26<15:53:39,  1.24s/it]  8%|▊         | 3771/50000 [1:20:27<15:53:51,  1.24s/it]  8%|▊         | 3772/50000 [1:20:28<15:54:01,  1.24s/it]  8%|▊         | 3773/50000 [1:20:29<15:54:16,  1.24s/it]  8%|▊         | 3774/50000 [1:20:31<15:54:07,  1.24s/it]  8%|▊         | 3775/50000 [1:20:32<15:54:13,  1.24s/it]  8%|▊         | 3776/50000 [1:20:33<15:54:04,  1.24s/it]  8%|▊         | 3777/50000 [1:20:34<15:54:20,  1.24s/it]  8%|▊         | 3778/50000 [1:20:36<15:54:18,  1.24s/it]  8%|▊         | 3779/50000 [1:20:37<15:54:55,  1.24s/it]  8%|▊         | 3780/50000 [1:20:38<15:54:37,  1.24s/it]                                                         {'loss': 155.4016, 'learning_rate': 1.9967303697793937e-05, 'epoch': 0.59}
  8%|▊         | 3780/50000 [1:20:38<15:54:37,  1.24s/it]  8%|▊         | 3781/50000 [1:20:39<15:54:50,  1.24s/it]  8%|▊         | 3782/50000 [1:20:41<15:54:40,  1.24s/it]  8%|▊         | 3783/50000 [1:20:42<15:54:17,  1.24s/it]  8%|▊         | 3784/50000 [1:20:43<15:54:21,  1.24s/it]  8%|▊         | 3785/50000 [1:20:44<15:54:27,  1.24s/it]  8%|▊         | 3786/50000 [1:20:46<15:54:22,  1.24s/it]  8%|▊         | 3787/50000 [1:20:47<15:54:22,  1.24s/it]  8%|▊         | 3788/50000 [1:20:48<15:54:27,  1.24s/it]  8%|▊         | 3789/50000 [1:20:49<15:54:18,  1.24s/it]  8%|▊         | 3790/50000 [1:20:51<15:54:01,  1.24s/it]                                                         {'loss': 141.3281, 'learning_rate': 1.9966767119235784e-05, 'epoch': 0.59}
  8%|▊         | 3790/50000 [1:20:51<15:54:01,  1.24s/it]  8%|▊         | 3791/50000 [1:20:52<15:54:22,  1.24s/it]  8%|▊         | 3792/50000 [1:20:53<15:54:18,  1.24s/it]  8%|▊         | 3793/50000 [1:20:54<15:54:15,  1.24s/it]  8%|▊         | 3794/50000 [1:20:55<15:54:06,  1.24s/it]  8%|▊         | 3795/50000 [1:20:57<15:53:59,  1.24s/it]  8%|▊         | 3796/50000 [1:20:58<15:53:56,  1.24s/it]  8%|▊         | 3797/50000 [1:20:59<15:53:51,  1.24s/it]  8%|▊         | 3798/50000 [1:21:00<15:54:02,  1.24s/it]  8%|▊         | 3799/50000 [1:21:02<15:53:48,  1.24s/it]  8%|▊         | 3800/50000 [1:21:03<15:53:28,  1.24s/it]                                                         {'loss': 134.4156, 'learning_rate': 1.9966226180875073e-05, 'epoch': 0.59}
  8%|▊         | 3800/50000 [1:21:03<15:53:28,  1.24s/it]  8%|▊         | 3801/50000 [1:21:04<15:53:32,  1.24s/it]  8%|▊         | 3802/50000 [1:21:05<15:53:45,  1.24s/it]  8%|▊         | 3803/50000 [1:21:07<15:54:03,  1.24s/it]  8%|▊         | 3804/50000 [1:21:08<15:53:57,  1.24s/it]  8%|▊         | 3805/50000 [1:21:09<15:53:43,  1.24s/it]  8%|▊         | 3806/50000 [1:21:10<15:53:49,  1.24s/it]  8%|▊         | 3807/50000 [1:21:12<15:53:42,  1.24s/it]  8%|▊         | 3808/50000 [1:21:13<15:53:34,  1.24s/it]  8%|▊         | 3809/50000 [1:21:14<15:53:25,  1.24s/it]  8%|▊         | 3810/50000 [1:21:15<15:53:20,  1.24s/it]                                                         {'loss': 155.1141, 'learning_rate': 1.9965680882948445e-05, 'epoch': 0.59}
  8%|▊         | 3810/50000 [1:21:15<15:53:20,  1.24s/it]  8%|▊         | 3811/50000 [1:21:17<15:53:24,  1.24s/it]  8%|▊         | 3812/50000 [1:21:18<15:53:22,  1.24s/it]  8%|▊         | 3813/50000 [1:21:19<15:53:18,  1.24s/it]  8%|▊         | 3814/50000 [1:21:20<15:53:29,  1.24s/it]  8%|▊         | 3815/50000 [1:21:21<15:53:13,  1.24s/it]  8%|▊         | 3816/50000 [1:21:23<15:53:07,  1.24s/it]  8%|▊         | 3817/50000 [1:21:24<15:53:16,  1.24s/it]  8%|▊         | 3818/50000 [1:21:25<15:53:22,  1.24s/it]  8%|▊         | 3819/50000 [1:21:26<15:53:23,  1.24s/it]  8%|▊         | 3820/50000 [1:21:28<15:53:15,  1.24s/it]                                                         {'loss': 165.1, 'learning_rate': 1.996513122569442e-05, 'epoch': 0.59}
  8%|▊         | 3820/50000 [1:21:28<15:53:15,  1.24s/it]  8%|▊         | 3821/50000 [1:21:29<15:53:36,  1.24s/it]  8%|▊         | 3822/50000 [1:21:30<15:53:28,  1.24s/it]  8%|▊         | 3823/50000 [1:21:31<15:53:34,  1.24s/it]  8%|▊         | 3824/50000 [1:21:33<15:53:24,  1.24s/it]  8%|▊         | 3825/50000 [1:21:34<15:53:27,  1.24s/it]  8%|▊         | 3826/50000 [1:21:35<15:53:24,  1.24s/it]  8%|▊         | 3827/50000 [1:21:36<15:53:23,  1.24s/it]  8%|▊         | 3828/50000 [1:21:38<15:53:06,  1.24s/it]  8%|▊         | 3829/50000 [1:21:39<15:53:00,  1.24s/it]  8%|▊         | 3830/50000 [1:21:40<15:53:44,  1.24s/it]                                                         {'loss': 190.0047, 'learning_rate': 1.9964577209353438e-05, 'epoch': 0.59}
  8%|▊         | 3830/50000 [1:21:40<15:53:44,  1.24s/it]  8%|▊         | 3831/50000 [1:21:41<15:53:49,  1.24s/it]  8%|▊         | 3832/50000 [1:21:43<15:53:27,  1.24s/it]  8%|▊         | 3833/50000 [1:21:44<15:53:19,  1.24s/it]  8%|▊         | 3834/50000 [1:21:45<15:53:10,  1.24s/it]  8%|▊         | 3835/50000 [1:21:46<15:53:19,  1.24s/it]  8%|▊         | 3836/50000 [1:21:47<15:53:06,  1.24s/it]  8%|▊         | 3837/50000 [1:21:49<15:52:51,  1.24s/it]  8%|▊         | 3838/50000 [1:21:50<15:52:39,  1.24s/it]  8%|▊         | 3839/50000 [1:21:51<15:52:51,  1.24s/it]  8%|▊         | 3840/50000 [1:21:52<15:52:46,  1.24s/it]                                                         {'loss': 138.675, 'learning_rate': 1.9964018834167848e-05, 'epoch': 0.6}
  8%|▊         | 3840/50000 [1:21:52<15:52:46,  1.24s/it]  8%|▊         | 3841/50000 [1:21:54<15:52:40,  1.24s/it]  8%|▊         | 3842/50000 [1:21:55<15:53:02,  1.24s/it]  8%|▊         | 3843/50000 [1:21:56<15:53:01,  1.24s/it]  8%|▊         | 3844/50000 [1:21:57<15:52:53,  1.24s/it]  8%|▊         | 3845/50000 [1:21:59<15:52:56,  1.24s/it]  8%|▊         | 3846/50000 [1:22:00<15:52:51,  1.24s/it]  8%|▊         | 3847/50000 [1:22:01<15:53:10,  1.24s/it]  8%|▊         | 3848/50000 [1:22:02<15:52:53,  1.24s/it]  8%|▊         | 3849/50000 [1:22:04<15:52:58,  1.24s/it]  8%|▊         | 3850/50000 [1:22:05<15:52:35,  1.24s/it]                                                         {'loss': 154.4375, 'learning_rate': 1.9963456100381904e-05, 'epoch': 0.6}
  8%|▊         | 3850/50000 [1:22:05<15:52:35,  1.24s/it]  8%|▊         | 3851/50000 [1:22:06<15:52:38,  1.24s/it]  8%|▊         | 3852/50000 [1:22:07<15:52:36,  1.24s/it]  8%|▊         | 3853/50000 [1:22:09<15:52:25,  1.24s/it]  8%|▊         | 3854/50000 [1:22:10<15:52:17,  1.24s/it]  8%|▊         | 3855/50000 [1:22:11<15:52:34,  1.24s/it]  8%|▊         | 3856/50000 [1:22:12<15:52:36,  1.24s/it]  8%|▊         | 3857/50000 [1:22:14<15:52:19,  1.24s/it]  8%|▊         | 3858/50000 [1:22:15<15:52:20,  1.24s/it]  8%|▊         | 3859/50000 [1:22:16<15:52:16,  1.24s/it]  8%|▊         | 3860/50000 [1:22:17<15:52:17,  1.24s/it]                                                         {'loss': 134.832, 'learning_rate': 1.996288900824176e-05, 'epoch': 0.6}
  8%|▊         | 3860/50000 [1:22:17<15:52:17,  1.24s/it]  8%|▊         | 3861/50000 [1:22:18<15:52:37,  1.24s/it]  8%|▊         | 3862/50000 [1:22:20<15:52:27,  1.24s/it]  8%|▊         | 3863/50000 [1:22:21<15:52:20,  1.24s/it]  8%|▊         | 3864/50000 [1:22:22<15:52:17,  1.24s/it]  8%|▊         | 3865/50000 [1:22:23<15:52:17,  1.24s/it]  8%|▊         | 3866/50000 [1:22:25<15:52:09,  1.24s/it]  8%|▊         | 3867/50000 [1:22:26<15:52:25,  1.24s/it]  8%|▊         | 3868/50000 [1:22:27<15:52:21,  1.24s/it]  8%|▊         | 3869/50000 [1:22:28<15:52:09,  1.24s/it]  8%|▊         | 3870/50000 [1:22:30<15:51:56,  1.24s/it]                                                         {'loss': 141.3609, 'learning_rate': 1.9962317557995483e-05, 'epoch': 0.6}
  8%|▊         | 3870/50000 [1:22:30<15:51:56,  1.24s/it]  8%|▊         | 3871/50000 [1:22:31<15:52:16,  1.24s/it]  8%|▊         | 3872/50000 [1:22:32<15:52:14,  1.24s/it]  8%|▊         | 3873/50000 [1:22:33<15:52:17,  1.24s/it]  8%|▊         | 3874/50000 [1:22:35<15:52:12,  1.24s/it]  8%|▊         | 3875/50000 [1:22:36<15:52:07,  1.24s/it]  8%|▊         | 3876/50000 [1:22:37<15:52:03,  1.24s/it]  8%|▊         | 3877/50000 [1:22:38<15:52:02,  1.24s/it]  8%|▊         | 3878/50000 [1:22:40<15:51:52,  1.24s/it]  8%|▊         | 3879/50000 [1:22:41<15:51:55,  1.24s/it]  8%|▊         | 3880/50000 [1:22:42<15:52:08,  1.24s/it]                                                         {'loss': 143.4781, 'learning_rate': 1.9961741749893045e-05, 'epoch': 0.6}
  8%|▊         | 3880/50000 [1:22:42<15:52:08,  1.24s/it]  8%|▊         | 3881/50000 [1:22:43<15:52:08,  1.24s/it]  8%|▊         | 3882/50000 [1:22:44<15:52:13,  1.24s/it]  8%|▊         | 3883/50000 [1:22:46<15:52:14,  1.24s/it]  8%|▊         | 3884/50000 [1:22:47<15:52:11,  1.24s/it]  8%|▊         | 3885/50000 [1:22:48<15:52:00,  1.24s/it]  8%|▊         | 3886/50000 [1:22:49<15:51:47,  1.24s/it]  8%|▊         | 3887/50000 [1:22:51<15:51:46,  1.24s/it]  8%|▊         | 3888/50000 [1:22:52<15:51:57,  1.24s/it]  8%|▊         | 3889/50000 [1:22:53<15:51:47,  1.24s/it]  8%|▊         | 3890/50000 [1:22:54<15:51:58,  1.24s/it]                                                         {'loss': 135.7859, 'learning_rate': 1.9961161584186323e-05, 'epoch': 0.6}
  8%|▊         | 3890/50000 [1:22:54<15:51:58,  1.24s/it]  8%|▊         | 3891/50000 [1:22:56<15:51:52,  1.24s/it]  8%|▊         | 3892/50000 [1:22:57<15:51:38,  1.24s/it]  8%|▊         | 3893/50000 [1:22:58<15:51:42,  1.24s/it]  8%|▊         | 3894/50000 [1:22:59<15:51:31,  1.24s/it]  8%|▊         | 3895/50000 [1:23:01<15:51:35,  1.24s/it]  8%|▊         | 3896/50000 [1:23:02<15:51:41,  1.24s/it]  8%|▊         | 3897/50000 [1:23:03<15:51:24,  1.24s/it]  8%|▊         | 3898/50000 [1:23:04<15:51:19,  1.24s/it]  8%|▊         | 3899/50000 [1:23:06<15:51:25,  1.24s/it]  8%|▊         | 3900/50000 [1:23:07<15:51:17,  1.24s/it]                                                         {'loss': 140.6813, 'learning_rate': 1.9960577061129104e-05, 'epoch': 0.6}
  8%|▊         | 3900/50000 [1:23:07<15:51:17,  1.24s/it]  8%|▊         | 3901/50000 [1:23:08<15:51:32,  1.24s/it]  8%|▊         | 3902/50000 [1:23:09<15:51:29,  1.24s/it]  8%|▊         | 3903/50000 [1:23:10<15:51:22,  1.24s/it]  8%|▊         | 3904/50000 [1:23:12<15:51:26,  1.24s/it]  8%|▊         | 3905/50000 [1:23:13<15:51:32,  1.24s/it]  8%|▊         | 3906/50000 [1:23:14<15:51:36,  1.24s/it]  8%|▊         | 3907/50000 [1:23:15<15:51:31,  1.24s/it]  8%|▊         | 3908/50000 [1:23:17<15:51:15,  1.24s/it]  8%|▊         | 3909/50000 [1:23:18<15:51:11,  1.24s/it]  8%|▊         | 3910/50000 [1:23:19<15:51:06,  1.24s/it]                                                         {'loss': 165.9938, 'learning_rate': 1.9959988180977076e-05, 'epoch': 0.61}
  8%|▊         | 3910/50000 [1:23:19<15:51:06,  1.24s/it]  8%|▊         | 3911/50000 [1:23:20<15:51:21,  1.24s/it]  8%|▊         | 3912/50000 [1:23:22<15:51:36,  1.24s/it]  8%|▊         | 3913/50000 [1:23:23<15:51:32,  1.24s/it]  8%|▊         | 3914/50000 [1:23:24<15:51:17,  1.24s/it]  8%|▊         | 3915/50000 [1:23:25<15:51:09,  1.24s/it]  8%|▊         | 3916/50000 [1:23:27<15:50:54,  1.24s/it]  8%|▊         | 3917/50000 [1:23:28<15:51:10,  1.24s/it]  8%|▊         | 3918/50000 [1:23:29<15:51:12,  1.24s/it]  8%|▊         | 3919/50000 [1:23:30<15:51:04,  1.24s/it]  8%|▊         | 3920/50000 [1:23:32<15:51:07,  1.24s/it]                                                         {'loss': 139.5455, 'learning_rate': 1.9959394943987836e-05, 'epoch': 0.61}
  8%|▊         | 3920/50000 [1:23:32<15:51:07,  1.24s/it]  8%|▊         | 3921/50000 [1:23:33<15:51:13,  1.24s/it]  8%|▊         | 3922/50000 [1:23:34<15:51:12,  1.24s/it]  8%|▊         | 3923/50000 [1:23:35<15:50:59,  1.24s/it]  8%|▊         | 3924/50000 [1:23:36<15:51:10,  1.24s/it]  8%|▊         | 3925/50000 [1:23:38<15:51:00,  1.24s/it]  8%|▊         | 3926/50000 [1:23:39<15:51:15,  1.24s/it]  8%|▊         | 3927/50000 [1:23:40<15:51:07,  1.24s/it]  8%|▊         | 3928/50000 [1:23:41<15:51:03,  1.24s/it]  8%|▊         | 3929/50000 [1:23:43<15:50:52,  1.24s/it]  8%|▊         | 3930/50000 [1:23:44<15:50:45,  1.24s/it]                                                         {'loss': 156.0359, 'learning_rate': 1.9958797350420886e-05, 'epoch': 0.61}
  8%|▊         | 3930/50000 [1:23:44<15:50:45,  1.24s/it]  8%|▊         | 3931/50000 [1:23:45<15:50:56,  1.24s/it]  8%|▊         | 3932/50000 [1:23:46<15:50:50,  1.24s/it]  8%|▊         | 3933/50000 [1:23:48<15:50:48,  1.24s/it]  8%|▊         | 3934/50000 [1:23:49<15:50:55,  1.24s/it]  8%|▊         | 3935/50000 [1:23:50<15:50:40,  1.24s/it]  8%|▊         | 3936/50000 [1:23:51<15:51:03,  1.24s/it]  8%|▊         | 3937/50000 [1:23:53<15:50:50,  1.24s/it]  8%|▊         | 3938/50000 [1:23:54<15:50:38,  1.24s/it]  8%|▊         | 3939/50000 [1:23:55<15:50:27,  1.24s/it]  8%|▊         | 3940/50000 [1:23:56<15:50:28,  1.24s/it]                                                         {'loss': 154.4578, 'learning_rate': 1.9958195400537633e-05, 'epoch': 0.61}
  8%|▊         | 3940/50000 [1:23:56<15:50:28,  1.24s/it]  8%|▊         | 3941/50000 [1:23:58<15:50:37,  1.24s/it]  8%|▊         | 3942/50000 [1:23:59<15:50:23,  1.24s/it]  8%|▊         | 3943/50000 [1:24:00<15:50:22,  1.24s/it]  8%|▊         | 3944/50000 [1:24:01<15:50:23,  1.24s/it]  8%|▊         | 3945/50000 [1:24:02<15:50:52,  1.24s/it]  8%|▊         | 3946/50000 [1:24:04<15:50:38,  1.24s/it]  8%|▊         | 3947/50000 [1:24:05<15:50:25,  1.24s/it]  8%|▊         | 3948/50000 [1:24:06<15:50:33,  1.24s/it]  8%|▊         | 3949/50000 [1:24:07<15:50:26,  1.24s/it]  8%|▊         | 3950/50000 [1:24:09<15:50:36,  1.24s/it]                                                         {'loss': 159.2391, 'learning_rate': 1.995758909460139e-05, 'epoch': 0.61}
  8%|▊         | 3950/50000 [1:24:09<15:50:36,  1.24s/it]  8%|▊         | 3951/50000 [1:24:10<15:50:58,  1.24s/it]  8%|▊         | 3952/50000 [1:24:11<15:50:54,  1.24s/it]  8%|▊         | 3953/50000 [1:24:12<15:51:01,  1.24s/it]  8%|▊         | 3954/50000 [1:24:14<15:50:45,  1.24s/it]  8%|▊         | 3955/50000 [1:24:15<15:50:38,  1.24s/it]  8%|▊         | 3956/50000 [1:24:16<15:50:35,  1.24s/it]  8%|▊         | 3957/50000 [1:24:17<15:50:37,  1.24s/it]  8%|▊         | 3958/50000 [1:24:19<15:50:33,  1.24s/it]  8%|▊         | 3959/50000 [1:24:20<15:50:29,  1.24s/it]  8%|▊         | 3960/50000 [1:24:21<15:50:24,  1.24s/it]                                                         {'loss': 134.7828, 'learning_rate': 1.995697843287738e-05, 'epoch': 0.61}
  8%|▊         | 3960/50000 [1:24:21<15:50:24,  1.24s/it]  8%|▊         | 3961/50000 [1:24:22<15:50:30,  1.24s/it][2023-07-03 13:24:33,814] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
  8%|▊         | 3962/50000 [1:24:23<14:53:09,  1.16s/it]  8%|▊         | 3963/50000 [1:24:25<15:10:03,  1.19s/it]  8%|▊         | 3964/50000 [1:24:26<15:22:02,  1.20s/it]  8%|▊         | 3965/50000 [1:24:27<15:30:24,  1.21s/it][2023-07-03 13:24:38,518] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
  8%|▊         | 3966/50000 [1:24:28<14:39:08,  1.15s/it]  8%|▊         | 3967/50000 [1:24:29<14:59:59,  1.17s/it]  8%|▊         | 3968/50000 [1:24:30<15:14:58,  1.19s/it]  8%|▊         | 3969/50000 [1:24:32<15:25:27,  1.21s/it]  8%|▊         | 3970/50000 [1:24:33<15:32:59,  1.22s/it]                                                         {'loss': 160.3078, 'learning_rate': 1.9956486767510417e-05, 'epoch': 0.62}
  8%|▊         | 3970/50000 [1:24:33<15:32:59,  1.22s/it]  8%|▊         | 3971/50000 [1:24:34<15:38:20,  1.22s/it]  8%|▊         | 3972/50000 [1:24:35<15:41:45,  1.23s/it]  8%|▊         | 3973/50000 [1:24:37<15:44:15,  1.23s/it]  8%|▊         | 3974/50000 [1:24:38<15:45:48,  1.23s/it]  8%|▊         | 3975/50000 [1:24:39<15:47:06,  1.23s/it]  8%|▊         | 3976/50000 [1:24:40<15:48:05,  1.24s/it]  8%|▊         | 3977/50000 [1:24:42<15:48:36,  1.24s/it]  8%|▊         | 3978/50000 [1:24:43<15:49:01,  1.24s/it]  8%|▊         | 3979/50000 [1:24:44<15:49:24,  1.24s/it]  8%|▊         | 3980/50000 [1:24:45<15:49:30,  1.24s/it]                                                         {'loss': 169.0891, 'learning_rate': 1.995586826604285e-05, 'epoch': 0.62}
  8%|▊         | 3980/50000 [1:24:45<15:49:30,  1.24s/it]  8%|▊         | 3981/50000 [1:24:47<15:49:51,  1.24s/it]  8%|▊         | 3982/50000 [1:24:48<15:49:43,  1.24s/it]  8%|▊         | 3983/50000 [1:24:49<15:49:43,  1.24s/it]  8%|▊         | 3984/50000 [1:24:50<15:49:48,  1.24s/it]  8%|▊         | 3985/50000 [1:24:52<15:49:49,  1.24s/it]  8%|▊         | 3986/50000 [1:24:53<15:49:48,  1.24s/it]  8%|▊         | 3987/50000 [1:24:54<15:49:48,  1.24s/it]  8%|▊         | 3988/50000 [1:24:55<15:49:40,  1.24s/it]  8%|▊         | 3989/50000 [1:24:56<15:49:40,  1.24s/it]  8%|▊         | 3990/50000 [1:24:58<15:49:41,  1.24s/it]                                                         {'loss': 154.1687, 'learning_rate': 1.9955245409540262e-05, 'epoch': 0.62}
  8%|▊         | 3990/50000 [1:24:58<15:49:41,  1.24s/it]  8%|▊         | 3991/50000 [1:24:59<15:49:49,  1.24s/it]  8%|▊         | 3992/50000 [1:25:00<15:49:39,  1.24s/it]  8%|▊         | 3993/50000 [1:25:01<15:50:16,  1.24s/it]  8%|▊         | 3994/50000 [1:25:03<15:50:00,  1.24s/it]  8%|▊         | 3995/50000 [1:25:04<15:49:56,  1.24s/it]  8%|▊         | 3996/50000 [1:25:05<15:49:44,  1.24s/it]  8%|▊         | 3997/50000 [1:25:06<15:49:46,  1.24s/it]  8%|▊         | 3998/50000 [1:25:08<15:49:37,  1.24s/it]  8%|▊         | 3999/50000 [1:25:09<15:49:19,  1.24s/it][2023-07-03 13:25:20,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=59, lr=[1.9954618198275112e-05], mom=[(0.9, 0.999)]
[2023-07-03 13:25:20,628] [INFO] [timer.py:199:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=3.2500111539795338, CurrSamplesPerSec=3.2477805731135163, MemAllocated=47.21GB, MaxMemAllocated=59.8GB
  8%|▊         | 4000/50000 [1:25:10<15:49:26,  1.24s/it]                                                         {'loss': 157.1625, 'learning_rate': 1.9954618198275112e-05, 'epoch': 0.62}
  8%|▊         | 4000/50000 [1:25:10<15:49:26,  1.24s/it][INFO|trainer.py:3129] 2023-07-03 13:25:20,635 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 13:25:20,635 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 13:25:20,635 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.54it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                         
                                             [A{'eval_loss': 166.375, 'eval_accuracy': 0.38928195092578655, 'eval_runtime': 3.1014, 'eval_samples_per_second': 8.383, 'eval_steps_per_second': 2.257, 'epoch': 0.62}
  8%|▊         | 4000/50000 [1:25:13<15:49:26,  1.24s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 13:25:23,738 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000
[INFO|trainer.py:2880] 2023-07-03 13:25:23,751 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 13:25:33,813 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 13:25:33,814 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/special_tokens_map.json
[2023-07-03 13:25:33,816] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 13:25:33,841] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/global_step4000/mp_rank_00_model_states.pt
[2023-07-03 13:25:33,841] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/global_step4000/mp_rank_00_model_states.pt...
[2023-07-03 13:25:44,399] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/global_step4000/mp_rank_00_model_states.pt.
[2023-07-03 13:25:44,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 13:26:01,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 13:26:01,026] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 13:26:01,026] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 13:26:01,076 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-3000] due to args.save_total_limit
  8%|▊         | 4001/50000 [1:26:01<205:16:14, 16.07s/it]  8%|▊         | 4002/50000 [1:26:02<148:46:12, 11.64s/it]  8%|▊         | 4003/50000 [1:26:03<108:54:14,  8.52s/it]  8%|▊         | 4004/50000 [1:26:05<81:01:14,  6.34s/it]   8%|▊         | 4005/50000 [1:26:06<61:30:38,  4.81s/it]  8%|▊         | 4006/50000 [1:26:07<47:50:53,  3.75s/it]  8%|▊         | 4007/50000 [1:26:08<38:15:38,  2.99s/it]  8%|▊         | 4008/50000 [1:26:10<31:31:09,  2.47s/it]  8%|▊         | 4009/50000 [1:26:11<26:48:17,  2.10s/it]  8%|▊         | 4010/50000 [1:26:12<23:30:13,  1.84s/it]                                                         {'loss': 154.1672, 'learning_rate': 1.9953986632521763e-05, 'epoch': 0.62}
  8%|▊         | 4010/50000 [1:26:12<23:30:13,  1.84s/it]  8%|▊         | 4011/50000 [1:26:13<21:12:00,  1.66s/it]  8%|▊         | 4012/50000 [1:26:15<19:35:35,  1.53s/it]  8%|▊         | 4013/50000 [1:26:16<18:28:37,  1.45s/it]  8%|▊         | 4014/50000 [1:26:17<17:41:18,  1.38s/it]  8%|▊         | 4015/50000 [1:26:18<17:08:00,  1.34s/it]  8%|▊         | 4016/50000 [1:26:19<16:44:46,  1.31s/it]  8%|▊         | 4017/50000 [1:26:21<16:27:52,  1.29s/it]  8%|▊         | 4018/50000 [1:26:22<16:15:31,  1.27s/it]  8%|▊         | 4019/50000 [1:26:23<16:07:33,  1.26s/it]  8%|▊         | 4020/50000 [1:26:24<16:02:17,  1.26s/it]                                                         {'loss': 149.9328, 'learning_rate': 1.995335071255648e-05, 'epoch': 0.62}
  8%|▊         | 4020/50000 [1:26:24<16:02:17,  1.26s/it]  8%|▊         | 4021/50000 [1:26:26<15:58:50,  1.25s/it]  8%|▊         | 4022/50000 [1:26:27<15:55:38,  1.25s/it]  8%|▊         | 4023/50000 [1:26:28<15:53:06,  1.24s/it]  8%|▊         | 4024/50000 [1:26:29<15:51:14,  1.24s/it]  8%|▊         | 4025/50000 [1:26:31<15:50:17,  1.24s/it]  8%|▊         | 4026/50000 [1:26:32<15:49:28,  1.24s/it]  8%|▊         | 4027/50000 [1:26:33<15:48:56,  1.24s/it]  8%|▊         | 4028/50000 [1:26:34<15:48:23,  1.24s/it]  8%|▊         | 4029/50000 [1:26:36<15:47:44,  1.24s/it]  8%|▊         | 4030/50000 [1:26:37<15:47:22,  1.24s/it]                                                         {'loss': 161.3172, 'learning_rate': 1.995271043865744e-05, 'epoch': 0.62}
  8%|▊         | 4030/50000 [1:26:37<15:47:22,  1.24s/it]  8%|▊         | 4031/50000 [1:26:38<15:47:40,  1.24s/it]  8%|▊         | 4032/50000 [1:26:39<15:47:31,  1.24s/it]  8%|▊         | 4033/50000 [1:26:41<15:47:27,  1.24s/it]  8%|▊         | 4034/50000 [1:26:42<15:47:16,  1.24s/it]  8%|▊         | 4035/50000 [1:26:43<15:47:04,  1.24s/it]  8%|▊         | 4036/50000 [1:26:44<15:47:04,  1.24s/it]  8%|▊         | 4037/50000 [1:26:45<15:47:01,  1.24s/it]  8%|▊         | 4038/50000 [1:26:47<15:47:36,  1.24s/it]  8%|▊         | 4039/50000 [1:26:48<15:47:08,  1.24s/it]  8%|▊         | 4040/50000 [1:26:49<15:47:23,  1.24s/it]                                                         {'loss': 146.0453, 'learning_rate': 1.995206581110472e-05, 'epoch': 0.63}
  8%|▊         | 4040/50000 [1:26:49<15:47:23,  1.24s/it]  8%|▊         | 4041/50000 [1:26:50<15:48:51,  1.24s/it]  8%|▊         | 4042/50000 [1:26:52<15:48:54,  1.24s/it]  8%|▊         | 4043/50000 [1:26:53<15:49:03,  1.24s/it]  8%|▊         | 4044/50000 [1:26:54<15:49:09,  1.24s/it]  8%|▊         | 4045/50000 [1:26:55<15:49:33,  1.24s/it]  8%|▊         | 4046/50000 [1:26:57<15:49:32,  1.24s/it]  8%|▊         | 4047/50000 [1:26:58<15:49:25,  1.24s/it]  8%|▊         | 4048/50000 [1:26:59<15:49:28,  1.24s/it]  8%|▊         | 4049/50000 [1:27:00<15:49:27,  1.24s/it]  8%|▊         | 4050/50000 [1:27:02<15:49:29,  1.24s/it]                                                         {'loss': 149.5406, 'learning_rate': 1.9951416830180302e-05, 'epoch': 0.63}
  8%|▊         | 4050/50000 [1:27:02<15:49:29,  1.24s/it]  8%|▊         | 4051/50000 [1:27:03<15:49:59,  1.24s/it]  8%|▊         | 4052/50000 [1:27:04<15:49:29,  1.24s/it]  8%|▊         | 4053/50000 [1:27:05<15:49:23,  1.24s/it]  8%|▊         | 4054/50000 [1:27:07<15:49:26,  1.24s/it]  8%|▊         | 4055/50000 [1:27:08<15:49:23,  1.24s/it]  8%|▊         | 4056/50000 [1:27:09<15:49:05,  1.24s/it]  8%|▊         | 4057/50000 [1:27:10<15:48:58,  1.24s/it]  8%|▊         | 4058/50000 [1:27:11<15:49:06,  1.24s/it]  8%|▊         | 4059/50000 [1:27:13<15:49:37,  1.24s/it]  8%|▊         | 4060/50000 [1:27:14<15:49:12,  1.24s/it]                                                         {'loss': 176.7156, 'learning_rate': 1.995076349616807e-05, 'epoch': 0.63}
  8%|▊         | 4060/50000 [1:27:14<15:49:12,  1.24s/it]  8%|▊         | 4061/50000 [1:27:15<15:49:24,  1.24s/it]  8%|▊         | 4062/50000 [1:27:16<15:49:08,  1.24s/it]  8%|▊         | 4063/50000 [1:27:18<15:58:42,  1.25s/it]  8%|▊         | 4064/50000 [1:27:19<15:55:53,  1.25s/it]  8%|▊         | 4065/50000 [1:27:20<15:54:33,  1.25s/it]  8%|▊         | 4066/50000 [1:27:21<15:52:46,  1.24s/it][2023-07-03 13:27:32,965] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
  8%|▊         | 4067/50000 [1:27:22<14:54:24,  1.17s/it]  8%|▊         | 4068/50000 [1:27:24<15:11:04,  1.19s/it][2023-07-03 13:27:35,198] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
  8%|▊         | 4069/50000 [1:27:25<14:25:27,  1.13s/it]  8%|▊         | 4070/50000 [1:27:26<14:51:29,  1.16s/it]                                                         {'loss': 182.0422, 'learning_rate': 1.9950237694927052e-05, 'epoch': 0.63}
  8%|▊         | 4070/50000 [1:27:26<14:51:29,  1.16s/it]  8%|▊         | 4071/50000 [1:27:27<15:08:59,  1.19s/it]  8%|▊         | 4072/50000 [1:27:28<15:20:41,  1.20s/it]  8%|▊         | 4073/50000 [1:27:30<15:28:54,  1.21s/it]  8%|▊         | 4074/50000 [1:27:31<15:34:58,  1.22s/it]  8%|▊         | 4075/50000 [1:27:32<15:38:49,  1.23s/it]  8%|▊         | 4076/50000 [1:27:33<15:41:44,  1.23s/it]  8%|▊         | 4077/50000 [1:27:35<15:43:52,  1.23s/it]  8%|▊         | 4078/50000 [1:27:36<15:45:25,  1.24s/it]  8%|▊         | 4079/50000 [1:27:37<15:46:28,  1.24s/it]  8%|▊         | 4080/50000 [1:27:38<15:47:09,  1.24s/it]                                                         {'loss': 174.0625, 'learning_rate': 1.994957652607823e-05, 'epoch': 0.63}
  8%|▊         | 4080/50000 [1:27:38<15:47:09,  1.24s/it]  8%|▊         | 4081/50000 [1:27:40<15:47:57,  1.24s/it]  8%|▊         | 4082/50000 [1:27:41<15:48:21,  1.24s/it]  8%|▊         | 4083/50000 [1:27:42<15:48:44,  1.24s/it]  8%|▊         | 4084/50000 [1:27:43<15:49:27,  1.24s/it]  8%|▊         | 4085/50000 [1:27:45<15:49:41,  1.24s/it]  8%|▊         | 4086/50000 [1:27:46<15:49:25,  1.24s/it]  8%|▊         | 4087/50000 [1:27:47<15:49:01,  1.24s/it]  8%|▊         | 4088/50000 [1:27:48<15:48:43,  1.24s/it]  8%|▊         | 4089/50000 [1:27:49<15:48:48,  1.24s/it]  8%|▊         | 4090/50000 [1:27:51<15:48:34,  1.24s/it]                                                         {'loss': 144.2531, 'learning_rate': 1.9948911004946606e-05, 'epoch': 0.63}
  8%|▊         | 4090/50000 [1:27:51<15:48:34,  1.24s/it]  8%|▊         | 4091/50000 [1:27:52<15:48:37,  1.24s/it]  8%|▊         | 4092/50000 [1:27:53<15:48:38,  1.24s/it]  8%|▊         | 4093/50000 [1:27:54<15:49:12,  1.24s/it]  8%|▊         | 4094/50000 [1:27:56<15:49:06,  1.24s/it]  8%|▊         | 4095/50000 [1:27:57<15:48:56,  1.24s/it]  8%|▊         | 4096/50000 [1:27:58<15:48:45,  1.24s/it]  8%|▊         | 4097/50000 [1:27:59<15:48:53,  1.24s/it]  8%|▊         | 4098/50000 [1:28:01<15:48:40,  1.24s/it]  8%|▊         | 4099/50000 [1:28:02<15:48:42,  1.24s/it]  8%|▊         | 4100/50000 [1:28:03<15:48:35,  1.24s/it]                                                         {'loss': 154.8141, 'learning_rate': 1.9948241131823306e-05, 'epoch': 0.64}
  8%|▊         | 4100/50000 [1:28:03<15:48:35,  1.24s/it]  8%|▊         | 4101/50000 [1:28:04<15:48:35,  1.24s/it]  8%|▊         | 4102/50000 [1:28:06<15:48:28,  1.24s/it]  8%|▊         | 4103/50000 [1:28:07<15:48:17,  1.24s/it]  8%|▊         | 4104/50000 [1:28:08<15:48:25,  1.24s/it]  8%|▊         | 4105/50000 [1:28:09<15:48:11,  1.24s/it]  8%|▊         | 4106/50000 [1:28:11<15:48:34,  1.24s/it]  8%|▊         | 4107/50000 [1:28:12<15:48:06,  1.24s/it]  8%|▊         | 4108/50000 [1:28:13<15:48:01,  1.24s/it][2023-07-03 13:28:24,552] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
  8%|▊         | 4109/50000 [1:28:14<14:51:02,  1.16s/it]  8%|▊         | 4110/50000 [1:28:15<15:07:59,  1.19s/it]                                                         {'loss': 220.7789, 'learning_rate': 1.9947634525301595e-05, 'epoch': 0.64}
  8%|▊         | 4110/50000 [1:28:15<15:07:59,  1.19s/it]  8%|▊         | 4111/50000 [1:28:17<15:20:35,  1.20s/it]  8%|▊         | 4112/50000 [1:28:18<15:28:45,  1.21s/it]  8%|▊         | 4113/50000 [1:28:19<15:35:28,  1.22s/it]  8%|▊         | 4114/50000 [1:28:20<15:51:52,  1.24s/it]  8%|▊         | 4115/50000 [1:28:22<15:51:00,  1.24s/it]  8%|▊         | 4116/50000 [1:28:23<15:50:47,  1.24s/it]  8%|▊         | 4117/50000 [1:28:24<15:50:02,  1.24s/it]  8%|▊         | 4118/50000 [1:28:25<15:50:55,  1.24s/it]  8%|▊         | 4119/50000 [1:28:27<15:50:25,  1.24s/it]  8%|▊         | 4120/50000 [1:28:28<15:49:24,  1.24s/it]                                                         {'loss': 165.8516, 'learning_rate': 1.994695638420296e-05, 'epoch': 0.64}
  8%|▊         | 4120/50000 [1:28:28<15:49:24,  1.24s/it]  8%|▊         | 4121/50000 [1:28:29<15:48:53,  1.24s/it]  8%|▊         | 4122/50000 [1:28:30<15:48:17,  1.24s/it]  8%|▊         | 4123/50000 [1:28:31<15:47:58,  1.24s/it]  8%|▊         | 4124/50000 [1:28:33<15:48:03,  1.24s/it]  8%|▊         | 4125/50000 [1:28:34<15:48:04,  1.24s/it]  8%|▊         | 4126/50000 [1:28:35<15:48:40,  1.24s/it]  8%|▊         | 4127/50000 [1:28:36<15:48:17,  1.24s/it]  8%|▊         | 4128/50000 [1:28:38<15:47:49,  1.24s/it]  8%|▊         | 4129/50000 [1:28:39<15:47:23,  1.24s/it]  8%|▊         | 4130/50000 [1:28:40<15:47:14,  1.24s/it]                                                         {'loss': 148.3531, 'learning_rate': 1.9946273891967664e-05, 'epoch': 0.64}
  8%|▊         | 4130/50000 [1:28:40<15:47:14,  1.24s/it]  8%|▊         | 4131/50000 [1:28:41<15:47:50,  1.24s/it]  8%|▊         | 4132/50000 [1:28:43<15:47:27,  1.24s/it]  8%|▊         | 4133/50000 [1:28:44<15:47:37,  1.24s/it]  8%|▊         | 4134/50000 [1:28:45<15:47:20,  1.24s/it]  8%|▊         | 4135/50000 [1:28:46<15:47:04,  1.24s/it]  8%|▊         | 4136/50000 [1:28:48<15:46:57,  1.24s/it]  8%|▊         | 4137/50000 [1:28:49<15:46:50,  1.24s/it]  8%|▊         | 4138/50000 [1:28:50<15:46:41,  1.24s/it]  8%|▊         | 4139/50000 [1:28:51<15:47:10,  1.24s/it]  8%|▊         | 4140/50000 [1:28:53<15:47:30,  1.24s/it]                                                         {'loss': 156.2734, 'learning_rate': 1.9945587048894254e-05, 'epoch': 0.64}
  8%|▊         | 4140/50000 [1:28:53<15:47:30,  1.24s/it]  8%|▊         | 4141/50000 [1:28:54<15:47:25,  1.24s/it]  8%|▊         | 4142/50000 [1:28:55<15:47:00,  1.24s/it]  8%|▊         | 4143/50000 [1:28:56<15:46:52,  1.24s/it]  8%|▊         | 4144/50000 [1:28:57<15:46:55,  1.24s/it]  8%|▊         | 4145/50000 [1:28:59<15:46:42,  1.24s/it]  8%|▊         | 4146/50000 [1:29:00<15:46:36,  1.24s/it][2023-07-03 13:29:11,470] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
  8%|▊         | 4147/50000 [1:29:01<14:49:28,  1.16s/it]  8%|▊         | 4148/50000 [1:29:02<15:06:12,  1.19s/it]  8%|▊         | 4149/50000 [1:29:03<15:18:25,  1.20s/it]  8%|▊         | 4150/50000 [1:29:05<15:26:50,  1.21s/it]                                                         {'loss': 146.4688, 'learning_rate': 1.9944965170409877e-05, 'epoch': 0.64}
  8%|▊         | 4150/50000 [1:29:05<15:26:50,  1.21s/it]  8%|▊         | 4151/50000 [1:29:06<15:32:47,  1.22s/it]  8%|▊         | 4152/50000 [1:29:07<15:36:54,  1.23s/it]  8%|▊         | 4153/50000 [1:29:08<15:39:41,  1.23s/it]  8%|▊         | 4154/50000 [1:29:10<15:41:43,  1.23s/it]  8%|▊         | 4155/50000 [1:29:11<15:43:50,  1.24s/it]  8%|▊         | 4156/50000 [1:29:12<15:45:04,  1.24s/it]  8%|▊         | 4157/50000 [1:29:13<15:45:47,  1.24s/it]  8%|▊         | 4158/50000 [1:29:15<15:46:15,  1.24s/it]  8%|▊         | 4159/50000 [1:29:16<15:46:34,  1.24s/it]  8%|▊         | 4160/50000 [1:29:17<15:46:33,  1.24s/it]                                                         {'loss': 147.6562, 'learning_rate': 1.9944270061573352e-05, 'epoch': 0.64}
  8%|▊         | 4160/50000 [1:29:17<15:46:33,  1.24s/it]  8%|▊         | 4161/50000 [1:29:18<15:47:00,  1.24s/it]  8%|▊         | 4162/50000 [1:29:20<15:47:06,  1.24s/it]  8%|▊         | 4163/50000 [1:29:21<15:47:25,  1.24s/it]  8%|▊         | 4164/50000 [1:29:22<15:47:23,  1.24s/it]  8%|▊         | 4165/50000 [1:29:23<15:47:32,  1.24s/it]  8%|▊         | 4166/50000 [1:29:25<15:47:22,  1.24s/it]  8%|▊         | 4167/50000 [1:29:26<15:47:15,  1.24s/it]  8%|▊         | 4168/50000 [1:29:27<15:47:07,  1.24s/it]  8%|▊         | 4169/50000 [1:29:28<15:47:21,  1.24s/it]  8%|▊         | 4170/50000 [1:29:29<15:47:14,  1.24s/it]                                                         {'loss': 147.0641, 'learning_rate': 1.9943570602775253e-05, 'epoch': 0.65}
  8%|▊         | 4170/50000 [1:29:29<15:47:14,  1.24s/it]  8%|▊         | 4171/50000 [1:29:31<15:47:25,  1.24s/it]  8%|▊         | 4172/50000 [1:29:32<15:47:14,  1.24s/it]  8%|▊         | 4173/50000 [1:29:33<15:47:11,  1.24s/it]  8%|▊         | 4174/50000 [1:29:34<15:46:58,  1.24s/it]  8%|▊         | 4175/50000 [1:29:36<15:46:54,  1.24s/it]  8%|▊         | 4176/50000 [1:29:37<15:46:57,  1.24s/it]  8%|▊         | 4177/50000 [1:29:38<15:46:45,  1.24s/it]  8%|▊         | 4178/50000 [1:29:39<15:46:55,  1.24s/it]  8%|▊         | 4179/50000 [1:29:41<15:46:45,  1.24s/it]  8%|▊         | 4180/50000 [1:29:42<15:46:32,  1.24s/it]                                                         {'loss': 156.6422, 'learning_rate': 1.994286679432155e-05, 'epoch': 0.65}
  8%|▊         | 4180/50000 [1:29:42<15:46:32,  1.24s/it]  8%|▊         | 4181/50000 [1:29:43<15:46:37,  1.24s/it]  8%|▊         | 4182/50000 [1:29:44<15:46:25,  1.24s/it]  8%|▊         | 4183/50000 [1:29:46<15:46:27,  1.24s/it]  8%|▊         | 4184/50000 [1:29:47<15:46:34,  1.24s/it]  8%|▊         | 4185/50000 [1:29:48<15:46:33,  1.24s/it]  8%|▊         | 4186/50000 [1:29:49<15:47:03,  1.24s/it]  8%|▊         | 4187/50000 [1:29:51<15:47:51,  1.24s/it]  8%|▊         | 4188/50000 [1:29:52<15:47:42,  1.24s/it]  8%|▊         | 4189/50000 [1:29:53<15:47:53,  1.24s/it]  8%|▊         | 4190/50000 [1:29:54<15:47:56,  1.24s/it]                                                         {'loss': 144.8781, 'learning_rate': 1.9942158636520112e-05, 'epoch': 0.65}
  8%|▊         | 4190/50000 [1:29:54<15:47:56,  1.24s/it]  8%|▊         | 4191/50000 [1:29:56<15:48:29,  1.24s/it]  8%|▊         | 4192/50000 [1:29:57<15:48:20,  1.24s/it]  8%|▊         | 4193/50000 [1:29:58<15:48:29,  1.24s/it]  8%|▊         | 4194/50000 [1:29:59<15:48:23,  1.24s/it][2023-07-03 13:30:10,755] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  8%|▊         | 4195/50000 [1:30:00<14:51:30,  1.17s/it]  8%|▊         | 4196/50000 [1:30:01<15:08:32,  1.19s/it]  8%|▊         | 4197/50000 [1:30:03<15:20:33,  1.21s/it]  8%|▊         | 4198/50000 [1:30:04<15:29:06,  1.22s/it]  8%|▊         | 4199/50000 [1:30:05<15:34:37,  1.22s/it]  8%|▊         | 4200/50000 [1:30:06<15:38:18,  1.23s/it]                                                         {'loss': 155.4078, 'learning_rate': 1.994151757606249e-05, 'epoch': 0.65}
  8%|▊         | 4200/50000 [1:30:06<15:38:18,  1.23s/it]  8%|▊         | 4201/50000 [1:30:08<15:41:21,  1.23s/it]  8%|▊         | 4202/50000 [1:30:09<15:42:56,  1.24s/it]  8%|▊         | 4203/50000 [1:30:10<15:44:10,  1.24s/it]  8%|▊         | 4204/50000 [1:30:11<15:44:59,  1.24s/it]  8%|▊         | 4205/50000 [1:30:13<15:45:50,  1.24s/it]  8%|▊         | 4206/50000 [1:30:14<15:46:29,  1.24s/it]  8%|▊         | 4207/50000 [1:30:15<15:46:47,  1.24s/it]  8%|▊         | 4208/50000 [1:30:16<15:46:47,  1.24s/it]  8%|▊         | 4209/50000 [1:30:18<15:46:48,  1.24s/it]  8%|▊         | 4210/50000 [1:30:19<15:47:06,  1.24s/it]                                                         {'loss': 170.3844, 'learning_rate': 1.994080115535535e-05, 'epoch': 0.65}
  8%|▊         | 4210/50000 [1:30:19<15:47:06,  1.24s/it]  8%|▊         | 4211/50000 [1:30:20<15:47:13,  1.24s/it]  8%|▊         | 4212/50000 [1:30:21<15:47:02,  1.24s/it]  8%|▊         | 4213/50000 [1:30:23<15:46:53,  1.24s/it]  8%|▊         | 4214/50000 [1:30:24<15:47:10,  1.24s/it]  8%|▊         | 4215/50000 [1:30:25<15:47:07,  1.24s/it]  8%|▊         | 4216/50000 [1:30:26<15:47:01,  1.24s/it]  8%|▊         | 4217/50000 [1:30:28<15:46:55,  1.24s/it]  8%|▊         | 4218/50000 [1:30:29<15:47:49,  1.24s/it]  8%|▊         | 4219/50000 [1:30:30<15:47:42,  1.24s/it]  8%|▊         | 4220/50000 [1:30:31<15:46:56,  1.24s/it]                                                         {'loss': 136.0016, 'learning_rate': 1.994008038620405e-05, 'epoch': 0.65}
  8%|▊         | 4220/50000 [1:30:31<15:46:56,  1.24s/it]  8%|▊         | 4221/50000 [1:30:33<15:47:06,  1.24s/it]  8%|▊         | 4222/50000 [1:30:34<15:47:23,  1.24s/it]  8%|▊         | 4223/50000 [1:30:35<15:47:30,  1.24s/it]  8%|▊         | 4224/50000 [1:30:36<15:46:54,  1.24s/it]  8%|▊         | 4225/50000 [1:30:37<15:46:45,  1.24s/it]  8%|▊         | 4226/50000 [1:30:39<15:46:45,  1.24s/it]  8%|▊         | 4227/50000 [1:30:40<15:47:02,  1.24s/it]  8%|▊         | 4228/50000 [1:30:41<15:47:00,  1.24s/it]  8%|▊         | 4229/50000 [1:30:42<15:46:43,  1.24s/it]  8%|▊         | 4230/50000 [1:30:44<15:46:49,  1.24s/it]                                                         {'loss': 139.4375, 'learning_rate': 1.9939355268923885e-05, 'epoch': 0.66}
  8%|▊         | 4230/50000 [1:30:44<15:46:49,  1.24s/it]  8%|▊         | 4231/50000 [1:30:45<15:47:14,  1.24s/it]  8%|▊         | 4232/50000 [1:30:46<15:46:33,  1.24s/it]  8%|▊         | 4233/50000 [1:30:47<15:46:33,  1.24s/it]  8%|▊         | 4234/50000 [1:30:49<15:46:24,  1.24s/it]  8%|▊         | 4235/50000 [1:30:50<15:46:30,  1.24s/it]  8%|▊         | 4236/50000 [1:30:51<15:46:38,  1.24s/it]  8%|▊         | 4237/50000 [1:30:52<15:46:55,  1.24s/it]  8%|▊         | 4238/50000 [1:30:54<15:46:46,  1.24s/it]  8%|▊         | 4239/50000 [1:30:55<15:47:02,  1.24s/it]  8%|▊         | 4240/50000 [1:30:56<15:49:31,  1.25s/it]                                                         {'loss': 147.0516, 'learning_rate': 1.993862580383205e-05, 'epoch': 0.66}
  8%|▊         | 4240/50000 [1:30:56<15:49:31,  1.25s/it]  8%|▊         | 4241/50000 [1:30:57<15:49:01,  1.24s/it]  8%|▊         | 4242/50000 [1:30:59<15:49:26,  1.24s/it]  8%|▊         | 4243/50000 [1:31:00<15:50:52,  1.25s/it]  8%|▊         | 4244/50000 [1:31:01<15:52:28,  1.25s/it]  8%|▊         | 4245/50000 [1:31:02<15:50:34,  1.25s/it]  8%|▊         | 4246/50000 [1:31:04<15:49:12,  1.24s/it]  8%|▊         | 4247/50000 [1:31:05<15:48:31,  1.24s/it]  8%|▊         | 4248/50000 [1:31:06<15:47:58,  1.24s/it]  8%|▊         | 4249/50000 [1:31:07<15:47:42,  1.24s/it]  8%|▊         | 4250/50000 [1:31:09<15:47:37,  1.24s/it]                                                         {'loss': 165.7109, 'learning_rate': 1.9937891991247625e-05, 'epoch': 0.66}
  8%|▊         | 4250/50000 [1:31:09<15:47:37,  1.24s/it]  9%|▊         | 4251/50000 [1:31:10<15:47:53,  1.24s/it]  9%|▊         | 4252/50000 [1:31:11<15:47:39,  1.24s/it]  9%|▊         | 4253/50000 [1:31:12<15:47:53,  1.24s/it]  9%|▊         | 4254/50000 [1:31:14<15:47:41,  1.24s/it]  9%|▊         | 4255/50000 [1:31:15<15:47:02,  1.24s/it]  9%|▊         | 4256/50000 [1:31:16<15:47:17,  1.24s/it]  9%|▊         | 4257/50000 [1:31:17<15:47:03,  1.24s/it]  9%|▊         | 4258/50000 [1:31:18<15:46:46,  1.24s/it]  9%|▊         | 4259/50000 [1:31:20<15:46:26,  1.24s/it]  9%|▊         | 4260/50000 [1:31:21<15:46:15,  1.24s/it]                                                         {'loss': 147.3187, 'learning_rate': 1.9937153831491615e-05, 'epoch': 0.66}
  9%|▊         | 4260/50000 [1:31:21<15:46:15,  1.24s/it]  9%|▊         | 4261/50000 [1:31:22<15:46:29,  1.24s/it]  9%|▊         | 4262/50000 [1:31:23<15:46:35,  1.24s/it]  9%|▊         | 4263/50000 [1:31:25<15:46:21,  1.24s/it]  9%|▊         | 4264/50000 [1:31:26<15:46:28,  1.24s/it]  9%|▊         | 4265/50000 [1:31:27<15:46:35,  1.24s/it]  9%|▊         | 4266/50000 [1:31:28<15:46:45,  1.24s/it]  9%|▊         | 4267/50000 [1:31:30<15:47:15,  1.24s/it]  9%|▊         | 4268/50000 [1:31:31<15:47:22,  1.24s/it]  9%|▊         | 4269/50000 [1:31:32<15:47:28,  1.24s/it]  9%|▊         | 4270/50000 [1:31:33<15:47:18,  1.24s/it]                                                         {'loss': 145.5797, 'learning_rate': 1.993641132488691e-05, 'epoch': 0.66}
  9%|▊         | 4270/50000 [1:31:33<15:47:18,  1.24s/it]  9%|▊         | 4271/50000 [1:31:35<15:47:52,  1.24s/it]  9%|▊         | 4272/50000 [1:31:36<15:47:58,  1.24s/it]  9%|▊         | 4273/50000 [1:31:37<15:47:28,  1.24s/it]  9%|▊         | 4274/50000 [1:31:38<15:47:14,  1.24s/it]  9%|▊         | 4275/50000 [1:31:40<15:46:46,  1.24s/it]  9%|▊         | 4276/50000 [1:31:41<15:46:29,  1.24s/it]  9%|▊         | 4277/50000 [1:31:42<15:46:08,  1.24s/it]  9%|▊         | 4278/50000 [1:31:43<15:45:57,  1.24s/it]  9%|▊         | 4279/50000 [1:31:45<15:45:51,  1.24s/it]  9%|▊         | 4280/50000 [1:31:46<15:45:27,  1.24s/it]                                                         {'loss': 142.3469, 'learning_rate': 1.993566447175831e-05, 'epoch': 0.66}
  9%|▊         | 4280/50000 [1:31:46<15:45:27,  1.24s/it]  9%|▊         | 4281/50000 [1:31:47<15:45:36,  1.24s/it]  9%|▊         | 4282/50000 [1:31:48<15:45:38,  1.24s/it]  9%|▊         | 4283/50000 [1:31:50<15:45:46,  1.24s/it]  9%|▊         | 4284/50000 [1:31:51<15:45:40,  1.24s/it]  9%|▊         | 4285/50000 [1:31:52<15:45:39,  1.24s/it]  9%|▊         | 4286/50000 [1:31:53<15:45:33,  1.24s/it]  9%|▊         | 4287/50000 [1:31:55<15:45:25,  1.24s/it]  9%|▊         | 4288/50000 [1:31:56<15:45:38,  1.24s/it]  9%|▊         | 4289/50000 [1:31:57<15:45:47,  1.24s/it]  9%|▊         | 4290/50000 [1:31:58<15:45:40,  1.24s/it]                                                         {'loss': 143.5344, 'learning_rate': 1.9934913272432518e-05, 'epoch': 0.67}
  9%|▊         | 4290/50000 [1:31:58<15:45:40,  1.24s/it]  9%|▊         | 4291/50000 [1:31:59<15:46:19,  1.24s/it]  9%|▊         | 4292/50000 [1:32:01<15:46:19,  1.24s/it]  9%|▊         | 4293/50000 [1:32:02<15:55:56,  1.25s/it]  9%|▊         | 4294/50000 [1:32:03<15:52:48,  1.25s/it]  9%|▊         | 4295/50000 [1:32:04<15:52:13,  1.25s/it]  9%|▊         | 4296/50000 [1:32:06<15:51:02,  1.25s/it]  9%|▊         | 4297/50000 [1:32:07<15:50:34,  1.25s/it]  9%|▊         | 4298/50000 [1:32:08<15:49:08,  1.25s/it]  9%|▊         | 4299/50000 [1:32:09<15:48:11,  1.24s/it]  9%|▊         | 4300/50000 [1:32:11<15:47:42,  1.24s/it]                                                         {'loss': 136.0437, 'learning_rate': 1.9934157727238133e-05, 'epoch': 0.67}
  9%|▊         | 4300/50000 [1:32:11<15:47:42,  1.24s/it]  9%|▊         | 4301/50000 [1:32:12<15:47:16,  1.24s/it]  9%|▊         | 4302/50000 [1:32:13<15:46:28,  1.24s/it]  9%|▊         | 4303/50000 [1:32:14<15:46:22,  1.24s/it]  9%|▊         | 4304/50000 [1:32:16<15:45:49,  1.24s/it]  9%|▊         | 4305/50000 [1:32:17<15:45:27,  1.24s/it]  9%|▊         | 4306/50000 [1:32:18<15:45:06,  1.24s/it]  9%|▊         | 4307/50000 [1:32:19<15:44:50,  1.24s/it]  9%|▊         | 4308/50000 [1:32:21<15:44:24,  1.24s/it]  9%|▊         | 4309/50000 [1:32:22<15:43:51,  1.24s/it]  9%|▊         | 4310/50000 [1:32:23<15:43:40,  1.24s/it]                                                         {'loss': 136.1797, 'learning_rate': 1.993339783650565e-05, 'epoch': 0.67}
  9%|▊         | 4310/50000 [1:32:23<15:43:40,  1.24s/it]  9%|▊         | 4311/50000 [1:32:24<15:44:23,  1.24s/it]  9%|▊         | 4312/50000 [1:32:26<15:43:58,  1.24s/it]  9%|▊         | 4313/50000 [1:32:27<15:43:51,  1.24s/it]  9%|▊         | 4314/50000 [1:32:28<15:43:27,  1.24s/it]  9%|▊         | 4315/50000 [1:32:29<15:43:11,  1.24s/it]  9%|▊         | 4316/50000 [1:32:31<15:43:24,  1.24s/it]  9%|▊         | 4317/50000 [1:32:32<15:43:32,  1.24s/it]  9%|▊         | 4318/50000 [1:32:33<15:43:55,  1.24s/it]  9%|▊         | 4319/50000 [1:32:34<15:43:50,  1.24s/it]  9%|▊         | 4320/50000 [1:32:36<15:43:39,  1.24s/it]                                                         {'loss': 159.4906, 'learning_rate': 1.9932633600567474e-05, 'epoch': 0.67}
  9%|▊         | 4320/50000 [1:32:36<15:43:39,  1.24s/it]  9%|▊         | 4321/50000 [1:32:37<15:43:39,  1.24s/it]  9%|▊         | 4322/50000 [1:32:38<15:43:36,  1.24s/it]  9%|▊         | 4323/50000 [1:32:39<15:43:34,  1.24s/it]  9%|▊         | 4324/50000 [1:32:40<15:43:59,  1.24s/it]  9%|▊         | 4325/50000 [1:32:42<15:43:57,  1.24s/it]  9%|▊         | 4326/50000 [1:32:43<15:44:07,  1.24s/it]  9%|▊         | 4327/50000 [1:32:44<15:44:02,  1.24s/it]  9%|▊         | 4328/50000 [1:32:45<15:44:04,  1.24s/it]  9%|▊         | 4329/50000 [1:32:47<15:43:57,  1.24s/it]  9%|▊         | 4330/50000 [1:32:48<15:44:42,  1.24s/it]                                                         {'loss': 148.4875, 'learning_rate': 1.993186501975791e-05, 'epoch': 0.67}
  9%|▊         | 4330/50000 [1:32:48<15:44:42,  1.24s/it]  9%|▊         | 4331/50000 [1:32:49<15:44:32,  1.24s/it]  9%|▊         | 4332/50000 [1:32:50<15:44:17,  1.24s/it]  9%|▊         | 4333/50000 [1:32:52<15:44:35,  1.24s/it]  9%|▊         | 4334/50000 [1:32:53<15:44:16,  1.24s/it]  9%|▊         | 4335/50000 [1:32:54<15:44:41,  1.24s/it]  9%|▊         | 4336/50000 [1:32:55<15:44:35,  1.24s/it]  9%|▊         | 4337/50000 [1:32:57<15:44:17,  1.24s/it]  9%|▊         | 4338/50000 [1:32:58<15:44:18,  1.24s/it]  9%|▊         | 4339/50000 [1:32:59<15:44:07,  1.24s/it]  9%|▊         | 4340/50000 [1:33:00<15:43:38,  1.24s/it]                                                         {'loss': 141.0266, 'learning_rate': 1.993109209441316e-05, 'epoch': 0.67}
  9%|▊         | 4340/50000 [1:33:00<15:43:38,  1.24s/it]  9%|▊         | 4341/50000 [1:33:02<15:43:39,  1.24s/it]  9%|▊         | 4342/50000 [1:33:03<15:44:56,  1.24s/it]  9%|▊         | 4343/50000 [1:33:04<16:00:24,  1.26s/it]  9%|▊         | 4344/50000 [1:33:05<15:55:56,  1.26s/it]  9%|▊         | 4345/50000 [1:33:07<15:53:27,  1.25s/it]  9%|▊         | 4346/50000 [1:33:08<15:50:46,  1.25s/it]  9%|▊         | 4347/50000 [1:33:09<15:49:59,  1.25s/it]  9%|▊         | 4348/50000 [1:33:10<15:49:03,  1.25s/it][2023-07-03 13:33:21,929] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, but hysteresis is 2. Reducing hysteresis to 1
  9%|▊         | 4349/50000 [1:33:11<15:10:27,  1.20s/it]  9%|▊         | 4350/50000 [1:33:13<15:25:31,  1.22s/it]                                                         {'loss': 160.0672, 'learning_rate': 1.9930392747304704e-05, 'epoch': 0.67}
  9%|▊         | 4350/50000 [1:33:13<15:25:31,  1.22s/it]  9%|▊         | 4351/50000 [1:33:14<15:35:36,  1.23s/it]  9%|▊         | 4352/50000 [1:33:15<15:39:35,  1.23s/it]  9%|▊         | 4353/50000 [1:33:16<15:42:50,  1.24s/it]  9%|▊         | 4354/50000 [1:33:18<15:45:13,  1.24s/it]  9%|▊         | 4355/50000 [1:33:19<15:45:52,  1.24s/it]  9%|▊         | 4356/50000 [1:33:20<15:46:43,  1.24s/it]  9%|▊         | 4357/50000 [1:33:21<15:47:07,  1.25s/it]  9%|▊         | 4358/50000 [1:33:23<15:48:33,  1.25s/it]  9%|▊         | 4359/50000 [1:33:24<15:47:47,  1.25s/it]  9%|▊         | 4360/50000 [1:33:25<15:47:33,  1.25s/it]                                                         {'loss': 134.3734, 'learning_rate': 1.9929611568276146e-05, 'epoch': 0.68}
  9%|▊         | 4360/50000 [1:33:25<15:47:33,  1.25s/it]  9%|▊         | 4361/50000 [1:33:26<15:47:29,  1.25s/it]  9%|▊         | 4362/50000 [1:33:28<15:46:59,  1.25s/it]  9%|▊         | 4363/50000 [1:33:29<15:46:12,  1.24s/it]  9%|▊         | 4364/50000 [1:33:30<15:45:39,  1.24s/it]  9%|▊         | 4365/50000 [1:33:31<15:45:57,  1.24s/it]  9%|▊         | 4366/50000 [1:33:33<15:45:06,  1.24s/it]  9%|▊         | 4367/50000 [1:33:34<15:44:56,  1.24s/it]  9%|▊         | 4368/50000 [1:33:35<15:44:19,  1.24s/it]  9%|▊         | 4369/50000 [1:33:36<15:44:18,  1.24s/it]  9%|▊         | 4370/50000 [1:33:38<15:44:03,  1.24s/it]                                                         {'loss': 155.1766, 'learning_rate': 1.9928826045698138e-05, 'epoch': 0.68}
  9%|▊         | 4370/50000 [1:33:38<15:44:03,  1.24s/it]  9%|▊         | 4371/50000 [1:33:39<15:44:00,  1.24s/it]  9%|▊         | 4372/50000 [1:33:40<15:44:19,  1.24s/it]  9%|▊         | 4373/50000 [1:33:41<15:44:08,  1.24s/it]  9%|▊         | 4374/50000 [1:33:43<15:44:13,  1.24s/it]  9%|▉         | 4375/50000 [1:33:44<15:44:10,  1.24s/it]  9%|▉         | 4376/50000 [1:33:45<15:44:04,  1.24s/it]  9%|▉         | 4377/50000 [1:33:46<15:43:46,  1.24s/it]  9%|▉         | 4378/50000 [1:33:48<15:43:29,  1.24s/it]  9%|▉         | 4379/50000 [1:33:49<15:43:48,  1.24s/it]  9%|▉         | 4380/50000 [1:33:50<15:44:18,  1.24s/it]                                                         {'loss': 168.0219, 'learning_rate': 1.9928036179914296e-05, 'epoch': 0.68}
  9%|▉         | 4380/50000 [1:33:50<15:44:18,  1.24s/it]  9%|▉         | 4381/50000 [1:33:51<15:44:32,  1.24s/it]  9%|▉         | 4382/50000 [1:33:52<15:44:12,  1.24s/it]  9%|▉         | 4383/50000 [1:33:54<15:43:54,  1.24s/it]  9%|▉         | 4384/50000 [1:33:55<15:43:41,  1.24s/it]  9%|▉         | 4385/50000 [1:33:56<15:43:36,  1.24s/it]  9%|▉         | 4386/50000 [1:33:57<15:43:23,  1.24s/it]  9%|▉         | 4387/50000 [1:33:59<15:43:16,  1.24s/it]  9%|▉         | 4388/50000 [1:34:00<15:43:22,  1.24s/it]  9%|▉         | 4389/50000 [1:34:01<15:43:30,  1.24s/it]  9%|▉         | 4390/50000 [1:34:02<15:43:14,  1.24s/it]                                                         {'loss': 161.6984, 'learning_rate': 1.992724197127013e-05, 'epoch': 0.68}
  9%|▉         | 4390/50000 [1:34:02<15:43:14,  1.24s/it]  9%|▉         | 4391/50000 [1:34:04<15:43:25,  1.24s/it]  9%|▉         | 4392/50000 [1:34:05<15:43:28,  1.24s/it]  9%|▉         | 4393/50000 [1:34:06<15:43:14,  1.24s/it]  9%|▉         | 4394/50000 [1:34:07<15:43:03,  1.24s/it]  9%|▉         | 4395/50000 [1:34:09<15:42:49,  1.24s/it]  9%|▉         | 4396/50000 [1:34:10<15:42:55,  1.24s/it]  9%|▉         | 4397/50000 [1:34:11<15:42:57,  1.24s/it]  9%|▉         | 4398/50000 [1:34:12<15:43:10,  1.24s/it]  9%|▉         | 4399/50000 [1:34:14<15:42:55,  1.24s/it]  9%|▉         | 4400/50000 [1:34:15<15:42:54,  1.24s/it]                                                         {'loss': 163.1125, 'learning_rate': 1.992644342011306e-05, 'epoch': 0.68}
  9%|▉         | 4400/50000 [1:34:15<15:42:54,  1.24s/it]  9%|▉         | 4401/50000 [1:34:16<15:43:10,  1.24s/it]  9%|▉         | 4402/50000 [1:34:17<15:43:09,  1.24s/it]  9%|▉         | 4403/50000 [1:34:19<15:42:52,  1.24s/it]  9%|▉         | 4404/50000 [1:34:20<15:42:48,  1.24s/it]  9%|▉         | 4405/50000 [1:34:21<15:42:50,  1.24s/it]  9%|▉         | 4406/50000 [1:34:22<15:42:49,  1.24s/it]  9%|▉         | 4407/50000 [1:34:24<15:42:41,  1.24s/it]  9%|▉         | 4408/50000 [1:34:25<15:42:39,  1.24s/it]  9%|▉         | 4409/50000 [1:34:26<15:42:29,  1.24s/it]  9%|▉         | 4410/50000 [1:34:27<15:42:21,  1.24s/it]                                                         {'loss': 156.7359, 'learning_rate': 1.9925640526792393e-05, 'epoch': 0.68}
  9%|▉         | 4410/50000 [1:34:27<15:42:21,  1.24s/it]  9%|▉         | 4411/50000 [1:34:28<15:42:40,  1.24s/it]  9%|▉         | 4412/50000 [1:34:30<15:42:22,  1.24s/it]  9%|▉         | 4413/50000 [1:34:31<15:42:20,  1.24s/it]  9%|▉         | 4414/50000 [1:34:32<15:42:48,  1.24s/it]  9%|▉         | 4415/50000 [1:34:33<15:42:33,  1.24s/it]  9%|▉         | 4416/50000 [1:34:35<15:42:15,  1.24s/it]  9%|▉         | 4417/50000 [1:34:36<15:42:13,  1.24s/it]  9%|▉         | 4418/50000 [1:34:37<15:42:16,  1.24s/it]  9%|▉         | 4419/50000 [1:34:38<15:41:45,  1.24s/it]  9%|▉         | 4420/50000 [1:34:40<15:41:45,  1.24s/it]                                                         {'loss': 149.6672, 'learning_rate': 1.9924833291659346e-05, 'epoch': 0.69}
  9%|▉         | 4420/50000 [1:34:40<15:41:45,  1.24s/it]  9%|▉         | 4421/50000 [1:34:41<15:42:08,  1.24s/it]  9%|▉         | 4422/50000 [1:34:42<15:42:25,  1.24s/it]  9%|▉         | 4423/50000 [1:34:43<15:42:23,  1.24s/it]  9%|▉         | 4424/50000 [1:34:45<15:42:02,  1.24s/it]  9%|▉         | 4425/50000 [1:34:46<15:41:37,  1.24s/it]  9%|▉         | 4426/50000 [1:34:47<15:41:32,  1.24s/it]  9%|▉         | 4427/50000 [1:34:48<15:41:39,  1.24s/it]  9%|▉         | 4428/50000 [1:34:50<15:41:48,  1.24s/it]  9%|▉         | 4429/50000 [1:34:51<15:41:20,  1.24s/it]  9%|▉         | 4430/50000 [1:34:52<15:41:04,  1.24s/it]                                                         {'loss': 181.8906, 'learning_rate': 1.992402171506703e-05, 'epoch': 0.69}
  9%|▉         | 4430/50000 [1:34:52<15:41:04,  1.24s/it]  9%|▉         | 4431/50000 [1:34:53<15:40:59,  1.24s/it]  9%|▉         | 4432/50000 [1:34:55<15:40:57,  1.24s/it]  9%|▉         | 4433/50000 [1:34:56<15:39:47,  1.24s/it]  9%|▉         | 4434/50000 [1:34:57<15:39:24,  1.24s/it]  9%|▉         | 4435/50000 [1:34:58<15:39:47,  1.24s/it]  9%|▉         | 4436/50000 [1:34:59<15:39:17,  1.24s/it]  9%|▉         | 4437/50000 [1:35:01<15:39:11,  1.24s/it]  9%|▉         | 4438/50000 [1:35:02<15:39:40,  1.24s/it]  9%|▉         | 4439/50000 [1:35:03<15:40:16,  1.24s/it]  9%|▉         | 4440/50000 [1:35:04<15:41:24,  1.24s/it]                                                         {'loss': 149.7859, 'learning_rate': 1.992320579737045e-05, 'epoch': 0.69}
  9%|▉         | 4440/50000 [1:35:04<15:41:24,  1.24s/it]  9%|▉         | 4441/50000 [1:35:06<15:41:28,  1.24s/it]  9%|▉         | 4442/50000 [1:35:07<15:42:19,  1.24s/it]  9%|▉         | 4443/50000 [1:35:08<15:49:15,  1.25s/it]  9%|▉         | 4444/50000 [1:35:09<15:46:55,  1.25s/it]  9%|▉         | 4445/50000 [1:35:11<15:45:13,  1.24s/it]  9%|▉         | 4446/50000 [1:35:12<15:44:36,  1.24s/it]  9%|▉         | 4447/50000 [1:35:13<15:43:44,  1.24s/it]  9%|▉         | 4448/50000 [1:35:14<15:43:38,  1.24s/it]  9%|▉         | 4449/50000 [1:35:16<15:44:06,  1.24s/it]  9%|▉         | 4450/50000 [1:35:17<15:43:24,  1.24s/it]                                                         {'loss': 158.9, 'learning_rate': 1.992238553892653e-05, 'epoch': 0.69}
  9%|▉         | 4450/50000 [1:35:17<15:43:24,  1.24s/it]  9%|▉         | 4451/50000 [1:35:18<15:44:15,  1.24s/it]  9%|▉         | 4452/50000 [1:35:19<15:43:32,  1.24s/it]  9%|▉         | 4453/50000 [1:35:21<15:42:51,  1.24s/it]  9%|▉         | 4454/50000 [1:35:22<15:41:49,  1.24s/it]  9%|▉         | 4455/50000 [1:35:23<15:41:03,  1.24s/it]  9%|▉         | 4456/50000 [1:35:24<15:44:21,  1.24s/it]  9%|▉         | 4457/50000 [1:35:26<15:43:09,  1.24s/it]  9%|▉         | 4458/50000 [1:35:27<15:43:09,  1.24s/it]  9%|▉         | 4459/50000 [1:35:28<15:42:03,  1.24s/it]  9%|▉         | 4460/50000 [1:35:29<15:41:32,  1.24s/it]                                                         {'loss': 134.7984, 'learning_rate': 1.9921560940094068e-05, 'epoch': 0.69}
  9%|▉         | 4460/50000 [1:35:29<15:41:32,  1.24s/it]  9%|▉         | 4461/50000 [1:35:31<15:41:36,  1.24s/it]  9%|▉         | 4462/50000 [1:35:32<15:41:18,  1.24s/it]  9%|▉         | 4463/50000 [1:35:33<15:40:41,  1.24s/it]  9%|▉         | 4464/50000 [1:35:34<15:40:19,  1.24s/it]  9%|▉         | 4465/50000 [1:35:35<15:40:00,  1.24s/it]  9%|▉         | 4466/50000 [1:35:37<15:39:52,  1.24s/it]  9%|▉         | 4467/50000 [1:35:38<15:40:00,  1.24s/it]  9%|▉         | 4468/50000 [1:35:39<15:39:48,  1.24s/it]  9%|▉         | 4469/50000 [1:35:40<15:39:54,  1.24s/it]  9%|▉         | 4470/50000 [1:35:42<15:39:43,  1.24s/it]                                                         {'loss': 158.225, 'learning_rate': 1.9920732001233773e-05, 'epoch': 0.69}
  9%|▉         | 4470/50000 [1:35:42<15:39:43,  1.24s/it]  9%|▉         | 4471/50000 [1:35:43<15:40:15,  1.24s/it]  9%|▉         | 4472/50000 [1:35:44<15:40:06,  1.24s/it]  9%|▉         | 4473/50000 [1:35:45<15:39:51,  1.24s/it]  9%|▉         | 4474/50000 [1:35:47<15:39:39,  1.24s/it]  9%|▉         | 4475/50000 [1:35:48<15:39:33,  1.24s/it]  9%|▉         | 4476/50000 [1:35:49<15:39:40,  1.24s/it]  9%|▉         | 4477/50000 [1:35:50<15:40:03,  1.24s/it]  9%|▉         | 4478/50000 [1:35:52<15:40:24,  1.24s/it]  9%|▉         | 4479/50000 [1:35:53<15:40:18,  1.24s/it]  9%|▉         | 4480/50000 [1:35:54<15:40:18,  1.24s/it]                                                         {'loss': 154.3125, 'learning_rate': 1.9919898722708253e-05, 'epoch': 0.69}
  9%|▉         | 4480/50000 [1:35:54<15:40:18,  1.24s/it]  9%|▉         | 4481/50000 [1:35:55<15:40:24,  1.24s/it]  9%|▉         | 4482/50000 [1:35:57<15:40:05,  1.24s/it]  9%|▉         | 4483/50000 [1:35:58<15:39:45,  1.24s/it]  9%|▉         | 4484/50000 [1:35:59<15:39:43,  1.24s/it]  9%|▉         | 4485/50000 [1:36:00<15:40:11,  1.24s/it]  9%|▉         | 4486/50000 [1:36:01<15:40:05,  1.24s/it]  9%|▉         | 4487/50000 [1:36:03<15:40:07,  1.24s/it]  9%|▉         | 4488/50000 [1:36:04<15:40:51,  1.24s/it]  9%|▉         | 4489/50000 [1:36:05<15:41:23,  1.24s/it]  9%|▉         | 4490/50000 [1:36:06<15:41:29,  1.24s/it]                                                         {'loss': 146.6266, 'learning_rate': 1.9919061104882012e-05, 'epoch': 0.7}
  9%|▉         | 4490/50000 [1:36:06<15:41:29,  1.24s/it]  9%|▉         | 4491/50000 [1:36:08<15:41:53,  1.24s/it]  9%|▉         | 4492/50000 [1:36:09<15:41:55,  1.24s/it]  9%|▉         | 4493/50000 [1:36:10<15:42:11,  1.24s/it]  9%|▉         | 4494/50000 [1:36:11<15:42:01,  1.24s/it]  9%|▉         | 4495/50000 [1:36:13<15:41:56,  1.24s/it]  9%|▉         | 4496/50000 [1:36:14<15:42:07,  1.24s/it]  9%|▉         | 4497/50000 [1:36:15<15:41:57,  1.24s/it]  9%|▉         | 4498/50000 [1:36:16<15:41:54,  1.24s/it]  9%|▉         | 4499/50000 [1:36:18<15:41:46,  1.24s/it]  9%|▉         | 4500/50000 [1:36:19<15:42:12,  1.24s/it]                                                         {'loss': 141.0109, 'learning_rate': 1.991821914812145e-05, 'epoch': 0.7}
  9%|▉         | 4500/50000 [1:36:19<15:42:12,  1.24s/it]  9%|▉         | 4501/50000 [1:36:20<15:42:11,  1.24s/it]  9%|▉         | 4502/50000 [1:36:21<15:41:56,  1.24s/it][2023-07-03 13:36:32,860] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, but hysteresis is 2. Reducing hysteresis to 1
  9%|▉         | 4503/50000 [1:36:22<14:45:15,  1.17s/it]  9%|▉         | 4504/50000 [1:36:24<15:02:26,  1.19s/it]  9%|▉         | 4505/50000 [1:36:25<15:14:00,  1.21s/it]  9%|▉         | 4506/50000 [1:36:26<15:22:12,  1.22s/it]  9%|▉         | 4507/50000 [1:36:27<15:27:59,  1.22s/it]  9%|▉         | 4508/50000 [1:36:29<15:33:04,  1.23s/it]  9%|▉         | 4509/50000 [1:36:30<15:35:25,  1.23s/it]  9%|▉         | 4510/50000 [1:36:31<15:37:44,  1.24s/it]                                                         {'loss': 143.3266, 'learning_rate': 1.991745767755247e-05, 'epoch': 0.7}
  9%|▉         | 4510/50000 [1:36:31<15:37:44,  1.24s/it]  9%|▉         | 4511/50000 [1:36:32<15:38:54,  1.24s/it]  9%|▉         | 4512/50000 [1:36:34<15:39:29,  1.24s/it]  9%|▉         | 4513/50000 [1:36:35<15:40:04,  1.24s/it]  9%|▉         | 4514/50000 [1:36:36<15:40:31,  1.24s/it]  9%|▉         | 4515/50000 [1:36:37<15:40:49,  1.24s/it]  9%|▉         | 4516/50000 [1:36:38<15:40:57,  1.24s/it]  9%|▉         | 4517/50000 [1:36:40<15:41:00,  1.24s/it]  9%|▉         | 4518/50000 [1:36:41<15:41:04,  1.24s/it]  9%|▉         | 4519/50000 [1:36:42<15:41:09,  1.24s/it]  9%|▉         | 4520/50000 [1:36:43<15:41:05,  1.24s/it]                                                         {'loss': 155.3891, 'learning_rate': 1.9916607477832937e-05, 'epoch': 0.7}
  9%|▉         | 4520/50000 [1:36:43<15:41:05,  1.24s/it]  9%|▉         | 4521/50000 [1:36:45<15:41:06,  1.24s/it]  9%|▉         | 4522/50000 [1:36:46<15:41:02,  1.24s/it]  9%|▉         | 4523/50000 [1:36:47<15:41:06,  1.24s/it]  9%|▉         | 4524/50000 [1:36:48<15:41:09,  1.24s/it]  9%|▉         | 4525/50000 [1:36:50<15:40:39,  1.24s/it]  9%|▉         | 4526/50000 [1:36:51<15:40:57,  1.24s/it]  9%|▉         | 4527/50000 [1:36:52<15:40:23,  1.24s/it]  9%|▉         | 4528/50000 [1:36:53<15:40:07,  1.24s/it]  9%|▉         | 4529/50000 [1:36:55<15:40:09,  1.24s/it]  9%|▉         | 4530/50000 [1:36:56<15:40:03,  1.24s/it]                                                         {'loss': 167.7063, 'learning_rate': 1.991575294025239e-05, 'epoch': 0.7}
  9%|▉         | 4530/50000 [1:36:56<15:40:03,  1.24s/it]  9%|▉         | 4531/50000 [1:36:57<15:40:20,  1.24s/it]  9%|▉         | 4532/50000 [1:36:58<15:40:33,  1.24s/it]  9%|▉         | 4533/50000 [1:37:00<15:41:07,  1.24s/it]  9%|▉         | 4534/50000 [1:37:01<15:52:28,  1.26s/it]  9%|▉         | 4535/50000 [1:37:02<15:49:56,  1.25s/it]  9%|▉         | 4536/50000 [1:37:03<15:48:53,  1.25s/it]  9%|▉         | 4537/50000 [1:37:05<15:49:57,  1.25s/it]  9%|▉         | 4538/50000 [1:37:06<15:47:28,  1.25s/it]  9%|▉         | 4539/50000 [1:37:07<15:46:11,  1.25s/it]  9%|▉         | 4540/50000 [1:37:08<15:44:18,  1.25s/it]                                                         {'loss': 148.2266, 'learning_rate': 1.9914894065184628e-05, 'epoch': 0.7}
  9%|▉         | 4540/50000 [1:37:08<15:44:18,  1.25s/it]  9%|▉         | 4541/50000 [1:37:10<15:43:24,  1.25s/it]  9%|▉         | 4542/50000 [1:37:11<15:42:50,  1.24s/it]  9%|▉         | 4543/50000 [1:37:12<15:42:16,  1.24s/it]  9%|▉         | 4544/50000 [1:37:13<15:41:45,  1.24s/it]  9%|▉         | 4545/50000 [1:37:15<15:41:30,  1.24s/it]  9%|▉         | 4546/50000 [1:37:16<15:41:44,  1.24s/it]  9%|▉         | 4547/50000 [1:37:17<15:41:24,  1.24s/it]  9%|▉         | 4548/50000 [1:37:18<15:41:15,  1.24s/it]  9%|▉         | 4549/50000 [1:37:20<15:42:03,  1.24s/it]  9%|▉         | 4550/50000 [1:37:21<15:41:32,  1.24s/it]                                                         {'loss': 147.8562, 'learning_rate': 1.991403085300535e-05, 'epoch': 0.71}
  9%|▉         | 4550/50000 [1:37:21<15:41:32,  1.24s/it]  9%|▉         | 4551/50000 [1:37:22<15:41:26,  1.24s/it]  9%|▉         | 4552/50000 [1:37:23<15:41:00,  1.24s/it]  9%|▉         | 4553/50000 [1:37:25<15:41:31,  1.24s/it]  9%|▉         | 4554/50000 [1:37:26<15:41:48,  1.24s/it]  9%|▉         | 4555/50000 [1:37:27<15:41:42,  1.24s/it]  9%|▉         | 4556/50000 [1:37:28<15:41:40,  1.24s/it]  9%|▉         | 4557/50000 [1:37:30<15:41:57,  1.24s/it]  9%|▉         | 4558/50000 [1:37:31<15:42:32,  1.24s/it]  9%|▉         | 4559/50000 [1:37:32<15:42:15,  1.24s/it]  9%|▉         | 4560/50000 [1:37:33<15:41:50,  1.24s/it]                                                         {'loss': 168.0391, 'learning_rate': 1.9913163304092156e-05, 'epoch': 0.71}
  9%|▉         | 4560/50000 [1:37:33<15:41:50,  1.24s/it]  9%|▉         | 4561/50000 [1:37:34<15:41:39,  1.24s/it]  9%|▉         | 4562/50000 [1:37:36<15:41:13,  1.24s/it]  9%|▉         | 4563/50000 [1:37:37<15:41:00,  1.24s/it]  9%|▉         | 4564/50000 [1:37:38<15:40:56,  1.24s/it][2023-07-03 13:37:49,708] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
  9%|▉         | 4565/50000 [1:37:39<14:43:55,  1.17s/it]  9%|▉         | 4566/50000 [1:37:40<15:01:28,  1.19s/it]  9%|▉         | 4567/50000 [1:37:42<15:13:36,  1.21s/it]  9%|▉         | 4568/50000 [1:37:43<15:21:38,  1.22s/it]  9%|▉         | 4569/50000 [1:37:44<15:27:19,  1.22s/it]  9%|▉         | 4570/50000 [1:37:45<15:31:29,  1.23s/it]                                                         {'loss': 182.9297, 'learning_rate': 1.99123788024764e-05, 'epoch': 0.71}
  9%|▉         | 4570/50000 [1:37:45<15:31:29,  1.23s/it]  9%|▉         | 4571/50000 [1:37:47<15:34:26,  1.23s/it]  9%|▉         | 4572/50000 [1:37:48<15:36:57,  1.24s/it]  9%|▉         | 4573/50000 [1:37:49<15:38:08,  1.24s/it]  9%|▉         | 4574/50000 [1:37:50<15:39:07,  1.24s/it]  9%|▉         | 4575/50000 [1:37:52<15:39:44,  1.24s/it]  9%|▉         | 4576/50000 [1:37:53<15:40:27,  1.24s/it]  9%|▉         | 4577/50000 [1:37:54<15:40:09,  1.24s/it]  9%|▉         | 4578/50000 [1:37:55<15:39:49,  1.24s/it]  9%|▉         | 4579/50000 [1:37:57<15:39:52,  1.24s/it]  9%|▉         | 4580/50000 [1:37:58<15:39:54,  1.24s/it]                                                         {'loss': 166.5141, 'learning_rate': 1.9911503014815845e-05, 'epoch': 0.71}
  9%|▉         | 4580/50000 [1:37:58<15:39:54,  1.24s/it]  9%|▉         | 4581/50000 [1:37:59<15:39:33,  1.24s/it]  9%|▉         | 4582/50000 [1:38:00<15:39:17,  1.24s/it]  9%|▉         | 4583/50000 [1:38:02<15:39:26,  1.24s/it]  9%|▉         | 4584/50000 [1:38:03<15:40:04,  1.24s/it]  9%|▉         | 4585/50000 [1:38:04<15:39:40,  1.24s/it]  9%|▉         | 4586/50000 [1:38:05<15:39:39,  1.24s/it]  9%|▉         | 4587/50000 [1:38:07<15:39:24,  1.24s/it]  9%|▉         | 4588/50000 [1:38:08<15:39:14,  1.24s/it]  9%|▉         | 4589/50000 [1:38:09<15:39:07,  1.24s/it]  9%|▉         | 4590/50000 [1:38:10<15:38:56,  1.24s/it]                                                         {'loss': 158.7375, 'learning_rate': 1.991062289152714e-05, 'epoch': 0.71}
  9%|▉         | 4590/50000 [1:38:10<15:38:56,  1.24s/it]  9%|▉         | 4591/50000 [1:38:11<15:39:27,  1.24s/it]  9%|▉         | 4592/50000 [1:38:13<15:39:24,  1.24s/it]  9%|▉         | 4593/50000 [1:38:14<15:39:18,  1.24s/it]  9%|▉         | 4594/50000 [1:38:15<15:39:28,  1.24s/it]  9%|▉         | 4595/50000 [1:38:16<15:39:18,  1.24s/it]  9%|▉         | 4596/50000 [1:38:18<15:38:41,  1.24s/it]  9%|▉         | 4597/50000 [1:38:19<15:38:41,  1.24s/it]  9%|▉         | 4598/50000 [1:38:20<15:38:58,  1.24s/it]  9%|▉         | 4599/50000 [1:38:21<15:39:27,  1.24s/it]  9%|▉         | 4600/50000 [1:38:23<15:39:35,  1.24s/it]                                                         {'loss': 270.1469, 'learning_rate': 1.990973843299527e-05, 'epoch': 0.71}
  9%|▉         | 4600/50000 [1:38:23<15:39:35,  1.24s/it]  9%|▉         | 4601/50000 [1:38:24<15:39:36,  1.24s/it]  9%|▉         | 4602/50000 [1:38:25<15:39:56,  1.24s/it]  9%|▉         | 4603/50000 [1:38:26<15:39:16,  1.24s/it]  9%|▉         | 4604/50000 [1:38:28<15:39:06,  1.24s/it]  9%|▉         | 4605/50000 [1:38:29<15:39:19,  1.24s/it]  9%|▉         | 4606/50000 [1:38:30<15:39:20,  1.24s/it]  9%|▉         | 4607/50000 [1:38:31<15:39:23,  1.24s/it]  9%|▉         | 4608/50000 [1:38:33<15:39:12,  1.24s/it]  9%|▉         | 4609/50000 [1:38:34<15:39:08,  1.24s/it]  9%|▉         | 4610/50000 [1:38:35<15:39:16,  1.24s/it]                                                         {'loss': 156.9141, 'learning_rate': 1.990884963960714e-05, 'epoch': 0.71}
  9%|▉         | 4610/50000 [1:38:35<15:39:16,  1.24s/it]  9%|▉         | 4611/50000 [1:38:36<15:39:22,  1.24s/it]  9%|▉         | 4612/50000 [1:38:38<15:39:21,  1.24s/it]  9%|▉         | 4613/50000 [1:38:39<15:39:16,  1.24s/it]  9%|▉         | 4614/50000 [1:38:40<15:38:54,  1.24s/it]  9%|▉         | 4615/50000 [1:38:41<15:46:58,  1.25s/it]  9%|▉         | 4616/50000 [1:38:43<16:08:24,  1.28s/it]  9%|▉         | 4617/50000 [1:38:44<16:01:39,  1.27s/it]  9%|▉         | 4618/50000 [1:38:45<15:56:13,  1.26s/it]  9%|▉         | 4619/50000 [1:38:46<15:51:19,  1.26s/it]  9%|▉         | 4620/50000 [1:38:48<15:47:50,  1.25s/it]                                                         {'loss': 133.5219, 'learning_rate': 1.9907956511751533e-05, 'epoch': 0.72}
  9%|▉         | 4620/50000 [1:38:48<15:47:50,  1.25s/it]  9%|▉         | 4621/50000 [1:38:49<15:45:43,  1.25s/it]  9%|▉         | 4622/50000 [1:38:50<15:44:11,  1.25s/it]  9%|▉         | 4623/50000 [1:38:51<15:43:39,  1.25s/it]  9%|▉         | 4624/50000 [1:38:53<15:44:01,  1.25s/it]  9%|▉         | 4625/50000 [1:38:54<15:43:05,  1.25s/it]  9%|▉         | 4626/50000 [1:38:55<15:43:01,  1.25s/it]  9%|▉         | 4627/50000 [1:38:56<15:42:20,  1.25s/it]  9%|▉         | 4628/50000 [1:38:58<15:42:43,  1.25s/it]  9%|▉         | 4629/50000 [1:38:59<15:41:23,  1.24s/it]  9%|▉         | 4630/50000 [1:39:00<15:40:26,  1.24s/it]                                                         {'loss': 148.0141, 'learning_rate': 1.9907059049819133e-05, 'epoch': 0.72}
  9%|▉         | 4630/50000 [1:39:00<15:40:26,  1.24s/it]  9%|▉         | 4631/50000 [1:39:01<15:40:03,  1.24s/it]  9%|▉         | 4632/50000 [1:39:03<15:39:37,  1.24s/it]  9%|▉         | 4633/50000 [1:39:04<15:42:34,  1.25s/it]  9%|▉         | 4634/50000 [1:39:05<15:42:00,  1.25s/it]  9%|▉         | 4635/50000 [1:39:06<15:41:34,  1.25s/it]  9%|▉         | 4636/50000 [1:39:08<15:41:31,  1.25s/it]  9%|▉         | 4637/50000 [1:39:09<15:40:26,  1.24s/it]  9%|▉         | 4638/50000 [1:39:10<15:39:42,  1.24s/it]  9%|▉         | 4639/50000 [1:39:11<15:39:02,  1.24s/it]  9%|▉         | 4640/50000 [1:39:13<15:38:33,  1.24s/it]                                                         {'loss': 137.5406, 'learning_rate': 1.9906157254202517e-05, 'epoch': 0.72}
  9%|▉         | 4640/50000 [1:39:13<15:38:33,  1.24s/it]  9%|▉         | 4641/50000 [1:39:14<15:38:31,  1.24s/it]  9%|▉         | 4642/50000 [1:39:15<15:38:42,  1.24s/it]  9%|▉         | 4643/50000 [1:39:16<15:38:40,  1.24s/it]  9%|▉         | 4644/50000 [1:39:17<15:38:56,  1.24s/it]  9%|▉         | 4645/50000 [1:39:19<15:38:28,  1.24s/it]  9%|▉         | 4646/50000 [1:39:20<15:38:17,  1.24s/it]  9%|▉         | 4647/50000 [1:39:21<15:38:29,  1.24s/it]  9%|▉         | 4648/50000 [1:39:22<15:38:09,  1.24s/it]  9%|▉         | 4649/50000 [1:39:24<15:37:53,  1.24s/it]  9%|▉         | 4650/50000 [1:39:25<15:37:42,  1.24s/it]                                                         {'loss': 172.9047, 'learning_rate': 1.990525112529617e-05, 'epoch': 0.72}
  9%|▉         | 4650/50000 [1:39:25<15:37:42,  1.24s/it]  9%|▉         | 4651/50000 [1:39:26<15:38:11,  1.24s/it]  9%|▉         | 4652/50000 [1:39:27<15:38:02,  1.24s/it]  9%|▉         | 4653/50000 [1:39:29<15:38:27,  1.24s/it]  9%|▉         | 4654/50000 [1:39:30<15:39:55,  1.24s/it]  9%|▉         | 4655/50000 [1:39:31<15:39:17,  1.24s/it]  9%|▉         | 4656/50000 [1:39:32<15:38:34,  1.24s/it]  9%|▉         | 4657/50000 [1:39:34<15:38:02,  1.24s/it]  9%|▉         | 4658/50000 [1:39:35<15:37:55,  1.24s/it]  9%|▉         | 4659/50000 [1:39:36<15:37:58,  1.24s/it]  9%|▉         | 4660/50000 [1:39:37<15:37:54,  1.24s/it]                                                         {'loss': 152.9766, 'learning_rate': 1.9904340663496457e-05, 'epoch': 0.72}
  9%|▉         | 4660/50000 [1:39:37<15:37:54,  1.24s/it]  9%|▉         | 4661/50000 [1:39:39<15:38:21,  1.24s/it]  9%|▉         | 4662/50000 [1:39:40<15:38:09,  1.24s/it]  9%|▉         | 4663/50000 [1:39:41<15:38:21,  1.24s/it]  9%|▉         | 4664/50000 [1:39:42<15:38:06,  1.24s/it]  9%|▉         | 4665/50000 [1:39:44<15:37:56,  1.24s/it]  9%|▉         | 4666/50000 [1:39:45<15:37:42,  1.24s/it]  9%|▉         | 4667/50000 [1:39:46<15:38:03,  1.24s/it]  9%|▉         | 4668/50000 [1:39:47<15:37:55,  1.24s/it]  9%|▉         | 4669/50000 [1:39:49<15:37:40,  1.24s/it]  9%|▉         | 4670/50000 [1:39:50<15:37:29,  1.24s/it]                                                         {'loss': 140.6117, 'learning_rate': 1.9903425869201648e-05, 'epoch': 0.72}
  9%|▉         | 4670/50000 [1:39:50<15:37:29,  1.24s/it]  9%|▉         | 4671/50000 [1:39:51<15:37:32,  1.24s/it]  9%|▉         | 4672/50000 [1:39:52<15:37:21,  1.24s/it]  9%|▉         | 4673/50000 [1:39:53<15:37:09,  1.24s/it]  9%|▉         | 4674/50000 [1:39:55<15:37:10,  1.24s/it]  9%|▉         | 4675/50000 [1:39:56<15:37:12,  1.24s/it]  9%|▉         | 4676/50000 [1:39:57<15:37:20,  1.24s/it]  9%|▉         | 4677/50000 [1:39:58<15:37:06,  1.24s/it]  9%|▉         | 4678/50000 [1:40:00<15:37:19,  1.24s/it]  9%|▉         | 4679/50000 [1:40:01<15:37:01,  1.24s/it]  9%|▉         | 4680/50000 [1:40:02<15:37:15,  1.24s/it]                                                         {'loss': 155.0641, 'learning_rate': 1.99025067428119e-05, 'epoch': 0.73}
  9%|▉         | 4680/50000 [1:40:02<15:37:15,  1.24s/it]  9%|▉         | 4681/50000 [1:40:03<15:37:29,  1.24s/it]  9%|▉         | 4682/50000 [1:40:05<15:37:22,  1.24s/it]  9%|▉         | 4683/50000 [1:40:06<15:37:02,  1.24s/it]  9%|▉         | 4684/50000 [1:40:07<15:36:43,  1.24s/it]  9%|▉         | 4685/50000 [1:40:08<15:36:19,  1.24s/it]  9%|▉         | 4686/50000 [1:40:10<15:36:29,  1.24s/it]  9%|▉         | 4687/50000 [1:40:11<15:36:31,  1.24s/it]  9%|▉         | 4688/50000 [1:40:12<15:36:56,  1.24s/it]  9%|▉         | 4689/50000 [1:40:13<15:37:06,  1.24s/it]  9%|▉         | 4690/50000 [1:40:15<15:38:20,  1.24s/it]                                                         {'loss': 150.8562, 'learning_rate': 1.9901583284729274e-05, 'epoch': 0.73}
  9%|▉         | 4690/50000 [1:40:15<15:38:20,  1.24s/it]  9%|▉         | 4691/50000 [1:40:16<15:38:30,  1.24s/it]  9%|▉         | 4692/50000 [1:40:17<15:38:11,  1.24s/it]  9%|▉         | 4693/50000 [1:40:18<15:38:22,  1.24s/it]  9%|▉         | 4694/50000 [1:40:20<15:39:23,  1.24s/it]  9%|▉         | 4695/50000 [1:40:21<15:39:15,  1.24s/it]  9%|▉         | 4696/50000 [1:40:22<15:39:04,  1.24s/it]  9%|▉         | 4697/50000 [1:40:23<15:39:04,  1.24s/it]  9%|▉         | 4698/50000 [1:40:25<15:38:11,  1.24s/it]  9%|▉         | 4699/50000 [1:40:26<15:37:27,  1.24s/it]  9%|▉         | 4700/50000 [1:40:27<15:38:09,  1.24s/it]                                                         {'loss': 151.3797, 'learning_rate': 1.9900655495357726e-05, 'epoch': 0.73}
  9%|▉         | 4700/50000 [1:40:27<15:38:09,  1.24s/it]  9%|▉         | 4701/50000 [1:40:28<15:38:31,  1.24s/it]  9%|▉         | 4702/50000 [1:40:30<15:38:21,  1.24s/it]  9%|▉         | 4703/50000 [1:40:31<15:38:10,  1.24s/it]  9%|▉         | 4704/50000 [1:40:32<15:37:51,  1.24s/it]  9%|▉         | 4705/50000 [1:40:33<15:37:20,  1.24s/it]  9%|▉         | 4706/50000 [1:40:34<15:36:46,  1.24s/it]  9%|▉         | 4707/50000 [1:40:36<15:36:33,  1.24s/it]  9%|▉         | 4708/50000 [1:40:37<15:36:38,  1.24s/it]  9%|▉         | 4709/50000 [1:40:38<15:37:05,  1.24s/it]  9%|▉         | 4710/50000 [1:40:39<15:36:36,  1.24s/it]                                                         {'loss': 135.5953, 'learning_rate': 1.9899723375103094e-05, 'epoch': 0.73}
  9%|▉         | 4710/50000 [1:40:39<15:36:36,  1.24s/it]  9%|▉         | 4711/50000 [1:40:41<15:36:51,  1.24s/it]  9%|▉         | 4712/50000 [1:40:42<15:36:29,  1.24s/it]  9%|▉         | 4713/50000 [1:40:43<15:36:38,  1.24s/it]  9%|▉         | 4714/50000 [1:40:44<15:36:06,  1.24s/it]  9%|▉         | 4715/50000 [1:40:46<15:36:10,  1.24s/it]  9%|▉         | 4716/50000 [1:40:47<15:36:17,  1.24s/it]  9%|▉         | 4717/50000 [1:40:48<15:36:09,  1.24s/it]  9%|▉         | 4718/50000 [1:40:49<15:36:17,  1.24s/it]  9%|▉         | 4719/50000 [1:40:51<15:36:02,  1.24s/it]  9%|▉         | 4720/50000 [1:40:52<15:36:37,  1.24s/it]                                                         {'loss': 137.1578, 'learning_rate': 1.9898786924373125e-05, 'epoch': 0.73}
  9%|▉         | 4720/50000 [1:40:52<15:36:37,  1.24s/it]  9%|▉         | 4721/50000 [1:40:53<15:36:20,  1.24s/it]  9%|▉         | 4722/50000 [1:40:54<15:36:07,  1.24s/it]  9%|▉         | 4723/50000 [1:40:56<15:36:23,  1.24s/it]  9%|▉         | 4724/50000 [1:40:57<15:36:08,  1.24s/it]  9%|▉         | 4725/50000 [1:40:58<15:36:18,  1.24s/it]  9%|▉         | 4726/50000 [1:40:59<15:36:01,  1.24s/it]  9%|▉         | 4727/50000 [1:41:01<15:35:52,  1.24s/it]  9%|▉         | 4728/50000 [1:41:02<15:35:41,  1.24s/it]  9%|▉         | 4729/50000 [1:41:03<15:35:32,  1.24s/it]  9%|▉         | 4730/50000 [1:41:04<15:36:14,  1.24s/it]                                                         {'loss': 135.5828, 'learning_rate': 1.989784614357745e-05, 'epoch': 0.73}
  9%|▉         | 4730/50000 [1:41:04<15:36:14,  1.24s/it]  9%|▉         | 4731/50000 [1:41:05<15:36:33,  1.24s/it]  9%|▉         | 4732/50000 [1:41:07<15:36:05,  1.24s/it]  9%|▉         | 4733/50000 [1:41:08<15:36:38,  1.24s/it]  9%|▉         | 4734/50000 [1:41:09<15:36:52,  1.24s/it]  9%|▉         | 4735/50000 [1:41:10<15:36:09,  1.24s/it]  9%|▉         | 4736/50000 [1:41:12<15:36:03,  1.24s/it]  9%|▉         | 4737/50000 [1:41:13<15:36:14,  1.24s/it]  9%|▉         | 4738/50000 [1:41:14<15:36:01,  1.24s/it]  9%|▉         | 4739/50000 [1:41:15<15:35:45,  1.24s/it]  9%|▉         | 4740/50000 [1:41:17<15:35:28,  1.24s/it]                                                         {'loss': 152.2797, 'learning_rate': 1.9896901033127603e-05, 'epoch': 0.73}
  9%|▉         | 4740/50000 [1:41:17<15:35:28,  1.24s/it]  9%|▉         | 4741/50000 [1:41:18<15:36:09,  1.24s/it]  9%|▉         | 4742/50000 [1:41:19<15:36:18,  1.24s/it]  9%|▉         | 4743/50000 [1:41:20<15:35:57,  1.24s/it]  9%|▉         | 4744/50000 [1:41:22<15:35:38,  1.24s/it]  9%|▉         | 4745/50000 [1:41:23<15:35:50,  1.24s/it]  9%|▉         | 4746/50000 [1:41:24<15:35:41,  1.24s/it]  9%|▉         | 4747/50000 [1:41:25<15:35:53,  1.24s/it]  9%|▉         | 4748/50000 [1:41:27<15:35:43,  1.24s/it]  9%|▉         | 4749/50000 [1:41:28<15:35:29,  1.24s/it] 10%|▉         | 4750/50000 [1:41:29<15:35:18,  1.24s/it]                                                         {'loss': 153.6438, 'learning_rate': 1.9895951593437005e-05, 'epoch': 0.74}
 10%|▉         | 4750/50000 [1:41:29<15:35:18,  1.24s/it] 10%|▉         | 4751/50000 [1:41:30<15:35:35,  1.24s/it] 10%|▉         | 4752/50000 [1:41:32<15:35:22,  1.24s/it] 10%|▉         | 4753/50000 [1:41:33<15:35:18,  1.24s/it] 10%|▉         | 4754/50000 [1:41:34<15:35:15,  1.24s/it] 10%|▉         | 4755/50000 [1:41:35<15:35:06,  1.24s/it] 10%|▉         | 4756/50000 [1:41:37<15:35:17,  1.24s/it] 10%|▉         | 4757/50000 [1:41:38<15:35:05,  1.24s/it] 10%|▉         | 4758/50000 [1:41:39<15:35:29,  1.24s/it] 10%|▉         | 4759/50000 [1:41:40<15:35:19,  1.24s/it] 10%|▉         | 4760/50000 [1:41:41<15:35:26,  1.24s/it]                                                         {'loss': 156.625, 'learning_rate': 1.9894997824920973e-05, 'epoch': 0.74}
 10%|▉         | 4760/50000 [1:41:41<15:35:26,  1.24s/it] 10%|▉         | 4761/50000 [1:41:43<15:35:33,  1.24s/it] 10%|▉         | 4762/50000 [1:41:44<15:35:15,  1.24s/it] 10%|▉         | 4763/50000 [1:41:45<15:35:44,  1.24s/it] 10%|▉         | 4764/50000 [1:41:46<15:35:10,  1.24s/it] 10%|▉         | 4765/50000 [1:41:48<15:34:59,  1.24s/it] 10%|▉         | 4766/50000 [1:41:49<15:34:59,  1.24s/it] 10%|▉         | 4767/50000 [1:41:50<15:34:46,  1.24s/it] 10%|▉         | 4768/50000 [1:41:51<15:34:51,  1.24s/it] 10%|▉         | 4769/50000 [1:41:53<15:35:05,  1.24s/it] 10%|▉         | 4770/50000 [1:41:54<15:38:01,  1.24s/it]                                                         {'loss': 150.8422, 'learning_rate': 1.9894039727996723e-05, 'epoch': 0.74}
 10%|▉         | 4770/50000 [1:41:54<15:38:01,  1.24s/it] 10%|▉         | 4771/50000 [1:41:55<15:38:26,  1.24s/it] 10%|▉         | 4772/50000 [1:41:56<15:38:40,  1.25s/it] 10%|▉         | 4773/50000 [1:41:58<15:37:24,  1.24s/it] 10%|▉         | 4774/50000 [1:41:59<15:36:40,  1.24s/it] 10%|▉         | 4775/50000 [1:42:00<15:36:52,  1.24s/it] 10%|▉         | 4776/50000 [1:42:01<15:42:25,  1.25s/it] 10%|▉         | 4777/50000 [1:42:03<15:41:46,  1.25s/it] 10%|▉         | 4778/50000 [1:42:04<15:40:50,  1.25s/it] 10%|▉         | 4779/50000 [1:42:05<15:39:16,  1.25s/it] 10%|▉         | 4780/50000 [1:42:06<15:38:36,  1.25s/it]                                                         {'loss': 177.0344, 'learning_rate': 1.989307730308335e-05, 'epoch': 0.74}
 10%|▉         | 4780/50000 [1:42:06<15:38:36,  1.25s/it] 10%|▉         | 4781/50000 [1:42:08<15:38:09,  1.24s/it] 10%|▉         | 4782/50000 [1:42:09<15:37:14,  1.24s/it] 10%|▉         | 4783/50000 [1:42:10<15:36:53,  1.24s/it] 10%|▉         | 4784/50000 [1:42:11<15:37:16,  1.24s/it] 10%|▉         | 4785/50000 [1:42:13<16:06:03,  1.28s/it] 10%|▉         | 4786/50000 [1:42:14<15:57:26,  1.27s/it] 10%|▉         | 4787/50000 [1:42:15<15:51:43,  1.26s/it] 10%|▉         | 4788/50000 [1:42:16<15:51:13,  1.26s/it] 10%|▉         | 4789/50000 [1:42:18<15:46:15,  1.26s/it] 10%|▉         | 4790/50000 [1:42:19<15:42:24,  1.25s/it]                                                         {'loss': 153.6656, 'learning_rate': 1.9892110550601858e-05, 'epoch': 0.74}
 10%|▉         | 4790/50000 [1:42:19<15:42:24,  1.25s/it] 10%|▉         | 4791/50000 [1:42:20<15:40:01,  1.25s/it] 10%|▉         | 4792/50000 [1:42:21<15:38:16,  1.25s/it] 10%|▉         | 4793/50000 [1:42:23<15:37:15,  1.24s/it] 10%|▉         | 4794/50000 [1:42:24<15:36:51,  1.24s/it] 10%|▉         | 4795/50000 [1:42:25<15:55:45,  1.27s/it] 10%|▉         | 4796/50000 [1:42:26<15:53:02,  1.26s/it] 10%|▉         | 4797/50000 [1:42:28<15:48:41,  1.26s/it] 10%|▉         | 4798/50000 [1:42:29<15:45:27,  1.25s/it] 10%|▉         | 4799/50000 [1:42:30<15:43:02,  1.25s/it] 10%|▉         | 4800/50000 [1:42:31<15:41:27,  1.25s/it]                                                         {'loss': 162.5797, 'learning_rate': 1.9891139470975133e-05, 'epoch': 0.74}
 10%|▉         | 4800/50000 [1:42:31<15:41:27,  1.25s/it] 10%|▉         | 4801/50000 [1:42:33<15:40:35,  1.25s/it] 10%|▉         | 4802/50000 [1:42:34<15:40:09,  1.25s/it] 10%|▉         | 4803/50000 [1:42:35<15:41:37,  1.25s/it] 10%|▉         | 4804/50000 [1:42:36<15:40:11,  1.25s/it] 10%|▉         | 4805/50000 [1:42:38<15:38:28,  1.25s/it] 10%|▉         | 4806/50000 [1:42:39<15:38:08,  1.25s/it] 10%|▉         | 4807/50000 [1:42:40<15:36:51,  1.24s/it] 10%|▉         | 4808/50000 [1:42:41<15:35:54,  1.24s/it] 10%|▉         | 4809/50000 [1:42:43<15:35:57,  1.24s/it] 10%|▉         | 4810/50000 [1:42:44<15:35:39,  1.24s/it]                                                         {'loss': 136.6445, 'learning_rate': 1.9890164064627963e-05, 'epoch': 0.75}
 10%|▉         | 4810/50000 [1:42:44<15:35:39,  1.24s/it] 10%|▉         | 4811/50000 [1:42:45<15:35:30,  1.24s/it] 10%|▉         | 4812/50000 [1:42:46<15:35:18,  1.24s/it] 10%|▉         | 4813/50000 [1:42:48<15:35:16,  1.24s/it] 10%|▉         | 4814/50000 [1:42:49<15:35:02,  1.24s/it] 10%|▉         | 4815/50000 [1:42:50<15:35:07,  1.24s/it] 10%|▉         | 4816/50000 [1:42:51<15:35:13,  1.24s/it] 10%|▉         | 4817/50000 [1:42:53<15:34:33,  1.24s/it] 10%|▉         | 4818/50000 [1:42:54<15:34:11,  1.24s/it] 10%|▉         | 4819/50000 [1:42:55<15:33:58,  1.24s/it] 10%|▉         | 4820/50000 [1:42:56<15:33:41,  1.24s/it]                                                         {'loss': 139.3313, 'learning_rate': 1.988918433198702e-05, 'epoch': 0.75}
 10%|▉         | 4820/50000 [1:42:56<15:33:41,  1.24s/it] 10%|▉         | 4821/50000 [1:42:58<15:34:25,  1.24s/it] 10%|▉         | 4822/50000 [1:42:59<15:34:07,  1.24s/it] 10%|▉         | 4823/50000 [1:43:00<15:33:54,  1.24s/it] 10%|▉         | 4824/50000 [1:43:01<15:34:35,  1.24s/it] 10%|▉         | 4825/50000 [1:43:03<15:34:09,  1.24s/it] 10%|▉         | 4826/50000 [1:43:04<16:02:44,  1.28s/it] 10%|▉         | 4827/50000 [1:43:05<15:54:39,  1.27s/it] 10%|▉         | 4828/50000 [1:43:06<15:48:33,  1.26s/it] 10%|▉         | 4829/50000 [1:43:08<15:43:56,  1.25s/it] 10%|▉         | 4830/50000 [1:43:09<15:41:05,  1.25s/it]                                                         {'loss': 135.0312, 'learning_rate': 1.9888200273480874e-05, 'epoch': 0.75}
 10%|▉         | 4830/50000 [1:43:09<15:41:05,  1.25s/it] 10%|▉         | 4831/50000 [1:43:10<15:39:02,  1.25s/it] 10%|▉         | 4832/50000 [1:43:11<15:38:26,  1.25s/it] 10%|▉         | 4833/50000 [1:43:13<15:36:53,  1.24s/it] 10%|▉         | 4834/50000 [1:43:14<15:38:02,  1.25s/it] 10%|▉         | 4835/50000 [1:43:15<15:36:33,  1.24s/it] 10%|▉         | 4836/50000 [1:43:16<15:35:26,  1.24s/it] 10%|▉         | 4837/50000 [1:43:18<15:35:52,  1.24s/it] 10%|▉         | 4838/50000 [1:43:19<15:37:41,  1.25s/it] 10%|▉         | 4839/50000 [1:43:20<15:36:16,  1.24s/it] 10%|▉         | 4840/50000 [1:43:21<15:35:43,  1.24s/it]                                                         {'loss': 150.9766, 'learning_rate': 1.988721188953999e-05, 'epoch': 0.75}
 10%|▉         | 4840/50000 [1:43:21<15:35:43,  1.24s/it] 10%|▉         | 4841/50000 [1:43:23<15:35:10,  1.24s/it] 10%|▉         | 4842/50000 [1:43:24<15:34:57,  1.24s/it] 10%|▉         | 4843/50000 [1:43:25<15:34:51,  1.24s/it] 10%|▉         | 4844/50000 [1:43:26<15:34:18,  1.24s/it] 10%|▉         | 4845/50000 [1:43:27<15:33:48,  1.24s/it] 10%|▉         | 4846/50000 [1:43:29<15:33:38,  1.24s/it] 10%|▉         | 4847/50000 [1:43:30<15:33:25,  1.24s/it] 10%|▉         | 4848/50000 [1:43:31<15:33:32,  1.24s/it] 10%|▉         | 4849/50000 [1:43:32<15:34:06,  1.24s/it] 10%|▉         | 4850/50000 [1:43:34<15:34:09,  1.24s/it]                                                         {'loss': 134.6266, 'learning_rate': 1.988621918059671e-05, 'epoch': 0.75}
 10%|▉         | 4850/50000 [1:43:34<15:34:09,  1.24s/it] 10%|▉         | 4851/50000 [1:43:35<15:33:59,  1.24s/it] 10%|▉         | 4852/50000 [1:43:36<15:33:30,  1.24s/it] 10%|▉         | 4853/50000 [1:43:37<15:33:28,  1.24s/it] 10%|▉         | 4854/50000 [1:43:39<15:33:13,  1.24s/it] 10%|▉         | 4855/50000 [1:43:40<15:33:03,  1.24s/it] 10%|▉         | 4856/50000 [1:43:41<15:32:57,  1.24s/it] 10%|▉         | 4857/50000 [1:43:42<15:33:01,  1.24s/it] 10%|▉         | 4858/50000 [1:43:44<15:33:07,  1.24s/it] 10%|▉         | 4859/50000 [1:43:45<15:33:54,  1.24s/it] 10%|▉         | 4860/50000 [1:43:46<15:33:37,  1.24s/it]                                                         {'loss': 134.5687, 'learning_rate': 1.9885222147085287e-05, 'epoch': 0.75}
 10%|▉         | 4860/50000 [1:43:46<15:33:37,  1.24s/it] 10%|▉         | 4861/50000 [1:43:47<15:33:36,  1.24s/it] 10%|▉         | 4862/50000 [1:43:49<15:33:29,  1.24s/it] 10%|▉         | 4863/50000 [1:43:50<15:33:13,  1.24s/it] 10%|▉         | 4864/50000 [1:43:51<15:32:47,  1.24s/it] 10%|▉         | 4865/50000 [1:43:52<15:32:46,  1.24s/it] 10%|▉         | 4866/50000 [1:43:54<15:33:09,  1.24s/it] 10%|▉         | 4867/50000 [1:43:55<15:33:34,  1.24s/it] 10%|▉         | 4868/50000 [1:43:56<15:33:20,  1.24s/it][2023-07-03 13:44:07,508] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
 10%|▉         | 4869/50000 [1:43:57<14:37:02,  1.17s/it][2023-07-03 13:44:08,499] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 10%|▉         | 4870/50000 [1:43:58<13:57:28,  1.11s/it]                                                         {'loss': 169.8609, 'learning_rate': 1.9884421406880102e-05, 'epoch': 0.75}
 10%|▉         | 4870/50000 [1:43:58<13:57:28,  1.11s/it] 10%|▉         | 4871/50000 [1:43:59<14:26:12,  1.15s/it][2023-07-03 13:44:10,730] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
 10%|▉         | 4872/50000 [1:44:00<13:49:47,  1.10s/it] 10%|▉         | 4873/50000 [1:44:01<14:20:42,  1.14s/it] 10%|▉         | 4874/50000 [1:44:03<14:42:08,  1.17s/it] 10%|▉         | 4875/50000 [1:44:04<14:57:07,  1.19s/it] 10%|▉         | 4876/50000 [1:44:05<15:07:46,  1.21s/it] 10%|▉         | 4877/50000 [1:44:06<15:15:21,  1.22s/it] 10%|▉         | 4878/50000 [1:44:08<15:20:27,  1.22s/it] 10%|▉         | 4879/50000 [1:44:09<15:24:22,  1.23s/it] 10%|▉         | 4880/50000 [1:44:10<15:26:37,  1.23s/it]                                                         {'loss': 143.4547, 'learning_rate': 1.988351726646739e-05, 'epoch': 0.76}
 10%|▉         | 4880/50000 [1:44:10<15:26:37,  1.23s/it] 10%|▉         | 4881/50000 [1:44:11<15:29:18,  1.24s/it] 10%|▉         | 4882/50000 [1:44:13<15:30:15,  1.24s/it] 10%|▉         | 4883/50000 [1:44:14<15:30:58,  1.24s/it] 10%|▉         | 4884/50000 [1:44:15<15:31:21,  1.24s/it] 10%|▉         | 4885/50000 [1:44:16<15:31:36,  1.24s/it] 10%|▉         | 4886/50000 [1:44:18<15:31:41,  1.24s/it] 10%|▉         | 4887/50000 [1:44:19<15:31:43,  1.24s/it] 10%|▉         | 4888/50000 [1:44:20<15:31:52,  1.24s/it] 10%|▉         | 4889/50000 [1:44:21<15:31:48,  1.24s/it] 10%|▉         | 4890/50000 [1:44:23<15:31:40,  1.24s/it]                                                         {'loss': 131.8123, 'learning_rate': 1.988250855880583e-05, 'epoch': 0.76}
 10%|▉         | 4890/50000 [1:44:23<15:31:40,  1.24s/it] 10%|▉         | 4891/50000 [1:44:24<15:32:04,  1.24s/it] 10%|▉         | 4892/50000 [1:44:25<15:32:00,  1.24s/it] 10%|▉         | 4893/50000 [1:44:26<15:31:54,  1.24s/it] 10%|▉         | 4894/50000 [1:44:27<15:31:44,  1.24s/it] 10%|▉         | 4895/50000 [1:44:29<15:31:37,  1.24s/it] 10%|▉         | 4896/50000 [1:44:30<15:31:40,  1.24s/it] 10%|▉         | 4897/50000 [1:44:31<15:31:40,  1.24s/it] 10%|▉         | 4898/50000 [1:44:32<15:31:43,  1.24s/it] 10%|▉         | 4899/50000 [1:44:34<15:31:58,  1.24s/it] 10%|▉         | 4900/50000 [1:44:35<15:31:46,  1.24s/it]                                                         {'loss': 148.4, 'learning_rate': 1.9881495528199276e-05, 'epoch': 0.76}
 10%|▉         | 4900/50000 [1:44:35<15:31:46,  1.24s/it] 10%|▉         | 4901/50000 [1:44:36<15:32:01,  1.24s/it] 10%|▉         | 4902/50000 [1:44:37<15:32:00,  1.24s/it] 10%|▉         | 4903/50000 [1:44:39<15:31:46,  1.24s/it] 10%|▉         | 4904/50000 [1:44:40<15:32:14,  1.24s/it] 10%|▉         | 4905/50000 [1:44:41<15:32:20,  1.24s/it] 10%|▉         | 4906/50000 [1:44:42<15:32:02,  1.24s/it] 10%|▉         | 4907/50000 [1:44:44<15:31:57,  1.24s/it] 10%|▉         | 4908/50000 [1:44:45<15:31:53,  1.24s/it] 10%|▉         | 4909/50000 [1:44:46<15:31:45,  1.24s/it] 10%|▉         | 4910/50000 [1:44:47<15:31:43,  1.24s/it]                                                         {'loss': 144.9313, 'learning_rate': 1.9880478175090862e-05, 'epoch': 0.76}
 10%|▉         | 4910/50000 [1:44:47<15:31:43,  1.24s/it] 10%|▉         | 4911/50000 [1:44:49<15:32:00,  1.24s/it][2023-07-03 13:45:00,080] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
 10%|▉         | 4912/50000 [1:44:50<14:35:27,  1.16s/it] 10%|▉         | 4913/50000 [1:44:51<14:51:56,  1.19s/it] 10%|▉         | 4914/50000 [1:44:52<15:04:06,  1.20s/it] 10%|▉         | 4915/50000 [1:44:53<15:12:14,  1.21s/it] 10%|▉         | 4916/50000 [1:44:55<15:17:51,  1.22s/it] 10%|▉         | 4917/50000 [1:44:56<15:21:49,  1.23s/it] 10%|▉         | 4918/50000 [1:44:57<15:24:32,  1.23s/it] 10%|▉         | 4919/50000 [1:44:58<15:26:17,  1.23s/it] 10%|▉         | 4920/50000 [1:44:59<15:27:44,  1.23s/it]                                                         {'loss': 140.2016, 'learning_rate': 1.9879558861921974e-05, 'epoch': 0.76}
 10%|▉         | 4920/50000 [1:44:59<15:27:44,  1.23s/it] 10%|▉         | 4921/50000 [1:45:01<15:28:50,  1.24s/it] 10%|▉         | 4922/50000 [1:45:02<15:29:56,  1.24s/it] 10%|▉         | 4923/50000 [1:45:03<15:30:17,  1.24s/it] 10%|▉         | 4924/50000 [1:45:04<15:30:35,  1.24s/it] 10%|▉         | 4925/50000 [1:45:06<15:30:51,  1.24s/it] 10%|▉         | 4926/50000 [1:45:07<15:31:19,  1.24s/it] 10%|▉         | 4927/50000 [1:45:08<15:31:40,  1.24s/it] 10%|▉         | 4928/50000 [1:45:09<15:31:38,  1.24s/it] 10%|▉         | 4929/50000 [1:45:11<15:31:50,  1.24s/it] 10%|▉         | 4930/50000 [1:45:12<15:31:58,  1.24s/it]                                                         {'loss': 135.6344, 'learning_rate': 1.9878533297287636e-05, 'epoch': 0.76}
 10%|▉         | 4930/50000 [1:45:12<15:31:58,  1.24s/it] 10%|▉         | 4931/50000 [1:45:13<15:31:49,  1.24s/it] 10%|▉         | 4932/50000 [1:45:14<15:31:45,  1.24s/it] 10%|▉         | 4933/50000 [1:45:16<15:31:54,  1.24s/it] 10%|▉         | 4934/50000 [1:45:17<15:31:41,  1.24s/it] 10%|▉         | 4935/50000 [1:45:18<15:31:42,  1.24s/it] 10%|▉         | 4936/50000 [1:45:19<15:31:48,  1.24s/it] 10%|▉         | 4937/50000 [1:45:21<15:31:41,  1.24s/it] 10%|▉         | 4938/50000 [1:45:22<15:31:39,  1.24s/it] 10%|▉         | 4939/50000 [1:45:23<15:31:40,  1.24s/it] 10%|▉         | 4940/50000 [1:45:24<15:35:01,  1.25s/it]                                                         {'loss': 144.8172, 'learning_rate': 1.9877503411447215e-05, 'epoch': 0.77}
 10%|▉         | 4940/50000 [1:45:24<15:35:01,  1.25s/it] 10%|▉         | 4941/50000 [1:45:26<15:34:09,  1.24s/it] 10%|▉         | 4942/50000 [1:45:27<15:33:12,  1.24s/it] 10%|▉         | 4943/50000 [1:45:28<15:32:42,  1.24s/it] 10%|▉         | 4944/50000 [1:45:29<15:32:16,  1.24s/it] 10%|▉         | 4945/50000 [1:45:31<15:31:52,  1.24s/it] 10%|▉         | 4946/50000 [1:45:32<15:32:11,  1.24s/it] 10%|▉         | 4947/50000 [1:45:33<15:31:47,  1.24s/it] 10%|▉         | 4948/50000 [1:45:34<15:31:21,  1.24s/it] 10%|▉         | 4949/50000 [1:45:35<15:31:09,  1.24s/it] 10%|▉         | 4950/50000 [1:45:37<15:31:06,  1.24s/it]                                                         {'loss': 141.3609, 'learning_rate': 1.9876469204851226e-05, 'epoch': 0.77}
 10%|▉         | 4950/50000 [1:45:37<15:31:06,  1.24s/it] 10%|▉         | 4951/50000 [1:45:38<15:31:08,  1.24s/it] 10%|▉         | 4952/50000 [1:45:39<15:31:07,  1.24s/it] 10%|▉         | 4953/50000 [1:45:40<15:31:13,  1.24s/it] 10%|▉         | 4954/50000 [1:45:42<15:31:30,  1.24s/it] 10%|▉         | 4955/50000 [1:45:43<15:31:07,  1.24s/it] 10%|▉         | 4956/50000 [1:45:44<15:31:06,  1.24s/it] 10%|▉         | 4957/50000 [1:45:45<15:31:28,  1.24s/it] 10%|▉         | 4958/50000 [1:45:47<15:32:41,  1.24s/it] 10%|▉         | 4959/50000 [1:45:48<15:32:36,  1.24s/it] 10%|▉         | 4960/50000 [1:45:49<15:32:05,  1.24s/it]                                                         {'loss': 149.3641, 'learning_rate': 1.987543067795206e-05, 'epoch': 0.77}
 10%|▉         | 4960/50000 [1:45:49<15:32:05,  1.24s/it] 10%|▉         | 4961/50000 [1:45:50<15:32:39,  1.24s/it] 10%|▉         | 4962/50000 [1:45:52<15:32:59,  1.24s/it] 10%|▉         | 4963/50000 [1:45:53<15:36:25,  1.25s/it] 10%|▉         | 4964/50000 [1:45:54<15:36:43,  1.25s/it] 10%|▉         | 4965/50000 [1:45:55<15:35:40,  1.25s/it] 10%|▉         | 4966/50000 [1:45:57<15:34:23,  1.24s/it] 10%|▉         | 4967/50000 [1:45:58<15:33:49,  1.24s/it] 10%|▉         | 4968/50000 [1:45:59<15:33:08,  1.24s/it] 10%|▉         | 4969/50000 [1:46:00<15:32:48,  1.24s/it] 10%|▉         | 4970/50000 [1:46:02<15:32:04,  1.24s/it]                                                         {'loss': 153.6937, 'learning_rate': 1.9874387831204007e-05, 'epoch': 0.77}
 10%|▉         | 4970/50000 [1:46:02<15:32:04,  1.24s/it] 10%|▉         | 4971/50000 [1:46:03<15:32:14,  1.24s/it] 10%|▉         | 4972/50000 [1:46:04<15:31:58,  1.24s/it] 10%|▉         | 4973/50000 [1:46:05<15:31:52,  1.24s/it] 10%|▉         | 4974/50000 [1:46:07<15:32:13,  1.24s/it] 10%|▉         | 4975/50000 [1:46:08<15:31:57,  1.24s/it] 10%|▉         | 4976/50000 [1:46:09<15:32:00,  1.24s/it] 10%|▉         | 4977/50000 [1:46:10<15:32:05,  1.24s/it] 10%|▉         | 4978/50000 [1:46:12<15:32:20,  1.24s/it] 10%|▉         | 4979/50000 [1:46:13<15:32:07,  1.24s/it] 10%|▉         | 4980/50000 [1:46:14<15:31:39,  1.24s/it]                                                         {'loss': 144.3641, 'learning_rate': 1.9873340665063244e-05, 'epoch': 0.77}
 10%|▉         | 4980/50000 [1:46:14<15:31:39,  1.24s/it] 10%|▉         | 4981/50000 [1:46:15<15:32:20,  1.24s/it] 10%|▉         | 4982/50000 [1:46:16<15:31:51,  1.24s/it] 10%|▉         | 4983/50000 [1:46:18<15:31:30,  1.24s/it] 10%|▉         | 4984/50000 [1:46:19<15:31:33,  1.24s/it] 10%|▉         | 4985/50000 [1:46:20<15:31:22,  1.24s/it] 10%|▉         | 4986/50000 [1:46:21<15:31:08,  1.24s/it] 10%|▉         | 4987/50000 [1:46:23<15:31:56,  1.24s/it] 10%|▉         | 4988/50000 [1:46:24<15:32:38,  1.24s/it] 10%|▉         | 4989/50000 [1:46:25<15:33:17,  1.24s/it] 10%|▉         | 4990/50000 [1:46:26<15:32:56,  1.24s/it]                                                         {'loss': 156.7188, 'learning_rate': 1.9872289179987833e-05, 'epoch': 0.77}
 10%|▉         | 4990/50000 [1:46:26<15:32:56,  1.24s/it] 10%|▉         | 4991/50000 [1:46:28<15:32:46,  1.24s/it] 10%|▉         | 4992/50000 [1:46:29<15:32:20,  1.24s/it] 10%|▉         | 4993/50000 [1:46:30<15:31:46,  1.24s/it] 10%|▉         | 4994/50000 [1:46:31<15:31:45,  1.24s/it] 10%|▉         | 4995/50000 [1:46:33<15:31:31,  1.24s/it] 10%|▉         | 4996/50000 [1:46:34<15:32:24,  1.24s/it] 10%|▉         | 4997/50000 [1:46:35<15:31:49,  1.24s/it] 10%|▉         | 4998/50000 [1:46:36<15:32:02,  1.24s/it] 10%|▉         | 4999/50000 [1:46:38<15:34:18,  1.25s/it] 10%|█         | 5000/50000 [1:46:39<15:34:36,  1.25s/it]                                                         {'loss': 137.6629, 'learning_rate': 1.9871233376437736e-05, 'epoch': 0.78}
 10%|█         | 5000/50000 [1:46:39<15:34:36,  1.25s/it][INFO|trainer.py:3129] 2023-07-03 13:46:49,374 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 13:46:49,374 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 13:46:49,374 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.43it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.14it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.73it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.54it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A                                                         
                                             [A{'eval_loss': 177.0, 'eval_accuracy': 0.3950022580159566, 'eval_runtime': 3.1365, 'eval_samples_per_second': 8.29, 'eval_steps_per_second': 2.232, 'epoch': 0.78}
 10%|█         | 5000/50000 [1:46:42<15:34:36,  1.25s/it]
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 13:46:52,512 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000
[INFO|trainer.py:2880] 2023-07-03 13:46:52,527 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 13:47:02,578 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 13:47:02,578 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/special_tokens_map.json
[2023-07-03 13:47:02,581] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 13:47:02,608] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/global_step5000/mp_rank_00_model_states.pt
[2023-07-03 13:47:02,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/global_step5000/mp_rank_00_model_states.pt...
[2023-07-03 13:47:12,891] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/global_step5000/mp_rank_00_model_states.pt.
[2023-07-03 13:47:12,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 13:47:29,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 13:47:29,753] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 13:47:29,753] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 13:47:29,952 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-4000] due to args.save_total_limit
 10%|█         | 5001/50000 [1:47:30<202:03:10, 16.16s/it] 10%|█         | 5002/50000 [1:47:31<146:05:54, 11.69s/it] 10%|█         | 5003/50000 [1:47:32<106:55:25,  8.55s/it] 10%|█         | 5004/50000 [1:47:34<79:31:13,  6.36s/it]  10%|█         | 5005/50000 [1:47:35<60:19:08,  4.83s/it] 10%|█         | 5006/50000 [1:47:36<46:52:19,  3.75s/it] 10%|█         | 5007/50000 [1:47:37<37:27:59,  3.00s/it] 10%|█         | 5008/50000 [1:47:39<30:53:10,  2.47s/it] 10%|█         | 5009/50000 [1:47:40<26:17:05,  2.10s/it] 10%|█         | 5010/50000 [1:47:41<23:04:00,  1.85s/it]                                                         {'loss': 169.8352, 'learning_rate': 1.987017325487479e-05, 'epoch': 0.78}
 10%|█         | 5010/50000 [1:47:41<23:04:00,  1.85s/it] 10%|█         | 5011/50000 [1:47:42<20:48:16,  1.66s/it] 10%|█         | 5012/50000 [1:47:43<19:12:56,  1.54s/it] 10%|█         | 5013/50000 [1:47:45<18:07:36,  1.45s/it] 10%|█         | 5014/50000 [1:47:46<17:20:18,  1.39s/it] 10%|█         | 5015/50000 [1:47:47<16:47:35,  1.34s/it] 10%|█         | 5016/50000 [1:47:48<16:24:20,  1.31s/it] 10%|█         | 5017/50000 [1:47:50<16:07:59,  1.29s/it] 10%|█         | 5018/50000 [1:47:51<15:56:36,  1.28s/it] 10%|█         | 5019/50000 [1:47:52<15:49:04,  1.27s/it] 10%|█         | 5020/50000 [1:47:53<15:42:51,  1.26s/it]                                                         {'loss': 160.6039, 'learning_rate': 1.9869108815762735e-05, 'epoch': 0.78}
 10%|█         | 5020/50000 [1:47:53<15:42:51,  1.26s/it] 10%|█         | 5021/50000 [1:47:55<15:39:20,  1.25s/it] 10%|█         | 5022/50000 [1:47:56<15:36:19,  1.25s/it] 10%|█         | 5023/50000 [1:47:57<15:34:26,  1.25s/it] 10%|█         | 5024/50000 [1:47:58<15:32:58,  1.24s/it] 10%|█         | 5025/50000 [1:48:00<15:32:00,  1.24s/it] 10%|█         | 5026/50000 [1:48:01<15:31:10,  1.24s/it] 10%|█         | 5027/50000 [1:48:02<15:30:22,  1.24s/it] 10%|█         | 5028/50000 [1:48:03<15:29:58,  1.24s/it] 10%|█         | 5029/50000 [1:48:05<15:30:22,  1.24s/it] 10%|█         | 5030/50000 [1:48:06<15:30:02,  1.24s/it]                                                         {'loss': 173.2859, 'learning_rate': 1.9868040059567185e-05, 'epoch': 0.78}
 10%|█         | 5030/50000 [1:48:06<15:30:02,  1.24s/it] 10%|█         | 5031/50000 [1:48:07<15:30:18,  1.24s/it] 10%|█         | 5032/50000 [1:48:08<15:29:56,  1.24s/it] 10%|█         | 5033/50000 [1:48:10<15:29:31,  1.24s/it] 10%|█         | 5034/50000 [1:48:11<15:29:12,  1.24s/it] 10%|█         | 5035/50000 [1:48:12<15:28:52,  1.24s/it] 10%|█         | 5036/50000 [1:48:13<15:28:51,  1.24s/it] 10%|█         | 5037/50000 [1:48:15<15:28:46,  1.24s/it] 10%|█         | 5038/50000 [1:48:16<15:28:40,  1.24s/it] 10%|█         | 5039/50000 [1:48:17<15:28:30,  1.24s/it] 10%|█         | 5040/50000 [1:48:18<15:28:03,  1.24s/it]                                                         {'loss': 137.3984, 'learning_rate': 1.986696698675566e-05, 'epoch': 0.78}
 10%|█         | 5040/50000 [1:48:18<15:28:03,  1.24s/it] 10%|█         | 5041/50000 [1:48:19<15:28:42,  1.24s/it] 10%|█         | 5042/50000 [1:48:21<15:28:47,  1.24s/it] 10%|█         | 5043/50000 [1:48:22<15:29:34,  1.24s/it] 10%|█         | 5044/50000 [1:48:23<15:29:24,  1.24s/it] 10%|█         | 5045/50000 [1:48:24<15:29:30,  1.24s/it] 10%|█         | 5046/50000 [1:48:26<15:29:51,  1.24s/it] 10%|█         | 5047/50000 [1:48:27<15:30:03,  1.24s/it] 10%|█         | 5048/50000 [1:48:28<15:29:52,  1.24s/it] 10%|█         | 5049/50000 [1:48:29<15:30:12,  1.24s/it] 10%|█         | 5050/50000 [1:48:31<15:30:06,  1.24s/it]                                                         {'loss': 151.7891, 'learning_rate': 1.986588959779755e-05, 'epoch': 0.78}
 10%|█         | 5050/50000 [1:48:31<15:30:06,  1.24s/it] 10%|█         | 5051/50000 [1:48:32<15:29:56,  1.24s/it] 10%|█         | 5052/50000 [1:48:33<15:29:56,  1.24s/it] 10%|█         | 5053/50000 [1:48:34<15:29:45,  1.24s/it] 10%|█         | 5054/50000 [1:48:36<15:29:52,  1.24s/it] 10%|█         | 5055/50000 [1:48:37<15:30:22,  1.24s/it] 10%|█         | 5056/50000 [1:48:38<15:30:39,  1.24s/it] 10%|█         | 5057/50000 [1:48:39<15:30:56,  1.24s/it] 10%|█         | 5058/50000 [1:48:41<15:31:49,  1.24s/it] 10%|█         | 5059/50000 [1:48:42<15:30:35,  1.24s/it] 10%|█         | 5060/50000 [1:48:43<15:29:45,  1.24s/it]                                                         {'loss': 148.4125, 'learning_rate': 1.9864807893164143e-05, 'epoch': 0.78}
 10%|█         | 5060/50000 [1:48:43<15:29:45,  1.24s/it] 10%|█         | 5061/50000 [1:48:44<15:29:17,  1.24s/it] 10%|█         | 5062/50000 [1:48:46<15:30:52,  1.24s/it] 10%|█         | 5063/50000 [1:48:47<15:32:57,  1.25s/it] 10%|█         | 5064/50000 [1:48:48<15:32:27,  1.25s/it] 10%|█         | 5065/50000 [1:48:49<15:33:40,  1.25s/it] 10%|█         | 5066/50000 [1:48:51<15:33:29,  1.25s/it] 10%|█         | 5067/50000 [1:48:52<15:32:34,  1.25s/it] 10%|█         | 5068/50000 [1:48:53<15:31:23,  1.24s/it] 10%|█         | 5069/50000 [1:48:54<15:33:02,  1.25s/it] 10%|█         | 5070/50000 [1:48:56<15:32:41,  1.25s/it]                                                         {'loss': 153.4445, 'learning_rate': 1.986372187332862e-05, 'epoch': 0.79}
 10%|█         | 5070/50000 [1:48:56<15:32:41,  1.25s/it][2023-07-03 13:49:07,030] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, but hysteresis is 2. Reducing hysteresis to 1
 10%|█         | 5071/50000 [1:48:57<14:37:01,  1.17s/it] 10%|█         | 5072/50000 [1:48:58<14:53:27,  1.19s/it] 10%|█         | 5073/50000 [1:48:59<15:04:15,  1.21s/it] 10%|█         | 5074/50000 [1:49:00<15:12:54,  1.22s/it] 10%|█         | 5075/50000 [1:49:01<15:19:02,  1.23s/it] 10%|█         | 5076/50000 [1:49:03<15:21:53,  1.23s/it] 10%|█         | 5077/50000 [1:49:04<15:23:56,  1.23s/it] 10%|█         | 5078/50000 [1:49:05<15:25:28,  1.24s/it] 10%|█         | 5079/50000 [1:49:06<15:27:25,  1.24s/it] 10%|█         | 5080/50000 [1:49:08<15:28:45,  1.24s/it]                                                         {'loss': 220.4047, 'learning_rate': 1.9862740766371434e-05, 'epoch': 0.79}
 10%|█         | 5080/50000 [1:49:08<15:28:45,  1.24s/it] 10%|█         | 5081/50000 [1:49:09<15:29:37,  1.24s/it] 10%|█         | 5082/50000 [1:49:10<15:31:01,  1.24s/it] 10%|█         | 5083/50000 [1:49:11<15:33:24,  1.25s/it] 10%|█         | 5084/50000 [1:49:13<15:31:59,  1.24s/it] 10%|█         | 5085/50000 [1:49:14<15:31:02,  1.24s/it] 10%|█         | 5086/50000 [1:49:15<15:30:57,  1.24s/it] 10%|█         | 5087/50000 [1:49:16<15:30:40,  1.24s/it] 10%|█         | 5088/50000 [1:49:18<15:30:50,  1.24s/it] 10%|█         | 5089/50000 [1:49:19<15:30:36,  1.24s/it] 10%|█         | 5090/50000 [1:49:20<15:30:53,  1.24s/it]                                                         {'loss': 148.6516, 'learning_rate': 1.986164654896224e-05, 'epoch': 0.79}
 10%|█         | 5090/50000 [1:49:20<15:30:53,  1.24s/it] 10%|█         | 5091/50000 [1:49:21<15:30:19,  1.24s/it] 10%|█         | 5092/50000 [1:49:23<15:30:43,  1.24s/it] 10%|█         | 5093/50000 [1:49:24<15:30:59,  1.24s/it] 10%|█         | 5094/50000 [1:49:25<15:30:36,  1.24s/it] 10%|█         | 5095/50000 [1:49:26<15:30:43,  1.24s/it] 10%|█         | 5096/50000 [1:49:28<15:30:20,  1.24s/it] 10%|█         | 5097/50000 [1:49:29<15:29:48,  1.24s/it] 10%|█         | 5098/50000 [1:49:30<15:29:05,  1.24s/it] 10%|█         | 5099/50000 [1:49:31<15:29:32,  1.24s/it] 10%|█         | 5100/50000 [1:49:33<15:29:41,  1.24s/it]                                                         {'loss': 156.2742, 'learning_rate': 1.9860548017733796e-05, 'epoch': 0.79}
 10%|█         | 5100/50000 [1:49:33<15:29:41,  1.24s/it] 10%|█         | 5101/50000 [1:49:34<15:29:50,  1.24s/it] 10%|█         | 5102/50000 [1:49:35<15:29:39,  1.24s/it] 10%|█         | 5103/50000 [1:49:36<15:30:05,  1.24s/it] 10%|█         | 5104/50000 [1:49:38<15:29:50,  1.24s/it] 10%|█         | 5105/50000 [1:49:39<15:29:13,  1.24s/it] 10%|█         | 5106/50000 [1:49:40<15:29:00,  1.24s/it] 10%|█         | 5107/50000 [1:49:41<15:29:12,  1.24s/it] 10%|█         | 5108/50000 [1:49:43<15:29:37,  1.24s/it] 10%|█         | 5109/50000 [1:49:44<15:29:49,  1.24s/it] 10%|█         | 5110/50000 [1:49:45<15:30:20,  1.24s/it]                                                         {'loss': 142.7953, 'learning_rate': 1.985944517316665e-05, 'epoch': 0.79}
 10%|█         | 5110/50000 [1:49:45<15:30:20,  1.24s/it] 10%|█         | 5111/50000 [1:49:46<15:30:18,  1.24s/it] 10%|█         | 5112/50000 [1:49:47<15:29:42,  1.24s/it] 10%|█         | 5113/50000 [1:49:49<15:29:16,  1.24s/it] 10%|█         | 5114/50000 [1:49:50<15:29:01,  1.24s/it] 10%|█         | 5115/50000 [1:49:51<15:28:45,  1.24s/it] 10%|█         | 5116/50000 [1:49:52<15:29:43,  1.24s/it] 10%|█         | 5117/50000 [1:49:54<15:29:14,  1.24s/it] 10%|█         | 5118/50000 [1:49:55<15:29:06,  1.24s/it] 10%|█         | 5119/50000 [1:49:56<15:28:53,  1.24s/it] 10%|█         | 5120/50000 [1:49:57<15:28:40,  1.24s/it]                                                         {'loss': 131.3852, 'learning_rate': 1.985833801574322e-05, 'epoch': 0.79}
 10%|█         | 5120/50000 [1:49:57<15:28:40,  1.24s/it] 10%|█         | 5121/50000 [1:49:59<15:29:21,  1.24s/it] 10%|█         | 5122/50000 [1:50:00<15:29:27,  1.24s/it] 10%|█         | 5123/50000 [1:50:01<15:29:02,  1.24s/it] 10%|█         | 5124/50000 [1:50:02<15:28:59,  1.24s/it] 10%|█         | 5125/50000 [1:50:04<15:28:45,  1.24s/it] 10%|█         | 5126/50000 [1:50:05<15:28:31,  1.24s/it] 10%|█         | 5127/50000 [1:50:06<15:28:24,  1.24s/it] 10%|█         | 5128/50000 [1:50:07<15:28:19,  1.24s/it] 10%|█         | 5129/50000 [1:50:09<15:28:13,  1.24s/it] 10%|█         | 5130/50000 [1:50:10<15:28:16,  1.24s/it]                                                         {'loss': 143.65, 'learning_rate': 1.9857226545947813e-05, 'epoch': 0.8}
 10%|█         | 5130/50000 [1:50:10<15:28:16,  1.24s/it] 10%|█         | 5131/50000 [1:50:11<15:28:30,  1.24s/it] 10%|█         | 5132/50000 [1:50:12<15:28:24,  1.24s/it] 10%|█         | 5133/50000 [1:50:14<15:28:41,  1.24s/it] 10%|█         | 5134/50000 [1:50:15<15:28:55,  1.24s/it] 10%|█         | 5135/50000 [1:50:16<15:28:48,  1.24s/it] 10%|█         | 5136/50000 [1:50:17<15:28:31,  1.24s/it] 10%|█         | 5137/50000 [1:50:19<15:28:24,  1.24s/it] 10%|█         | 5138/50000 [1:50:20<15:28:22,  1.24s/it] 10%|█         | 5139/50000 [1:50:21<15:28:33,  1.24s/it] 10%|█         | 5140/50000 [1:50:22<15:28:33,  1.24s/it]                                                         {'loss': 145.8422, 'learning_rate': 1.9856110764266622e-05, 'epoch': 0.8}
 10%|█         | 5140/50000 [1:50:22<15:28:33,  1.24s/it] 10%|█         | 5141/50000 [1:50:24<15:28:20,  1.24s/it] 10%|█         | 5142/50000 [1:50:25<15:28:26,  1.24s/it] 10%|█         | 5143/50000 [1:50:26<15:28:20,  1.24s/it] 10%|█         | 5144/50000 [1:50:27<15:28:04,  1.24s/it] 10%|█         | 5145/50000 [1:50:28<15:27:39,  1.24s/it] 10%|█         | 5146/50000 [1:50:30<15:28:00,  1.24s/it] 10%|█         | 5147/50000 [1:50:31<15:27:39,  1.24s/it] 10%|█         | 5148/50000 [1:50:32<15:28:34,  1.24s/it] 10%|█         | 5149/50000 [1:50:33<15:28:14,  1.24s/it] 10%|█         | 5150/50000 [1:50:35<15:28:17,  1.24s/it]                                                         {'loss': 160.3047, 'learning_rate': 1.9854990671187733e-05, 'epoch': 0.8}
 10%|█         | 5150/50000 [1:50:35<15:28:17,  1.24s/it] 10%|█         | 5151/50000 [1:50:36<15:28:05,  1.24s/it] 10%|█         | 5152/50000 [1:50:37<15:27:54,  1.24s/it] 10%|█         | 5153/50000 [1:50:38<15:27:41,  1.24s/it] 10%|█         | 5154/50000 [1:50:40<15:27:50,  1.24s/it] 10%|█         | 5155/50000 [1:50:41<15:28:02,  1.24s/it] 10%|█         | 5156/50000 [1:50:42<15:27:33,  1.24s/it] 10%|█         | 5157/50000 [1:50:43<15:27:44,  1.24s/it] 10%|█         | 5158/50000 [1:50:45<15:27:55,  1.24s/it] 10%|█         | 5159/50000 [1:50:46<15:27:37,  1.24s/it] 10%|█         | 5160/50000 [1:50:47<15:27:27,  1.24s/it]                                                         {'loss': 193.0406, 'learning_rate': 1.9853866267201107e-05, 'epoch': 0.8}
 10%|█         | 5160/50000 [1:50:47<15:27:27,  1.24s/it] 10%|█         | 5161/50000 [1:50:48<15:27:38,  1.24s/it] 10%|█         | 5162/50000 [1:50:50<15:27:23,  1.24s/it] 10%|█         | 5163/50000 [1:50:51<15:27:18,  1.24s/it] 10%|█         | 5164/50000 [1:50:52<15:27:12,  1.24s/it] 10%|█         | 5165/50000 [1:50:53<15:27:03,  1.24s/it] 10%|█         | 5166/50000 [1:50:55<15:26:59,  1.24s/it] 10%|█         | 5167/50000 [1:50:56<15:27:09,  1.24s/it] 10%|█         | 5168/50000 [1:50:57<15:27:09,  1.24s/it] 10%|█         | 5169/50000 [1:50:58<15:27:00,  1.24s/it] 10%|█         | 5170/50000 [1:50:59<15:26:51,  1.24s/it]                                                         {'loss': 166.0094, 'learning_rate': 1.98527375527986e-05, 'epoch': 0.8}
 10%|█         | 5170/50000 [1:50:59<15:26:51,  1.24s/it] 10%|█         | 5171/50000 [1:51:01<15:26:52,  1.24s/it] 10%|█         | 5172/50000 [1:51:02<15:26:56,  1.24s/it] 10%|█         | 5173/50000 [1:51:03<15:27:03,  1.24s/it] 10%|█         | 5174/50000 [1:51:04<15:26:56,  1.24s/it] 10%|█         | 5175/50000 [1:51:06<15:27:15,  1.24s/it] 10%|█         | 5176/50000 [1:51:07<15:27:16,  1.24s/it] 10%|█         | 5177/50000 [1:51:08<15:27:30,  1.24s/it] 10%|█         | 5178/50000 [1:51:09<15:27:47,  1.24s/it] 10%|█         | 5179/50000 [1:51:11<15:27:39,  1.24s/it] 10%|█         | 5180/50000 [1:51:12<15:27:21,  1.24s/it]                                                         {'loss': 153.4281, 'learning_rate': 1.9851604528473948e-05, 'epoch': 0.8}
 10%|█         | 5180/50000 [1:51:12<15:27:21,  1.24s/it] 10%|█         | 5181/50000 [1:51:13<15:27:06,  1.24s/it] 10%|█         | 5182/50000 [1:51:14<15:26:59,  1.24s/it] 10%|█         | 5183/50000 [1:51:16<15:26:51,  1.24s/it] 10%|█         | 5184/50000 [1:51:17<15:26:45,  1.24s/it] 10%|█         | 5185/50000 [1:51:18<15:26:50,  1.24s/it] 10%|█         | 5186/50000 [1:51:19<15:26:51,  1.24s/it] 10%|█         | 5187/50000 [1:51:21<15:26:40,  1.24s/it] 10%|█         | 5188/50000 [1:51:22<15:26:24,  1.24s/it] 10%|█         | 5189/50000 [1:51:23<15:26:29,  1.24s/it] 10%|█         | 5190/50000 [1:51:24<15:26:26,  1.24s/it]                                                         {'loss': 151.3219, 'learning_rate': 1.985046719472278e-05, 'epoch': 0.8}
 10%|█         | 5190/50000 [1:51:24<15:26:26,  1.24s/it] 10%|█         | 5191/50000 [1:51:26<15:26:41,  1.24s/it] 10%|█         | 5192/50000 [1:51:27<15:26:27,  1.24s/it] 10%|█         | 5193/50000 [1:51:28<15:26:56,  1.24s/it] 10%|█         | 5194/50000 [1:51:29<15:26:55,  1.24s/it] 10%|█         | 5195/50000 [1:51:31<15:27:46,  1.24s/it] 10%|█         | 5196/50000 [1:51:32<15:27:29,  1.24s/it] 10%|█         | 5197/50000 [1:51:33<15:27:03,  1.24s/it] 10%|█         | 5198/50000 [1:51:34<15:27:48,  1.24s/it] 10%|█         | 5199/50000 [1:51:35<15:28:12,  1.24s/it] 10%|█         | 5200/50000 [1:51:37<15:28:18,  1.24s/it]                                                         {'loss': 145.9203, 'learning_rate': 1.9849325552042595e-05, 'epoch': 0.81}
 10%|█         | 5200/50000 [1:51:37<15:28:18,  1.24s/it] 10%|█         | 5201/50000 [1:51:38<15:27:54,  1.24s/it] 10%|█         | 5202/50000 [1:51:39<15:27:25,  1.24s/it] 10%|█         | 5203/50000 [1:51:40<15:27:18,  1.24s/it] 10%|█         | 5204/50000 [1:51:42<15:26:32,  1.24s/it] 10%|█         | 5205/50000 [1:51:43<15:26:37,  1.24s/it] 10%|█         | 5206/50000 [1:51:44<15:26:40,  1.24s/it] 10%|█         | 5207/50000 [1:51:45<15:26:37,  1.24s/it] 10%|█         | 5208/50000 [1:51:47<15:26:33,  1.24s/it] 10%|█         | 5209/50000 [1:51:48<15:26:30,  1.24s/it] 10%|█         | 5210/50000 [1:51:49<15:26:37,  1.24s/it]                                                         {'loss': 147.3609, 'learning_rate': 1.9848179600932793e-05, 'epoch': 0.81}
 10%|█         | 5210/50000 [1:51:49<15:26:37,  1.24s/it] 10%|█         | 5211/50000 [1:51:50<15:26:59,  1.24s/it] 10%|█         | 5212/50000 [1:51:52<15:26:49,  1.24s/it] 10%|█         | 5213/50000 [1:51:53<15:26:26,  1.24s/it] 10%|█         | 5214/50000 [1:51:54<15:26:39,  1.24s/it] 10%|█         | 5215/50000 [1:51:55<15:26:38,  1.24s/it] 10%|█         | 5216/50000 [1:51:57<15:26:59,  1.24s/it] 10%|█         | 5217/50000 [1:51:58<15:26:52,  1.24s/it] 10%|█         | 5218/50000 [1:51:59<15:26:27,  1.24s/it] 10%|█         | 5219/50000 [1:52:00<15:26:13,  1.24s/it] 10%|█         | 5220/50000 [1:52:02<15:26:10,  1.24s/it]                                                         {'loss': 147.6297, 'learning_rate': 1.9847029341894652e-05, 'epoch': 0.81}
 10%|█         | 5220/50000 [1:52:02<15:26:10,  1.24s/it] 10%|█         | 5221/50000 [1:52:03<15:26:13,  1.24s/it] 10%|█         | 5222/50000 [1:52:04<15:26:12,  1.24s/it] 10%|█         | 5223/50000 [1:52:05<15:26:31,  1.24s/it] 10%|█         | 5224/50000 [1:52:07<15:26:39,  1.24s/it] 10%|█         | 5225/50000 [1:52:08<15:26:26,  1.24s/it] 10%|█         | 5226/50000 [1:52:09<15:26:06,  1.24s/it] 10%|█         | 5227/50000 [1:52:10<15:26:28,  1.24s/it] 10%|█         | 5228/50000 [1:52:11<15:26:36,  1.24s/it] 10%|█         | 5229/50000 [1:52:13<15:26:22,  1.24s/it] 10%|█         | 5230/50000 [1:52:14<15:26:20,  1.24s/it]                                                         {'loss': 125.0, 'learning_rate': 1.984587477543133e-05, 'epoch': 0.81}
 10%|█         | 5230/50000 [1:52:14<15:26:20,  1.24s/it] 10%|█         | 5231/50000 [1:52:15<15:26:27,  1.24s/it] 10%|█         | 5232/50000 [1:52:16<15:26:19,  1.24s/it] 10%|█         | 5233/50000 [1:52:18<15:26:02,  1.24s/it] 10%|█         | 5234/50000 [1:52:19<15:26:00,  1.24s/it] 10%|█         | 5235/50000 [1:52:20<15:25:45,  1.24s/it] 10%|█         | 5236/50000 [1:52:21<15:26:12,  1.24s/it] 10%|█         | 5237/50000 [1:52:23<15:25:59,  1.24s/it] 10%|█         | 5238/50000 [1:52:24<15:26:09,  1.24s/it] 10%|█         | 5239/50000 [1:52:25<15:25:58,  1.24s/it] 10%|█         | 5240/50000 [1:52:26<15:25:56,  1.24s/it]                                                         {'loss': 145.2891, 'learning_rate': 1.9844715902047877e-05, 'epoch': 0.81}
 10%|█         | 5240/50000 [1:52:26<15:25:56,  1.24s/it] 10%|█         | 5241/50000 [1:52:28<15:26:19,  1.24s/it] 10%|█         | 5242/50000 [1:52:29<15:25:53,  1.24s/it] 10%|█         | 5243/50000 [1:52:30<15:25:42,  1.24s/it] 10%|█         | 5244/50000 [1:52:31<15:25:51,  1.24s/it] 10%|█         | 5245/50000 [1:52:33<15:25:42,  1.24s/it] 10%|█         | 5246/50000 [1:52:34<15:25:35,  1.24s/it] 10%|█         | 5247/50000 [1:52:35<15:38:40,  1.26s/it][2023-07-03 13:52:46,642] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, but hysteresis is 2. Reducing hysteresis to 1
 10%|█         | 5248/50000 [1:52:36<14:38:58,  1.18s/it] 10%|█         | 5249/50000 [1:52:37<14:52:44,  1.20s/it] 10%|█         | 5250/50000 [1:52:39<15:02:37,  1.21s/it]                                                         {'loss': 136.1844, 'learning_rate': 1.9843669234004998e-05, 'epoch': 0.81}
 10%|█         | 5250/50000 [1:52:39<15:02:37,  1.21s/it] 11%|█         | 5251/50000 [1:52:40<15:09:55,  1.22s/it] 11%|█         | 5252/50000 [1:52:41<15:15:01,  1.23s/it] 11%|█         | 5253/50000 [1:52:42<15:18:07,  1.23s/it] 11%|█         | 5254/50000 [1:52:44<15:20:21,  1.23s/it] 11%|█         | 5255/50000 [1:52:45<15:22:01,  1.24s/it] 11%|█         | 5256/50000 [1:52:46<15:23:01,  1.24s/it] 11%|█         | 5257/50000 [1:52:47<15:23:35,  1.24s/it] 11%|█         | 5258/50000 [1:52:49<15:24:03,  1.24s/it] 11%|█         | 5259/50000 [1:52:50<15:24:58,  1.24s/it] 11%|█         | 5260/50000 [1:52:51<15:25:21,  1.24s/it]                                                         {'loss': 130.6656, 'learning_rate': 1.9842502178871445e-05, 'epoch': 0.82}
 11%|█         | 5260/50000 [1:52:51<15:25:21,  1.24s/it] 11%|█         | 5261/50000 [1:52:52<15:25:39,  1.24s/it] 11%|█         | 5262/50000 [1:52:54<15:25:38,  1.24s/it] 11%|█         | 5263/50000 [1:52:55<15:26:21,  1.24s/it] 11%|█         | 5264/50000 [1:52:56<15:26:26,  1.24s/it] 11%|█         | 5265/50000 [1:52:57<15:25:42,  1.24s/it] 11%|█         | 5266/50000 [1:52:58<15:25:28,  1.24s/it] 11%|█         | 5267/50000 [1:53:00<15:25:13,  1.24s/it] 11%|█         | 5268/50000 [1:53:01<15:25:11,  1.24s/it] 11%|█         | 5269/50000 [1:53:02<15:25:08,  1.24s/it] 11%|█         | 5270/50000 [1:53:03<15:25:12,  1.24s/it]                                                         {'loss': 136.9344, 'learning_rate': 1.9841330818293046e-05, 'epoch': 0.82}
 11%|█         | 5270/50000 [1:53:03<15:25:12,  1.24s/it] 11%|█         | 5271/50000 [1:53:05<15:25:12,  1.24s/it] 11%|█         | 5272/50000 [1:53:06<15:25:08,  1.24s/it] 11%|█         | 5273/50000 [1:53:07<15:25:00,  1.24s/it] 11%|█         | 5274/50000 [1:53:08<15:25:14,  1.24s/it] 11%|█         | 5275/50000 [1:53:10<15:25:14,  1.24s/it] 11%|█         | 5276/50000 [1:53:11<15:25:35,  1.24s/it] 11%|█         | 5277/50000 [1:53:12<15:25:16,  1.24s/it] 11%|█         | 5278/50000 [1:53:13<15:24:55,  1.24s/it] 11%|█         | 5279/50000 [1:53:15<15:24:57,  1.24s/it] 11%|█         | 5280/50000 [1:53:16<15:24:50,  1.24s/it]                                                         {'loss': 144.6672, 'learning_rate': 1.98401551527822e-05, 'epoch': 0.82}
 11%|█         | 5280/50000 [1:53:16<15:24:50,  1.24s/it] 11%|█         | 5281/50000 [1:53:17<15:25:09,  1.24s/it] 11%|█         | 5282/50000 [1:53:18<15:25:01,  1.24s/it] 11%|█         | 5283/50000 [1:53:20<15:24:55,  1.24s/it] 11%|█         | 5284/50000 [1:53:21<15:24:52,  1.24s/it] 11%|█         | 5285/50000 [1:53:22<15:24:51,  1.24s/it] 11%|█         | 5286/50000 [1:53:23<15:24:35,  1.24s/it] 11%|█         | 5287/50000 [1:53:25<15:24:44,  1.24s/it] 11%|█         | 5288/50000 [1:53:26<15:25:02,  1.24s/it] 11%|█         | 5289/50000 [1:53:27<15:24:45,  1.24s/it] 11%|█         | 5290/50000 [1:53:28<15:24:27,  1.24s/it]                                                         {'loss': 131.7578, 'learning_rate': 1.9838975182853183e-05, 'epoch': 0.82}
 11%|█         | 5290/50000 [1:53:28<15:24:27,  1.24s/it] 11%|█         | 5291/50000 [1:53:30<15:24:31,  1.24s/it] 11%|█         | 5292/50000 [1:53:31<15:24:19,  1.24s/it] 11%|█         | 5293/50000 [1:53:32<15:24:15,  1.24s/it] 11%|█         | 5294/50000 [1:53:33<15:24:21,  1.24s/it] 11%|█         | 5295/50000 [1:53:34<15:24:50,  1.24s/it] 11%|█         | 5296/50000 [1:53:36<15:24:28,  1.24s/it] 11%|█         | 5297/50000 [1:53:37<15:24:29,  1.24s/it] 11%|█         | 5298/50000 [1:53:38<15:24:20,  1.24s/it] 11%|█         | 5299/50000 [1:53:39<15:24:22,  1.24s/it] 11%|█         | 5300/50000 [1:53:41<15:24:24,  1.24s/it]                                                         {'loss': 147.05, 'learning_rate': 1.9837790909022143e-05, 'epoch': 0.82}
 11%|█         | 5300/50000 [1:53:41<15:24:24,  1.24s/it] 11%|█         | 5301/50000 [1:53:42<15:24:38,  1.24s/it] 11%|█         | 5302/50000 [1:53:43<15:24:28,  1.24s/it] 11%|█         | 5303/50000 [1:53:44<15:24:22,  1.24s/it] 11%|█         | 5304/50000 [1:53:46<15:24:05,  1.24s/it] 11%|█         | 5305/50000 [1:53:47<15:23:59,  1.24s/it] 11%|█         | 5306/50000 [1:53:48<15:24:10,  1.24s/it] 11%|█         | 5307/50000 [1:53:49<15:24:14,  1.24s/it] 11%|█         | 5308/50000 [1:53:51<15:24:12,  1.24s/it] 11%|█         | 5309/50000 [1:53:52<15:24:02,  1.24s/it] 11%|█         | 5310/50000 [1:53:53<15:23:56,  1.24s/it]                                                         {'loss': 145.3891, 'learning_rate': 1.9836602331807133e-05, 'epoch': 0.82}
 11%|█         | 5310/50000 [1:53:53<15:23:56,  1.24s/it] 11%|█         | 5311/50000 [1:53:54<15:24:02,  1.24s/it] 11%|█         | 5312/50000 [1:53:56<15:24:04,  1.24s/it] 11%|█         | 5313/50000 [1:53:57<15:23:46,  1.24s/it] 11%|█         | 5314/50000 [1:53:58<15:23:48,  1.24s/it] 11%|█         | 5315/50000 [1:53:59<15:23:52,  1.24s/it] 11%|█         | 5316/50000 [1:54:01<15:23:41,  1.24s/it] 11%|█         | 5317/50000 [1:54:02<15:24:26,  1.24s/it] 11%|█         | 5318/50000 [1:54:03<15:23:39,  1.24s/it] 11%|█         | 5319/50000 [1:54:04<15:24:01,  1.24s/it] 11%|█         | 5320/50000 [1:54:05<15:23:54,  1.24s/it]                                                         {'loss': 128.3484, 'learning_rate': 1.9835409451728073e-05, 'epoch': 0.82}
 11%|█         | 5320/50000 [1:54:05<15:23:54,  1.24s/it] 11%|█         | 5321/50000 [1:54:07<15:24:08,  1.24s/it] 11%|█         | 5322/50000 [1:54:08<15:23:55,  1.24s/it] 11%|█         | 5323/50000 [1:54:09<15:23:48,  1.24s/it] 11%|█         | 5324/50000 [1:54:10<15:23:47,  1.24s/it] 11%|█         | 5325/50000 [1:54:12<15:23:41,  1.24s/it] 11%|█         | 5326/50000 [1:54:13<15:23:42,  1.24s/it] 11%|█         | 5327/50000 [1:54:14<15:23:46,  1.24s/it] 11%|█         | 5328/50000 [1:54:15<15:23:41,  1.24s/it] 11%|█         | 5329/50000 [1:54:17<15:23:43,  1.24s/it] 11%|█         | 5330/50000 [1:54:18<15:23:41,  1.24s/it]                                                         {'loss': 124.9313, 'learning_rate': 1.983421226930677e-05, 'epoch': 0.83}
 11%|█         | 5330/50000 [1:54:18<15:23:41,  1.24s/it] 11%|█         | 5331/50000 [1:54:19<15:23:40,  1.24s/it] 11%|█         | 5332/50000 [1:54:20<15:23:29,  1.24s/it] 11%|█         | 5333/50000 [1:54:22<15:23:37,  1.24s/it] 11%|█         | 5334/50000 [1:54:23<15:23:41,  1.24s/it] 11%|█         | 5335/50000 [1:54:24<15:23:41,  1.24s/it] 11%|█         | 5336/50000 [1:54:25<15:23:39,  1.24s/it] 11%|█         | 5337/50000 [1:54:27<15:23:42,  1.24s/it] 11%|█         | 5338/50000 [1:54:28<15:24:00,  1.24s/it] 11%|█         | 5339/50000 [1:54:29<15:23:46,  1.24s/it] 11%|█         | 5340/50000 [1:54:30<15:23:47,  1.24s/it]                                                         {'loss': 137.4172, 'learning_rate': 1.983301078506691e-05, 'epoch': 0.83}
 11%|█         | 5340/50000 [1:54:30<15:23:47,  1.24s/it] 11%|█         | 5341/50000 [1:54:32<15:23:42,  1.24s/it] 11%|█         | 5342/50000 [1:54:33<15:23:28,  1.24s/it] 11%|█         | 5343/50000 [1:54:34<15:23:19,  1.24s/it] 11%|█         | 5344/50000 [1:54:35<15:23:12,  1.24s/it] 11%|█         | 5345/50000 [1:54:37<15:23:20,  1.24s/it] 11%|█         | 5346/50000 [1:54:38<15:23:08,  1.24s/it] 11%|█         | 5347/50000 [1:54:39<15:22:59,  1.24s/it] 11%|█         | 5348/50000 [1:54:40<15:23:01,  1.24s/it] 11%|█         | 5349/50000 [1:54:41<15:22:53,  1.24s/it] 11%|█         | 5350/50000 [1:54:43<15:23:00,  1.24s/it]                                                         {'loss': 138.1672, 'learning_rate': 1.9831804999534063e-05, 'epoch': 0.83}
 11%|█         | 5350/50000 [1:54:43<15:23:00,  1.24s/it] 11%|█         | 5351/50000 [1:54:44<15:23:07,  1.24s/it] 11%|█         | 5352/50000 [1:54:45<15:23:11,  1.24s/it] 11%|█         | 5353/50000 [1:54:46<15:23:38,  1.24s/it] 11%|█         | 5354/50000 [1:54:48<15:23:29,  1.24s/it] 11%|█         | 5355/50000 [1:54:49<15:23:18,  1.24s/it] 11%|█         | 5356/50000 [1:54:50<15:23:45,  1.24s/it] 11%|█         | 5357/50000 [1:54:51<15:23:27,  1.24s/it] 11%|█         | 5358/50000 [1:54:53<15:23:22,  1.24s/it][2023-07-03 13:55:04,139] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
 11%|█         | 5359/50000 [1:54:54<14:27:55,  1.17s/it] 11%|█         | 5360/50000 [1:54:55<14:44:14,  1.19s/it]                                                         {'loss': 172.8328, 'learning_rate': 1.9830716115384902e-05, 'epoch': 0.83}
 11%|█         | 5360/50000 [1:54:55<14:44:14,  1.19s/it] 11%|█         | 5361/50000 [1:54:56<14:55:49,  1.20s/it] 11%|█         | 5362/50000 [1:54:57<15:03:49,  1.21s/it] 11%|█         | 5363/50000 [1:54:59<15:09:27,  1.22s/it] 11%|█         | 5364/50000 [1:55:00<15:13:43,  1.23s/it] 11%|█         | 5365/50000 [1:55:01<15:16:14,  1.23s/it] 11%|█         | 5366/50000 [1:55:02<15:18:19,  1.23s/it] 11%|█         | 5367/50000 [1:55:04<15:19:43,  1.24s/it] 11%|█         | 5368/50000 [1:55:05<15:20:46,  1.24s/it] 11%|█         | 5369/50000 [1:55:06<15:21:33,  1.24s/it] 11%|█         | 5370/50000 [1:55:07<15:22:01,  1.24s/it]                                                         {'loss': 146.1086, 'learning_rate': 1.9829502158850068e-05, 'epoch': 0.83}
 11%|█         | 5370/50000 [1:55:07<15:22:01,  1.24s/it] 11%|█         | 5371/50000 [1:55:09<15:22:14,  1.24s/it] 11%|█         | 5372/50000 [1:55:10<15:22:30,  1.24s/it] 11%|█         | 5373/50000 [1:55:11<15:22:50,  1.24s/it] 11%|█         | 5374/50000 [1:55:12<15:22:47,  1.24s/it] 11%|█         | 5375/50000 [1:55:13<15:22:37,  1.24s/it] 11%|█         | 5376/50000 [1:55:15<15:22:52,  1.24s/it] 11%|█         | 5377/50000 [1:55:16<15:22:48,  1.24s/it] 11%|█         | 5378/50000 [1:55:17<15:23:03,  1.24s/it] 11%|█         | 5379/50000 [1:55:18<15:23:10,  1.24s/it] 11%|█         | 5380/50000 [1:55:20<15:22:57,  1.24s/it]                                                         {'loss': 140.7906, 'learning_rate': 1.982828390255704e-05, 'epoch': 0.83}
 11%|█         | 5380/50000 [1:55:20<15:22:57,  1.24s/it] 11%|█         | 5381/50000 [1:55:21<15:23:04,  1.24s/it][2023-07-03 13:55:32,431] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 11%|█         | 5382/50000 [1:55:22<14:27:27,  1.17s/it] 11%|█         | 5383/50000 [1:55:23<14:43:45,  1.19s/it] 11%|█         | 5384/50000 [1:55:24<14:59:29,  1.21s/it] 11%|█         | 5385/50000 [1:55:26<15:06:21,  1.22s/it] 11%|█         | 5386/50000 [1:55:27<15:11:02,  1.23s/it] 11%|█         | 5387/50000 [1:55:28<15:14:29,  1.23s/it] 11%|█         | 5388/50000 [1:55:29<15:16:51,  1.23s/it] 11%|█         | 5389/50000 [1:55:31<15:18:34,  1.24s/it] 11%|█         | 5390/50000 [1:55:32<15:19:40,  1.24s/it]                                                         {'loss': 142.1141, 'learning_rate': 1.982718379604047e-05, 'epoch': 0.84}
 11%|█         | 5390/50000 [1:55:32<15:19:40,  1.24s/it] 11%|█         | 5391/50000 [1:55:33<15:21:04,  1.24s/it] 11%|█         | 5392/50000 [1:55:34<15:21:34,  1.24s/it] 11%|█         | 5393/50000 [1:55:36<15:39:19,  1.26s/it] 11%|█         | 5394/50000 [1:55:37<15:34:23,  1.26s/it][2023-07-03 13:55:48,411] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
 11%|█         | 5395/50000 [1:55:38<14:35:37,  1.18s/it] 11%|█         | 5396/50000 [1:55:39<14:49:58,  1.20s/it] 11%|█         | 5397/50000 [1:55:40<14:59:52,  1.21s/it] 11%|█         | 5398/50000 [1:55:42<15:07:02,  1.22s/it] 11%|█         | 5399/50000 [1:55:43<15:11:59,  1.23s/it] 11%|█         | 5400/50000 [1:55:44<15:16:21,  1.23s/it]                                                         {'loss': 143.2828, 'learning_rate': 1.9826080207541187e-05, 'epoch': 0.84}
 11%|█         | 5400/50000 [1:55:44<15:16:21,  1.23s/it] 11%|█         | 5401/50000 [1:55:45<15:19:53,  1.24s/it] 11%|█         | 5402/50000 [1:55:47<15:24:08,  1.24s/it] 11%|█         | 5403/50000 [1:55:48<15:23:53,  1.24s/it] 11%|█         | 5404/50000 [1:55:49<15:23:22,  1.24s/it] 11%|█         | 5405/50000 [1:55:50<15:22:57,  1.24s/it] 11%|█         | 5406/50000 [1:55:52<15:22:42,  1.24s/it] 11%|█         | 5407/50000 [1:55:53<15:22:56,  1.24s/it] 11%|█         | 5408/50000 [1:55:54<15:22:50,  1.24s/it] 11%|█         | 5409/50000 [1:55:55<15:22:46,  1.24s/it] 11%|█         | 5410/50000 [1:55:57<15:22:42,  1.24s/it]                                                         {'loss': 150.15, 'learning_rate': 1.9824849914766284e-05, 'epoch': 0.84}
 11%|█         | 5410/50000 [1:55:57<15:22:42,  1.24s/it] 11%|█         | 5411/50000 [1:55:58<15:22:55,  1.24s/it] 11%|█         | 5412/50000 [1:55:59<15:22:29,  1.24s/it] 11%|█         | 5413/50000 [1:56:00<15:22:17,  1.24s/it] 11%|█         | 5414/50000 [1:56:02<15:22:13,  1.24s/it] 11%|█         | 5415/50000 [1:56:03<15:22:18,  1.24s/it] 11%|█         | 5416/50000 [1:56:04<15:22:14,  1.24s/it] 11%|█         | 5417/50000 [1:56:05<15:22:10,  1.24s/it] 11%|█         | 5418/50000 [1:56:06<15:21:57,  1.24s/it] 11%|█         | 5419/50000 [1:56:08<15:21:39,  1.24s/it] 11%|█         | 5420/50000 [1:56:09<15:21:45,  1.24s/it]                                                         {'loss': 155.6406, 'learning_rate': 1.9823615324268243e-05, 'epoch': 0.84}
 11%|█         | 5420/50000 [1:56:09<15:21:45,  1.24s/it] 11%|█         | 5421/50000 [1:56:10<15:22:18,  1.24s/it] 11%|█         | 5422/50000 [1:56:11<15:22:52,  1.24s/it] 11%|█         | 5423/50000 [1:56:13<15:22:35,  1.24s/it] 11%|█         | 5424/50000 [1:56:14<15:22:16,  1.24s/it] 11%|█         | 5425/50000 [1:56:15<15:22:30,  1.24s/it] 11%|█         | 5426/50000 [1:56:16<15:22:19,  1.24s/it] 11%|█         | 5427/50000 [1:56:18<15:22:15,  1.24s/it] 11%|█         | 5428/50000 [1:56:19<15:22:20,  1.24s/it] 11%|█         | 5429/50000 [1:56:20<15:22:20,  1.24s/it] 11%|█         | 5430/50000 [1:56:21<15:22:16,  1.24s/it]                                                         {'loss': 144.0016, 'learning_rate': 1.9822376436587106e-05, 'epoch': 0.84}
 11%|█         | 5430/50000 [1:56:21<15:22:16,  1.24s/it] 11%|█         | 5431/50000 [1:56:23<15:22:47,  1.24s/it] 11%|█         | 5432/50000 [1:56:24<15:23:01,  1.24s/it] 11%|█         | 5433/50000 [1:56:25<15:22:38,  1.24s/it] 11%|█         | 5434/50000 [1:56:26<15:22:13,  1.24s/it] 11%|█         | 5435/50000 [1:56:28<15:22:01,  1.24s/it] 11%|█         | 5436/50000 [1:56:29<15:21:56,  1.24s/it] 11%|█         | 5437/50000 [1:56:30<15:21:42,  1.24s/it] 11%|█         | 5438/50000 [1:56:31<15:21:43,  1.24s/it] 11%|█         | 5439/50000 [1:56:33<15:21:50,  1.24s/it] 11%|█         | 5440/50000 [1:56:34<15:21:52,  1.24s/it]                                                         {'loss': 166.5062, 'learning_rate': 1.9821133252264812e-05, 'epoch': 0.84}
 11%|█         | 5440/50000 [1:56:34<15:21:52,  1.24s/it] 11%|█         | 5441/50000 [1:56:35<15:48:11,  1.28s/it] 11%|█         | 5442/50000 [1:56:36<15:39:42,  1.27s/it] 11%|█         | 5443/50000 [1:56:38<15:34:14,  1.26s/it] 11%|█         | 5444/50000 [1:56:39<15:30:29,  1.25s/it] 11%|█         | 5445/50000 [1:56:40<15:28:39,  1.25s/it] 11%|█         | 5446/50000 [1:56:41<15:28:40,  1.25s/it] 11%|█         | 5447/50000 [1:56:43<15:26:46,  1.25s/it] 11%|█         | 5448/50000 [1:56:44<15:25:10,  1.25s/it] 11%|█         | 5449/50000 [1:56:45<15:24:03,  1.24s/it] 11%|█         | 5450/50000 [1:56:46<15:23:06,  1.24s/it]                                                         {'loss': 136.6625, 'learning_rate': 1.9819885771845165e-05, 'epoch': 0.84}
 11%|█         | 5450/50000 [1:56:46<15:23:06,  1.24s/it] 11%|█         | 5451/50000 [1:56:48<15:24:03,  1.24s/it] 11%|█         | 5452/50000 [1:56:49<15:23:39,  1.24s/it] 11%|█         | 5453/50000 [1:56:50<15:23:55,  1.24s/it] 11%|█         | 5454/50000 [1:56:51<15:23:46,  1.24s/it] 11%|█         | 5455/50000 [1:56:53<15:23:06,  1.24s/it] 11%|█         | 5456/50000 [1:56:54<15:22:28,  1.24s/it] 11%|█         | 5457/50000 [1:56:55<15:22:02,  1.24s/it] 11%|█         | 5458/50000 [1:56:56<15:21:51,  1.24s/it] 11%|█         | 5459/50000 [1:56:58<15:21:53,  1.24s/it] 11%|█         | 5460/50000 [1:56:59<15:22:21,  1.24s/it]                                                         {'loss': 146.2422, 'learning_rate': 1.9818633995873862e-05, 'epoch': 0.85}
 11%|█         | 5460/50000 [1:56:59<15:22:21,  1.24s/it] 11%|█         | 5461/50000 [1:57:00<15:22:28,  1.24s/it] 11%|█         | 5462/50000 [1:57:01<15:22:34,  1.24s/it] 11%|█         | 5463/50000 [1:57:02<15:22:39,  1.24s/it] 11%|█         | 5464/50000 [1:57:04<15:23:03,  1.24s/it] 11%|█         | 5465/50000 [1:57:05<15:22:41,  1.24s/it] 11%|█         | 5466/50000 [1:57:06<15:22:28,  1.24s/it] 11%|█         | 5467/50000 [1:57:07<15:22:52,  1.24s/it] 11%|█         | 5468/50000 [1:57:09<15:22:05,  1.24s/it] 11%|█         | 5469/50000 [1:57:10<15:21:41,  1.24s/it] 11%|█         | 5470/50000 [1:57:11<15:22:13,  1.24s/it]                                                         {'loss': 154.5578, 'learning_rate': 1.9817377924898465e-05, 'epoch': 0.85}
 11%|█         | 5470/50000 [1:57:11<15:22:13,  1.24s/it] 11%|█         | 5471/50000 [1:57:12<15:22:40,  1.24s/it] 11%|█         | 5472/50000 [1:57:14<15:21:57,  1.24s/it] 11%|█         | 5473/50000 [1:57:15<15:21:57,  1.24s/it] 11%|█         | 5474/50000 [1:57:16<15:22:05,  1.24s/it] 11%|█         | 5475/50000 [1:57:17<15:22:49,  1.24s/it] 11%|█         | 5476/50000 [1:57:19<15:21:55,  1.24s/it] 11%|█         | 5477/50000 [1:57:20<15:21:16,  1.24s/it] 11%|█         | 5478/50000 [1:57:21<15:19:33,  1.24s/it] 11%|█         | 5479/50000 [1:57:22<15:18:18,  1.24s/it] 11%|█         | 5480/50000 [1:57:24<15:17:56,  1.24s/it]                                                         {'loss': 143.3531, 'learning_rate': 1.9816117559468434e-05, 'epoch': 0.85}
 11%|█         | 5480/50000 [1:57:24<15:17:56,  1.24s/it] 11%|█         | 5481/50000 [1:57:25<15:18:22,  1.24s/it] 11%|█         | 5482/50000 [1:57:26<15:18:51,  1.24s/it] 11%|█         | 5483/50000 [1:57:27<15:19:11,  1.24s/it] 11%|█         | 5484/50000 [1:57:29<15:18:40,  1.24s/it] 11%|█         | 5485/50000 [1:57:30<15:18:35,  1.24s/it] 11%|█         | 5486/50000 [1:57:31<15:18:22,  1.24s/it] 11%|█         | 5487/50000 [1:57:32<15:19:25,  1.24s/it] 11%|█         | 5488/50000 [1:57:34<15:19:36,  1.24s/it] 11%|█         | 5489/50000 [1:57:35<15:19:28,  1.24s/it] 11%|█         | 5490/50000 [1:57:36<15:19:16,  1.24s/it]                                                         {'loss': 164.1328, 'learning_rate': 1.9814852900135087e-05, 'epoch': 0.85}
 11%|█         | 5490/50000 [1:57:36<15:19:16,  1.24s/it] 11%|█         | 5491/50000 [1:57:37<15:19:24,  1.24s/it] 11%|█         | 5492/50000 [1:57:38<15:19:21,  1.24s/it] 11%|█         | 5493/50000 [1:57:40<15:19:14,  1.24s/it] 11%|█         | 5494/50000 [1:57:41<15:19:43,  1.24s/it] 11%|█         | 5495/50000 [1:57:42<15:19:32,  1.24s/it] 11%|█         | 5496/50000 [1:57:43<15:19:38,  1.24s/it] 11%|█         | 5497/50000 [1:57:45<15:19:46,  1.24s/it] 11%|█         | 5498/50000 [1:57:46<15:19:38,  1.24s/it] 11%|█         | 5499/50000 [1:57:47<15:19:40,  1.24s/it] 11%|█         | 5500/50000 [1:57:48<15:19:38,  1.24s/it]                                                         {'loss': 159.1875, 'learning_rate': 1.9813583947451627e-05, 'epoch': 0.85}
 11%|█         | 5500/50000 [1:57:48<15:19:38,  1.24s/it] 11%|█         | 5501/50000 [1:57:50<15:18:36,  1.24s/it] 11%|█         | 5502/50000 [1:57:51<15:17:34,  1.24s/it] 11%|█         | 5503/50000 [1:57:52<15:16:51,  1.24s/it] 11%|█         | 5504/50000 [1:57:53<15:16:18,  1.24s/it] 11%|█         | 5505/50000 [1:57:55<15:16:02,  1.24s/it] 11%|█         | 5506/50000 [1:57:56<15:15:42,  1.23s/it] 11%|█         | 5507/50000 [1:57:57<15:15:38,  1.23s/it] 11%|█         | 5508/50000 [1:57:58<15:15:35,  1.23s/it] 11%|█         | 5509/50000 [1:57:59<15:15:19,  1.23s/it] 11%|█         | 5510/50000 [1:58:01<15:15:07,  1.23s/it]                                                         {'loss': 146.6891, 'learning_rate': 1.981231070197314e-05, 'epoch': 0.85}
 11%|█         | 5510/50000 [1:58:01<15:15:07,  1.23s/it] 11%|█         | 5511/50000 [1:58:02<15:15:34,  1.23s/it] 11%|█         | 5512/50000 [1:58:03<15:15:20,  1.23s/it] 11%|█         | 5513/50000 [1:58:04<15:15:31,  1.23s/it] 11%|█         | 5514/50000 [1:58:06<15:15:19,  1.23s/it] 11%|█         | 5515/50000 [1:58:07<15:15:20,  1.23s/it] 11%|█         | 5516/50000 [1:58:08<15:15:01,  1.23s/it] 11%|█         | 5517/50000 [1:58:09<15:14:45,  1.23s/it] 11%|█         | 5518/50000 [1:58:11<15:15:47,  1.24s/it] 11%|█         | 5519/50000 [1:58:12<15:16:02,  1.24s/it] 11%|█         | 5520/50000 [1:58:13<15:17:23,  1.24s/it]                                                         {'loss': 135.4375, 'learning_rate': 1.9811033164256593e-05, 'epoch': 0.86}
 11%|█         | 5520/50000 [1:58:13<15:17:23,  1.24s/it] 11%|█         | 5521/50000 [1:58:14<15:20:03,  1.24s/it] 11%|█         | 5522/50000 [1:58:16<15:22:00,  1.24s/it] 11%|█         | 5523/50000 [1:58:17<15:22:33,  1.24s/it] 11%|█         | 5524/50000 [1:58:18<15:21:24,  1.24s/it] 11%|█         | 5525/50000 [1:58:19<15:21:00,  1.24s/it] 11%|█         | 5526/50000 [1:58:21<15:21:38,  1.24s/it] 11%|█         | 5527/50000 [1:58:22<15:21:15,  1.24s/it] 11%|█         | 5528/50000 [1:58:23<15:21:05,  1.24s/it] 11%|█         | 5529/50000 [1:58:24<15:21:49,  1.24s/it] 11%|█         | 5530/50000 [1:58:26<15:22:33,  1.24s/it]                                                         {'loss': 139.0094, 'learning_rate': 1.9809751334860815e-05, 'epoch': 0.86}
 11%|█         | 5530/50000 [1:58:26<15:22:33,  1.24s/it] 11%|█         | 5531/50000 [1:58:27<15:23:30,  1.25s/it] 11%|█         | 5532/50000 [1:58:28<15:22:18,  1.24s/it] 11%|█         | 5533/50000 [1:58:29<15:21:43,  1.24s/it] 11%|█         | 5534/50000 [1:58:31<15:22:23,  1.24s/it] 11%|█         | 5535/50000 [1:58:32<15:21:10,  1.24s/it] 11%|█         | 5536/50000 [1:58:33<15:20:31,  1.24s/it] 11%|█         | 5537/50000 [1:58:34<15:21:23,  1.24s/it] 11%|█         | 5538/50000 [1:58:35<15:22:16,  1.24s/it] 11%|█         | 5539/50000 [1:58:37<15:21:56,  1.24s/it] 11%|█         | 5540/50000 [1:58:38<15:22:09,  1.24s/it]                                                         {'loss': 122.2531, 'learning_rate': 1.9808465214346525e-05, 'epoch': 0.86}
 11%|█         | 5540/50000 [1:58:38<15:22:09,  1.24s/it] 11%|█         | 5541/50000 [1:58:39<15:21:58,  1.24s/it] 11%|█         | 5542/50000 [1:58:40<15:21:33,  1.24s/it] 11%|█         | 5543/50000 [1:58:42<15:20:42,  1.24s/it] 11%|█         | 5544/50000 [1:58:43<15:20:14,  1.24s/it] 11%|█         | 5545/50000 [1:58:44<15:20:00,  1.24s/it] 11%|█         | 5546/50000 [1:58:45<15:20:05,  1.24s/it] 11%|█         | 5547/50000 [1:58:47<15:20:00,  1.24s/it] 11%|█         | 5548/50000 [1:58:48<15:20:13,  1.24s/it] 11%|█         | 5549/50000 [1:58:49<15:20:04,  1.24s/it] 11%|█         | 5550/50000 [1:58:50<15:19:58,  1.24s/it]                                                         {'loss': 126.6984, 'learning_rate': 1.9807174803276317e-05, 'epoch': 0.86}
 11%|█         | 5550/50000 [1:58:50<15:19:58,  1.24s/it] 11%|█         | 5551/50000 [1:58:52<15:20:11,  1.24s/it] 11%|█         | 5552/50000 [1:58:53<15:19:44,  1.24s/it] 11%|█         | 5553/50000 [1:58:54<15:19:20,  1.24s/it] 11%|█         | 5554/50000 [1:58:55<15:19:03,  1.24s/it] 11%|█         | 5555/50000 [1:58:57<15:19:00,  1.24s/it] 11%|█         | 5556/50000 [1:58:58<15:18:51,  1.24s/it] 11%|█         | 5557/50000 [1:58:59<15:18:47,  1.24s/it] 11%|█         | 5558/50000 [1:59:00<15:18:48,  1.24s/it] 11%|█         | 5559/50000 [1:59:02<15:18:51,  1.24s/it] 11%|█         | 5560/50000 [1:59:03<15:18:55,  1.24s/it]                                                         {'loss': 145.3359, 'learning_rate': 1.980588010221466e-05, 'epoch': 0.86}
 11%|█         | 5560/50000 [1:59:03<15:18:55,  1.24s/it] 11%|█         | 5561/50000 [1:59:04<15:19:15,  1.24s/it] 11%|█         | 5562/50000 [1:59:05<15:20:38,  1.24s/it] 11%|█         | 5563/50000 [1:59:07<15:21:28,  1.24s/it] 11%|█         | 5564/50000 [1:59:08<15:20:35,  1.24s/it] 11%|█         | 5565/50000 [1:59:09<15:20:46,  1.24s/it] 11%|█         | 5566/50000 [1:59:10<15:20:50,  1.24s/it] 11%|█         | 5567/50000 [1:59:11<15:20:20,  1.24s/it] 11%|█         | 5568/50000 [1:59:13<15:19:53,  1.24s/it] 11%|█         | 5569/50000 [1:59:14<15:20:12,  1.24s/it] 11%|█         | 5570/50000 [1:59:15<15:19:28,  1.24s/it]                                                         {'loss': 135.1172, 'learning_rate': 1.98045811117279e-05, 'epoch': 0.86}
 11%|█         | 5570/50000 [1:59:15<15:19:28,  1.24s/it] 11%|█         | 5571/50000 [1:59:16<15:19:06,  1.24s/it] 11%|█         | 5572/50000 [1:59:18<15:19:09,  1.24s/it] 11%|█         | 5573/50000 [1:59:19<15:18:56,  1.24s/it] 11%|█         | 5574/50000 [1:59:20<15:18:45,  1.24s/it] 11%|█         | 5575/50000 [1:59:21<15:18:42,  1.24s/it] 11%|█         | 5576/50000 [1:59:23<15:18:42,  1.24s/it] 11%|█         | 5577/50000 [1:59:24<15:18:55,  1.24s/it] 11%|█         | 5578/50000 [1:59:25<15:19:35,  1.24s/it] 11%|█         | 5579/50000 [1:59:26<15:19:45,  1.24s/it] 11%|█         | 5580/50000 [1:59:28<15:19:46,  1.24s/it]                                                         {'loss': 138.9203, 'learning_rate': 1.980327783238426e-05, 'epoch': 0.86}
 11%|█         | 5580/50000 [1:59:28<15:19:46,  1.24s/it] 11%|█         | 5581/50000 [1:59:29<15:20:10,  1.24s/it] 11%|█         | 5582/50000 [1:59:30<15:19:57,  1.24s/it] 11%|█         | 5583/50000 [1:59:31<15:19:29,  1.24s/it] 11%|█         | 5584/50000 [1:59:33<15:18:56,  1.24s/it] 11%|█         | 5585/50000 [1:59:34<15:18:44,  1.24s/it] 11%|█         | 5586/50000 [1:59:35<15:18:45,  1.24s/it] 11%|█         | 5587/50000 [1:59:36<15:18:48,  1.24s/it] 11%|█         | 5588/50000 [1:59:38<15:18:52,  1.24s/it] 11%|█         | 5589/50000 [1:59:39<15:18:43,  1.24s/it] 11%|█         | 5590/50000 [1:59:40<15:18:39,  1.24s/it]                                                         {'loss': 143.3891, 'learning_rate': 1.9801970264753835e-05, 'epoch': 0.87}
 11%|█         | 5590/50000 [1:59:40<15:18:39,  1.24s/it] 11%|█         | 5591/50000 [1:59:41<15:18:54,  1.24s/it] 11%|█         | 5592/50000 [1:59:43<15:18:22,  1.24s/it] 11%|█         | 5593/50000 [1:59:44<15:18:02,  1.24s/it] 11%|█         | 5594/50000 [1:59:45<15:17:57,  1.24s/it] 11%|█         | 5595/50000 [1:59:46<15:17:57,  1.24s/it] 11%|█         | 5596/50000 [1:59:47<15:17:57,  1.24s/it] 11%|█         | 5597/50000 [1:59:49<15:18:37,  1.24s/it] 11%|█         | 5598/50000 [1:59:50<15:18:26,  1.24s/it] 11%|█         | 5599/50000 [1:59:51<15:19:05,  1.24s/it] 11%|█         | 5600/50000 [1:59:52<15:19:16,  1.24s/it]                                                         {'loss': 149.8328, 'learning_rate': 1.9800658409408605e-05, 'epoch': 0.87}
 11%|█         | 5600/50000 [1:59:52<15:19:16,  1.24s/it] 11%|█         | 5601/50000 [1:59:54<15:19:17,  1.24s/it] 11%|█         | 5602/50000 [1:59:55<15:18:34,  1.24s/it] 11%|█         | 5603/50000 [1:59:56<15:18:29,  1.24s/it] 11%|█         | 5604/50000 [1:59:57<15:18:08,  1.24s/it] 11%|█         | 5605/50000 [1:59:59<15:18:03,  1.24s/it] 11%|█         | 5606/50000 [2:00:00<15:17:56,  1.24s/it] 11%|█         | 5607/50000 [2:00:01<15:17:45,  1.24s/it] 11%|█         | 5608/50000 [2:00:02<15:17:40,  1.24s/it] 11%|█         | 5609/50000 [2:00:04<15:17:35,  1.24s/it] 11%|█         | 5610/50000 [2:00:05<15:17:51,  1.24s/it]                                                         {'loss': 147.6672, 'learning_rate': 1.9799342266922416e-05, 'epoch': 0.87}
 11%|█         | 5610/50000 [2:00:05<15:17:51,  1.24s/it][2023-07-03 14:00:16,376] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
 11%|█         | 5611/50000 [2:00:06<14:22:35,  1.17s/it] 11%|█         | 5612/50000 [2:00:07<14:38:51,  1.19s/it] 11%|█         | 5613/50000 [2:00:08<14:50:30,  1.20s/it] 11%|█         | 5614/50000 [2:00:10<14:58:23,  1.21s/it] 11%|█         | 5615/50000 [2:00:11<15:04:19,  1.22s/it] 11%|█         | 5616/50000 [2:00:12<15:08:03,  1.23s/it] 11%|█         | 5617/50000 [2:00:13<15:10:50,  1.23s/it] 11%|█         | 5618/50000 [2:00:15<15:12:51,  1.23s/it] 11%|█         | 5619/50000 [2:00:16<15:14:14,  1.24s/it] 11%|█         | 5620/50000 [2:00:17<15:15:06,  1.24s/it]                                                         {'loss': 130.6953, 'learning_rate': 1.979815407365513e-05, 'epoch': 0.87}
 11%|█         | 5620/50000 [2:00:17<15:15:06,  1.24s/it] 11%|█         | 5621/50000 [2:00:18<15:16:31,  1.24s/it] 11%|█         | 5622/50000 [2:00:20<15:17:17,  1.24s/it] 11%|█         | 5623/50000 [2:00:21<15:17:30,  1.24s/it] 11%|█         | 5624/50000 [2:00:22<15:45:13,  1.28s/it] 11%|█▏        | 5625/50000 [2:00:23<15:38:24,  1.27s/it] 11%|█▏        | 5626/50000 [2:00:25<15:32:55,  1.26s/it] 11%|█▏        | 5627/50000 [2:00:26<15:31:51,  1.26s/it] 11%|█▏        | 5628/50000 [2:00:27<15:28:23,  1.26s/it] 11%|█▏        | 5629/50000 [2:00:28<15:26:25,  1.25s/it] 11%|█▏        | 5630/50000 [2:00:30<15:24:44,  1.25s/it]                                                         {'loss': 148.9437, 'learning_rate': 1.9796829787188798e-05, 'epoch': 0.87}
 11%|█▏        | 5630/50000 [2:00:30<15:24:44,  1.25s/it] 11%|█▏        | 5631/50000 [2:00:31<15:23:23,  1.25s/it] 11%|█▏        | 5632/50000 [2:00:32<15:21:27,  1.25s/it] 11%|█▏        | 5633/50000 [2:00:33<15:20:28,  1.24s/it] 11%|█▏        | 5634/50000 [2:00:35<15:19:28,  1.24s/it] 11%|█▏        | 5635/50000 [2:00:36<15:19:02,  1.24s/it] 11%|█▏        | 5636/50000 [2:00:37<15:18:00,  1.24s/it] 11%|█▏        | 5637/50000 [2:00:38<15:19:31,  1.24s/it] 11%|█▏        | 5638/50000 [2:00:40<15:19:16,  1.24s/it] 11%|█▏        | 5639/50000 [2:00:41<15:19:28,  1.24s/it] 11%|█▏        | 5640/50000 [2:00:42<15:19:09,  1.24s/it]                                                         {'loss': 147.0969, 'learning_rate': 1.9795501215256276e-05, 'epoch': 0.87}
 11%|█▏        | 5640/50000 [2:00:42<15:19:09,  1.24s/it] 11%|█▏        | 5641/50000 [2:00:43<15:19:54,  1.24s/it] 11%|█▏        | 5642/50000 [2:00:45<15:19:08,  1.24s/it] 11%|█▏        | 5643/50000 [2:00:46<15:18:20,  1.24s/it] 11%|█▏        | 5644/50000 [2:00:47<15:17:46,  1.24s/it] 11%|█▏        | 5645/50000 [2:00:48<15:17:36,  1.24s/it] 11%|█▏        | 5646/50000 [2:00:49<15:17:22,  1.24s/it] 11%|█▏        | 5647/50000 [2:00:51<15:17:26,  1.24s/it] 11%|█▏        | 5648/50000 [2:00:52<15:17:25,  1.24s/it][2023-07-03 14:01:03,463] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 11%|█▏        | 5649/50000 [2:00:53<14:22:15,  1.17s/it] 11%|█▏        | 5650/50000 [2:00:54<14:38:26,  1.19s/it]                                                         {'loss': 119.7172, 'learning_rate': 1.979430183692371e-05, 'epoch': 0.88}
 11%|█▏        | 5650/50000 [2:00:54<14:38:26,  1.19s/it] 11%|█▏        | 5651/50000 [2:00:55<14:50:16,  1.20s/it] 11%|█▏        | 5652/50000 [2:00:57<14:58:05,  1.22s/it] 11%|█▏        | 5653/50000 [2:00:58<15:03:35,  1.22s/it] 11%|█▏        | 5654/50000 [2:00:59<15:07:20,  1.23s/it] 11%|█▏        | 5655/50000 [2:01:00<15:10:05,  1.23s/it] 11%|█▏        | 5656/50000 [2:01:02<15:12:12,  1.23s/it] 11%|█▏        | 5657/50000 [2:01:03<15:13:40,  1.24s/it] 11%|█▏        | 5658/50000 [2:01:04<15:14:32,  1.24s/it] 11%|█▏        | 5659/50000 [2:01:05<15:15:03,  1.24s/it][2023-07-03 14:01:16,857] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
 11%|█▏        | 5660/50000 [2:01:06<14:20:10,  1.16s/it]                                                         {'loss': 140.5172, 'learning_rate': 1.9793098988259215e-05, 'epoch': 0.88}
 11%|█▏        | 5660/50000 [2:01:06<14:20:10,  1.16s/it] 11%|█▏        | 5661/50000 [2:01:08<14:37:00,  1.19s/it] 11%|█▏        | 5662/50000 [2:01:09<14:48:50,  1.20s/it][2023-07-03 14:01:20,328] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
 11%|█▏        | 5663/50000 [2:01:10<14:01:54,  1.14s/it] 11%|█▏        | 5664/50000 [2:01:11<14:24:03,  1.17s/it] 11%|█▏        | 5665/50000 [2:01:12<14:40:06,  1.19s/it] 11%|█▏        | 5666/50000 [2:01:14<14:50:53,  1.21s/it] 11%|█▏        | 5667/50000 [2:01:15<14:58:18,  1.22s/it] 11%|█▏        | 5668/50000 [2:01:16<15:03:47,  1.22s/it] 11%|█▏        | 5669/50000 [2:01:17<15:07:24,  1.23s/it] 11%|█▏        | 5670/50000 [2:01:18<15:10:02,  1.23s/it]                                                         {'loss': 138.0281, 'learning_rate': 1.979189266968899e-05, 'epoch': 0.88}
 11%|█▏        | 5670/50000 [2:01:18<15:10:02,  1.23s/it] 11%|█▏        | 5671/50000 [2:01:20<15:12:12,  1.23s/it] 11%|█▏        | 5672/50000 [2:01:21<15:13:16,  1.24s/it] 11%|█▏        | 5673/50000 [2:01:22<15:14:08,  1.24s/it] 11%|█▏        | 5674/50000 [2:01:23<15:14:42,  1.24s/it] 11%|█▏        | 5675/50000 [2:01:25<15:15:10,  1.24s/it] 11%|█▏        | 5676/50000 [2:01:26<15:15:30,  1.24s/it] 11%|█▏        | 5677/50000 [2:01:27<15:16:30,  1.24s/it] 11%|█▏        | 5678/50000 [2:01:28<15:16:18,  1.24s/it] 11%|█▏        | 5679/50000 [2:01:30<15:16:20,  1.24s/it] 11%|█▏        | 5680/50000 [2:01:31<15:16:31,  1.24s/it]                                                         {'loss': 116.0555, 'learning_rate': 1.9790548246599447e-05, 'epoch': 0.88}
 11%|█▏        | 5680/50000 [2:01:31<15:16:31,  1.24s/it] 11%|█▏        | 5681/50000 [2:01:32<15:16:28,  1.24s/it] 11%|█▏        | 5682/50000 [2:01:33<15:16:30,  1.24s/it] 11%|█▏        | 5683/50000 [2:01:35<15:16:19,  1.24s/it] 11%|█▏        | 5684/50000 [2:01:36<15:16:21,  1.24s/it] 11%|█▏        | 5685/50000 [2:01:37<15:16:13,  1.24s/it] 11%|█▏        | 5686/50000 [2:01:38<15:16:05,  1.24s/it] 11%|█▏        | 5687/50000 [2:01:40<15:16:20,  1.24s/it] 11%|█▏        | 5688/50000 [2:01:41<15:16:36,  1.24s/it] 11%|█▏        | 5689/50000 [2:01:42<15:16:32,  1.24s/it] 11%|█▏        | 5690/50000 [2:01:43<15:17:56,  1.24s/it]                                                         {'loss': 143.7656, 'learning_rate': 1.9789199540791483e-05, 'epoch': 0.88}
 11%|█▏        | 5690/50000 [2:01:43<15:17:56,  1.24s/it] 11%|█▏        | 5691/50000 [2:01:45<15:18:11,  1.24s/it] 11%|█▏        | 5692/50000 [2:01:46<15:17:32,  1.24s/it] 11%|█▏        | 5693/50000 [2:01:47<15:18:52,  1.24s/it] 11%|█▏        | 5694/50000 [2:01:48<15:18:56,  1.24s/it] 11%|█▏        | 5695/50000 [2:01:50<15:18:30,  1.24s/it] 11%|█▏        | 5696/50000 [2:01:51<15:18:26,  1.24s/it] 11%|█▏        | 5697/50000 [2:01:52<15:18:16,  1.24s/it] 11%|█▏        | 5698/50000 [2:01:53<15:17:59,  1.24s/it] 11%|█▏        | 5699/50000 [2:01:55<15:17:41,  1.24s/it] 11%|█▏        | 5700/50000 [2:01:56<15:17:01,  1.24s/it]                                                         {'loss': 137.5156, 'learning_rate': 1.9787846552855053e-05, 'epoch': 0.88}
 11%|█▏        | 5700/50000 [2:01:56<15:17:01,  1.24s/it] 11%|█▏        | 5701/50000 [2:01:57<15:16:46,  1.24s/it] 11%|█▏        | 5702/50000 [2:01:58<15:16:29,  1.24s/it] 11%|█▏        | 5703/50000 [2:01:59<15:16:48,  1.24s/it] 11%|█▏        | 5704/50000 [2:02:01<15:17:17,  1.24s/it] 11%|█▏        | 5705/50000 [2:02:02<15:16:51,  1.24s/it] 11%|█▏        | 5706/50000 [2:02:03<15:17:28,  1.24s/it] 11%|█▏        | 5707/50000 [2:02:04<15:17:21,  1.24s/it] 11%|█▏        | 5708/50000 [2:02:06<15:17:29,  1.24s/it] 11%|█▏        | 5709/50000 [2:02:07<15:17:11,  1.24s/it] 11%|█▏        | 5710/50000 [2:02:08<15:17:17,  1.24s/it]                                                         {'loss': 161.3891, 'learning_rate': 1.9786489283382012e-05, 'epoch': 0.89}
 11%|█▏        | 5710/50000 [2:02:08<15:17:17,  1.24s/it] 11%|█▏        | 5711/50000 [2:02:09<15:17:15,  1.24s/it] 11%|█▏        | 5712/50000 [2:02:11<15:17:21,  1.24s/it] 11%|█▏        | 5713/50000 [2:02:12<15:16:44,  1.24s/it] 11%|█▏        | 5714/50000 [2:02:13<15:16:13,  1.24s/it] 11%|█▏        | 5715/50000 [2:02:14<15:16:02,  1.24s/it] 11%|█▏        | 5716/50000 [2:02:16<15:15:43,  1.24s/it] 11%|█▏        | 5717/50000 [2:02:17<15:15:50,  1.24s/it] 11%|█▏        | 5718/50000 [2:02:18<15:16:23,  1.24s/it] 11%|█▏        | 5719/50000 [2:02:19<15:16:12,  1.24s/it] 11%|█▏        | 5720/50000 [2:02:21<15:15:47,  1.24s/it]                                                         {'loss': 136.4344, 'learning_rate': 1.9785127732966068e-05, 'epoch': 0.89}
 11%|█▏        | 5720/50000 [2:02:21<15:15:47,  1.24s/it] 11%|█▏        | 5721/50000 [2:02:22<15:15:57,  1.24s/it] 11%|█▏        | 5722/50000 [2:02:23<15:15:47,  1.24s/it] 11%|█▏        | 5723/50000 [2:02:24<15:15:32,  1.24s/it] 11%|█▏        | 5724/50000 [2:02:26<15:15:25,  1.24s/it] 11%|█▏        | 5725/50000 [2:02:27<15:15:16,  1.24s/it] 11%|█▏        | 5726/50000 [2:02:28<15:15:29,  1.24s/it] 11%|█▏        | 5727/50000 [2:02:29<15:15:16,  1.24s/it] 11%|█▏        | 5728/50000 [2:02:31<15:15:05,  1.24s/it] 11%|█▏        | 5729/50000 [2:02:32<15:14:50,  1.24s/it] 11%|█▏        | 5730/50000 [2:02:33<15:14:39,  1.24s/it]                                                         {'loss': 131.6609, 'learning_rate': 1.9783761902202814e-05, 'epoch': 0.89}
 11%|█▏        | 5730/50000 [2:02:33<15:14:39,  1.24s/it] 11%|█▏        | 5731/50000 [2:02:34<15:15:28,  1.24s/it] 11%|█▏        | 5732/50000 [2:02:35<15:15:30,  1.24s/it] 11%|█▏        | 5733/50000 [2:02:37<15:15:14,  1.24s/it] 11%|█▏        | 5734/50000 [2:02:38<15:15:00,  1.24s/it] 11%|█▏        | 5735/50000 [2:02:39<15:14:57,  1.24s/it] 11%|█▏        | 5736/50000 [2:02:40<15:14:48,  1.24s/it] 11%|█▏        | 5737/50000 [2:02:42<15:14:44,  1.24s/it] 11%|█▏        | 5738/50000 [2:02:43<15:15:03,  1.24s/it] 11%|█▏        | 5739/50000 [2:02:44<15:14:56,  1.24s/it] 11%|█▏        | 5740/50000 [2:02:45<15:15:06,  1.24s/it]                                                         {'loss': 137.1734, 'learning_rate': 1.9782391791689713e-05, 'epoch': 0.89}
 11%|█▏        | 5740/50000 [2:02:45<15:15:06,  1.24s/it] 11%|█▏        | 5741/50000 [2:02:47<15:16:12,  1.24s/it] 11%|█▏        | 5742/50000 [2:02:48<15:16:02,  1.24s/it] 11%|█▏        | 5743/50000 [2:02:49<15:15:32,  1.24s/it] 11%|█▏        | 5744/50000 [2:02:50<15:15:12,  1.24s/it] 11%|█▏        | 5745/50000 [2:02:52<15:14:54,  1.24s/it] 11%|█▏        | 5746/50000 [2:02:53<15:15:06,  1.24s/it] 11%|█▏        | 5747/50000 [2:02:54<15:15:00,  1.24s/it] 11%|█▏        | 5748/50000 [2:02:55<15:14:47,  1.24s/it] 11%|█▏        | 5749/50000 [2:02:57<15:15:20,  1.24s/it] 12%|█▏        | 5750/50000 [2:02:58<15:15:02,  1.24s/it]                                                         {'loss': 132.7656, 'learning_rate': 1.9781017402026088e-05, 'epoch': 0.89}
 12%|█▏        | 5750/50000 [2:02:58<15:15:02,  1.24s/it] 12%|█▏        | 5751/50000 [2:02:59<15:15:14,  1.24s/it] 12%|█▏        | 5752/50000 [2:03:00<15:14:51,  1.24s/it] 12%|█▏        | 5753/50000 [2:03:02<15:14:56,  1.24s/it] 12%|█▏        | 5754/50000 [2:03:03<15:14:48,  1.24s/it] 12%|█▏        | 5755/50000 [2:03:04<15:14:29,  1.24s/it] 12%|█▏        | 5756/50000 [2:03:05<15:14:51,  1.24s/it] 12%|█▏        | 5757/50000 [2:03:06<15:14:39,  1.24s/it] 12%|█▏        | 5758/50000 [2:03:08<15:14:57,  1.24s/it] 12%|█▏        | 5759/50000 [2:03:09<15:14:40,  1.24s/it] 12%|█▏        | 5760/50000 [2:03:10<15:14:32,  1.24s/it]                                                         {'loss': 153.8531, 'learning_rate': 1.9779638733813153e-05, 'epoch': 0.89}
 12%|█▏        | 5760/50000 [2:03:10<15:14:32,  1.24s/it] 12%|█▏        | 5761/50000 [2:03:11<15:14:41,  1.24s/it] 12%|█▏        | 5762/50000 [2:03:13<15:14:39,  1.24s/it] 12%|█▏        | 5763/50000 [2:03:14<15:15:11,  1.24s/it] 12%|█▏        | 5764/50000 [2:03:15<15:14:49,  1.24s/it] 12%|█▏        | 5765/50000 [2:03:16<15:14:23,  1.24s/it] 12%|█▏        | 5766/50000 [2:03:18<15:14:15,  1.24s/it] 12%|█▏        | 5767/50000 [2:03:19<15:14:21,  1.24s/it] 12%|█▏        | 5768/50000 [2:03:20<15:14:21,  1.24s/it] 12%|█▏        | 5769/50000 [2:03:21<15:14:08,  1.24s/it] 12%|█▏        | 5770/50000 [2:03:23<15:14:00,  1.24s/it]                                                         {'loss': 160.1578, 'learning_rate': 1.977825578765398e-05, 'epoch': 0.89}
 12%|█▏        | 5770/50000 [2:03:23<15:14:00,  1.24s/it] 12%|█▏        | 5771/50000 [2:03:24<15:14:12,  1.24s/it] 12%|█▏        | 5772/50000 [2:03:25<15:14:42,  1.24s/it] 12%|█▏        | 5773/50000 [2:03:26<15:14:24,  1.24s/it] 12%|█▏        | 5774/50000 [2:03:28<15:14:13,  1.24s/it] 12%|█▏        | 5775/50000 [2:03:29<15:14:01,  1.24s/it] 12%|█▏        | 5776/50000 [2:03:30<15:14:18,  1.24s/it] 12%|█▏        | 5777/50000 [2:03:31<15:13:58,  1.24s/it] 12%|█▏        | 5778/50000 [2:03:33<15:13:56,  1.24s/it] 12%|█▏        | 5779/50000 [2:03:34<15:13:58,  1.24s/it] 12%|█▏        | 5780/50000 [2:03:35<15:13:55,  1.24s/it]                                                         {'loss': 173.2203, 'learning_rate': 1.9776868564153517e-05, 'epoch': 0.9}
 12%|█▏        | 5780/50000 [2:03:35<15:13:55,  1.24s/it] 12%|█▏        | 5781/50000 [2:03:36<15:14:00,  1.24s/it] 12%|█▏        | 5782/50000 [2:03:37<15:14:03,  1.24s/it] 12%|█▏        | 5783/50000 [2:03:39<15:13:54,  1.24s/it] 12%|█▏        | 5784/50000 [2:03:40<15:13:30,  1.24s/it] 12%|█▏        | 5785/50000 [2:03:41<15:13:58,  1.24s/it] 12%|█▏        | 5786/50000 [2:03:42<15:14:04,  1.24s/it] 12%|█▏        | 5787/50000 [2:03:44<15:14:17,  1.24s/it] 12%|█▏        | 5788/50000 [2:03:45<15:14:13,  1.24s/it] 12%|█▏        | 5789/50000 [2:03:46<15:14:12,  1.24s/it] 12%|█▏        | 5790/50000 [2:03:47<15:14:11,  1.24s/it]                                                         {'loss': 158.2406, 'learning_rate': 1.9775477063918585e-05, 'epoch': 0.9}
 12%|█▏        | 5790/50000 [2:03:47<15:14:11,  1.24s/it] 12%|█▏        | 5791/50000 [2:03:49<15:14:27,  1.24s/it] 12%|█▏        | 5792/50000 [2:03:50<15:14:47,  1.24s/it] 12%|█▏        | 5793/50000 [2:03:51<15:14:17,  1.24s/it] 12%|█▏        | 5794/50000 [2:03:52<15:14:09,  1.24s/it] 12%|█▏        | 5795/50000 [2:03:54<15:13:56,  1.24s/it] 12%|█▏        | 5796/50000 [2:03:55<15:14:13,  1.24s/it] 12%|█▏        | 5797/50000 [2:03:56<15:13:53,  1.24s/it] 12%|█▏        | 5798/50000 [2:03:57<15:13:52,  1.24s/it] 12%|█▏        | 5799/50000 [2:03:59<15:14:20,  1.24s/it] 12%|█▏        | 5800/50000 [2:04:00<15:32:35,  1.27s/it]                                                         {'loss': 138.0031, 'learning_rate': 1.9774081287557873e-05, 'epoch': 0.9}
 12%|█▏        | 5800/50000 [2:04:00<15:32:35,  1.27s/it] 12%|█▏        | 5801/50000 [2:04:01<15:27:21,  1.26s/it] 12%|█▏        | 5802/50000 [2:04:02<15:23:30,  1.25s/it] 12%|█▏        | 5803/50000 [2:04:04<15:21:10,  1.25s/it] 12%|█▏        | 5804/50000 [2:04:05<15:19:26,  1.25s/it] 12%|█▏        | 5805/50000 [2:04:06<15:17:37,  1.25s/it] 12%|█▏        | 5806/50000 [2:04:07<15:16:18,  1.24s/it] 12%|█▏        | 5807/50000 [2:04:09<15:15:35,  1.24s/it] 12%|█▏        | 5808/50000 [2:04:10<15:14:53,  1.24s/it] 12%|█▏        | 5809/50000 [2:04:11<15:14:13,  1.24s/it] 12%|█▏        | 5810/50000 [2:04:12<15:13:46,  1.24s/it]                                                         {'loss': 157.2188, 'learning_rate': 1.9772681235681936e-05, 'epoch': 0.9}
 12%|█▏        | 5810/50000 [2:04:12<15:13:46,  1.24s/it] 12%|█▏        | 5811/50000 [2:04:14<15:13:52,  1.24s/it] 12%|█▏        | 5812/50000 [2:04:15<15:13:40,  1.24s/it] 12%|█▏        | 5813/50000 [2:04:16<15:13:26,  1.24s/it] 12%|█▏        | 5814/50000 [2:04:17<15:13:19,  1.24s/it] 12%|█▏        | 5815/50000 [2:04:19<15:13:19,  1.24s/it] 12%|█▏        | 5816/50000 [2:04:20<15:13:03,  1.24s/it] 12%|█▏        | 5817/50000 [2:04:21<15:13:22,  1.24s/it] 12%|█▏        | 5818/50000 [2:04:22<15:13:25,  1.24s/it] 12%|█▏        | 5819/50000 [2:04:23<15:13:28,  1.24s/it] 12%|█▏        | 5820/50000 [2:04:25<15:13:39,  1.24s/it]                                                         {'loss': 161.9633, 'learning_rate': 1.977127690890321e-05, 'epoch': 0.9}
 12%|█▏        | 5820/50000 [2:04:25<15:13:39,  1.24s/it] 12%|█▏        | 5821/50000 [2:04:26<15:13:45,  1.24s/it] 12%|█▏        | 5822/50000 [2:04:27<15:13:09,  1.24s/it] 12%|█▏        | 5823/50000 [2:04:28<15:13:06,  1.24s/it] 12%|█▏        | 5824/50000 [2:04:30<15:13:01,  1.24s/it] 12%|█▏        | 5825/50000 [2:04:31<15:13:07,  1.24s/it] 12%|█▏        | 5826/50000 [2:04:32<15:13:05,  1.24s/it] 12%|█▏        | 5827/50000 [2:04:33<15:12:58,  1.24s/it] 12%|█▏        | 5828/50000 [2:04:35<15:12:58,  1.24s/it] 12%|█▏        | 5829/50000 [2:04:36<15:13:04,  1.24s/it] 12%|█▏        | 5830/50000 [2:04:37<15:13:03,  1.24s/it]                                                         {'loss': 142.2281, 'learning_rate': 1.9769868307835996e-05, 'epoch': 0.9}
 12%|█▏        | 5830/50000 [2:04:37<15:13:03,  1.24s/it] 12%|█▏        | 5831/50000 [2:04:38<15:13:06,  1.24s/it] 12%|█▏        | 5832/50000 [2:04:40<15:13:19,  1.24s/it] 12%|█▏        | 5833/50000 [2:04:41<15:13:13,  1.24s/it] 12%|█▏        | 5834/50000 [2:04:42<15:12:59,  1.24s/it] 12%|█▏        | 5835/50000 [2:04:43<15:12:50,  1.24s/it] 12%|█▏        | 5836/50000 [2:04:45<15:13:03,  1.24s/it] 12%|█▏        | 5837/50000 [2:04:46<15:12:57,  1.24s/it] 12%|█▏        | 5838/50000 [2:04:47<15:13:00,  1.24s/it] 12%|█▏        | 5839/50000 [2:04:48<15:12:56,  1.24s/it] 12%|█▏        | 5840/50000 [2:04:50<15:13:26,  1.24s/it]                                                         {'loss': 137.8516, 'learning_rate': 1.976845543309646e-05, 'epoch': 0.91}
 12%|█▏        | 5840/50000 [2:04:50<15:13:26,  1.24s/it] 12%|█▏        | 5841/50000 [2:04:51<15:13:35,  1.24s/it] 12%|█▏        | 5842/50000 [2:04:52<15:13:24,  1.24s/it] 12%|█▏        | 5843/50000 [2:04:53<15:13:05,  1.24s/it] 12%|█▏        | 5844/50000 [2:04:54<15:13:07,  1.24s/it] 12%|█▏        | 5845/50000 [2:04:56<15:13:28,  1.24s/it] 12%|█▏        | 5846/50000 [2:04:57<15:13:02,  1.24s/it] 12%|█▏        | 5847/50000 [2:04:58<15:12:43,  1.24s/it] 12%|█▏        | 5848/50000 [2:04:59<15:12:46,  1.24s/it] 12%|█▏        | 5849/50000 [2:05:01<15:12:41,  1.24s/it] 12%|█▏        | 5850/50000 [2:05:02<15:13:12,  1.24s/it]                                                         {'loss': 120.0938, 'learning_rate': 1.976703828530264e-05, 'epoch': 0.91}
 12%|█▏        | 5850/50000 [2:05:02<15:13:12,  1.24s/it] 12%|█▏        | 5851/50000 [2:05:03<15:13:57,  1.24s/it] 12%|█▏        | 5852/50000 [2:05:04<15:13:49,  1.24s/it] 12%|█▏        | 5853/50000 [2:05:06<15:15:10,  1.24s/it] 12%|█▏        | 5854/50000 [2:05:07<15:29:26,  1.26s/it] 12%|█▏        | 5855/50000 [2:05:08<15:25:16,  1.26s/it] 12%|█▏        | 5856/50000 [2:05:09<15:23:57,  1.26s/it] 12%|█▏        | 5857/50000 [2:05:11<15:21:10,  1.25s/it] 12%|█▏        | 5858/50000 [2:05:12<15:19:03,  1.25s/it] 12%|█▏        | 5859/50000 [2:05:13<15:16:51,  1.25s/it] 12%|█▏        | 5860/50000 [2:05:14<15:16:14,  1.25s/it]                                                         {'loss': 133.0781, 'learning_rate': 1.9765616865074447e-05, 'epoch': 0.91}
 12%|█▏        | 5860/50000 [2:05:14<15:16:14,  1.25s/it] 12%|█▏        | 5861/50000 [2:05:16<15:15:26,  1.24s/it] 12%|█▏        | 5862/50000 [2:05:17<15:14:21,  1.24s/it] 12%|█▏        | 5863/50000 [2:05:18<15:13:36,  1.24s/it] 12%|█▏        | 5864/50000 [2:05:19<15:14:15,  1.24s/it] 12%|█▏        | 5865/50000 [2:05:21<15:13:42,  1.24s/it] 12%|█▏        | 5866/50000 [2:05:22<15:13:29,  1.24s/it] 12%|█▏        | 5867/50000 [2:05:23<15:12:38,  1.24s/it] 12%|█▏        | 5868/50000 [2:05:24<15:16:24,  1.25s/it] 12%|█▏        | 5869/50000 [2:05:26<15:15:37,  1.24s/it] 12%|█▏        | 5870/50000 [2:05:27<15:14:59,  1.24s/it]                                                         {'loss': 134.0, 'learning_rate': 1.976419117303366e-05, 'epoch': 0.91}
 12%|█▏        | 5870/50000 [2:05:27<15:14:59,  1.24s/it] 12%|█▏        | 5871/50000 [2:05:28<15:14:51,  1.24s/it] 12%|█▏        | 5872/50000 [2:05:29<15:14:06,  1.24s/it] 12%|█▏        | 5873/50000 [2:05:31<15:13:49,  1.24s/it] 12%|█▏        | 5874/50000 [2:05:32<15:13:44,  1.24s/it] 12%|█▏        | 5875/50000 [2:05:33<15:13:29,  1.24s/it] 12%|█▏        | 5876/50000 [2:05:34<15:13:03,  1.24s/it] 12%|█▏        | 5877/50000 [2:05:36<15:13:14,  1.24s/it] 12%|█▏        | 5878/50000 [2:05:37<15:13:09,  1.24s/it] 12%|█▏        | 5879/50000 [2:05:38<15:13:02,  1.24s/it] 12%|█▏        | 5880/50000 [2:05:39<15:12:44,  1.24s/it]                                                         {'loss': 141.65, 'learning_rate': 1.976276120980393e-05, 'epoch': 0.91}
 12%|█▏        | 5880/50000 [2:05:39<15:12:44,  1.24s/it] 12%|█▏        | 5881/50000 [2:05:41<15:13:09,  1.24s/it] 12%|█▏        | 5882/50000 [2:05:42<15:13:05,  1.24s/it] 12%|█▏        | 5883/50000 [2:05:43<15:12:51,  1.24s/it] 12%|█▏        | 5884/50000 [2:05:44<15:12:40,  1.24s/it] 12%|█▏        | 5885/50000 [2:05:46<15:12:32,  1.24s/it] 12%|█▏        | 5886/50000 [2:05:47<15:12:21,  1.24s/it] 12%|█▏        | 5887/50000 [2:05:48<15:12:25,  1.24s/it] 12%|█▏        | 5888/50000 [2:05:49<15:12:20,  1.24s/it] 12%|█▏        | 5889/50000 [2:05:50<15:12:14,  1.24s/it] 12%|█▏        | 5890/50000 [2:05:52<15:12:18,  1.24s/it]                                                         {'loss': 145.9469, 'learning_rate': 1.976132697601076e-05, 'epoch': 0.91}
 12%|█▏        | 5890/50000 [2:05:52<15:12:18,  1.24s/it] 12%|█▏        | 5891/50000 [2:05:53<15:12:43,  1.24s/it] 12%|█▏        | 5892/50000 [2:05:54<15:12:26,  1.24s/it] 12%|█▏        | 5893/50000 [2:05:55<15:12:20,  1.24s/it] 12%|█▏        | 5894/50000 [2:05:57<15:12:18,  1.24s/it] 12%|█▏        | 5895/50000 [2:05:58<15:12:16,  1.24s/it] 12%|█▏        | 5896/50000 [2:05:59<15:11:58,  1.24s/it] 12%|█▏        | 5897/50000 [2:06:00<15:11:43,  1.24s/it] 12%|█▏        | 5898/50000 [2:06:02<15:11:51,  1.24s/it] 12%|█▏        | 5899/50000 [2:06:03<15:11:47,  1.24s/it] 12%|█▏        | 5900/50000 [2:06:04<15:11:58,  1.24s/it]                                                         {'loss': 132.4469, 'learning_rate': 1.9759888472281537e-05, 'epoch': 0.91}
 12%|█▏        | 5900/50000 [2:06:04<15:11:58,  1.24s/it] 12%|█▏        | 5901/50000 [2:06:05<15:12:19,  1.24s/it] 12%|█▏        | 5902/50000 [2:06:07<15:12:13,  1.24s/it] 12%|█▏        | 5903/50000 [2:06:08<15:12:08,  1.24s/it] 12%|█▏        | 5904/50000 [2:06:09<15:12:26,  1.24s/it] 12%|█▏        | 5905/50000 [2:06:10<15:12:57,  1.24s/it] 12%|█▏        | 5906/50000 [2:06:12<15:13:18,  1.24s/it] 12%|█▏        | 5907/50000 [2:06:13<15:14:17,  1.24s/it] 12%|█▏        | 5908/50000 [2:06:14<15:14:43,  1.24s/it] 12%|█▏        | 5909/50000 [2:06:15<15:14:26,  1.24s/it] 12%|█▏        | 5910/50000 [2:06:17<15:13:49,  1.24s/it]                                                         {'loss': 155.9688, 'learning_rate': 1.9758445699245515e-05, 'epoch': 0.92}
 12%|█▏        | 5910/50000 [2:06:17<15:13:49,  1.24s/it] 12%|█▏        | 5911/50000 [2:06:18<15:13:18,  1.24s/it] 12%|█▏        | 5912/50000 [2:06:19<15:12:27,  1.24s/it] 12%|█▏        | 5913/50000 [2:06:20<15:12:02,  1.24s/it] 12%|█▏        | 5914/50000 [2:06:22<15:11:59,  1.24s/it] 12%|█▏        | 5915/50000 [2:06:23<15:13:09,  1.24s/it] 12%|█▏        | 5916/50000 [2:06:24<15:12:48,  1.24s/it] 12%|█▏        | 5917/50000 [2:06:25<15:28:17,  1.26s/it] 12%|█▏        | 5918/50000 [2:06:27<15:48:47,  1.29s/it] 12%|█▏        | 5919/50000 [2:06:28<15:37:43,  1.28s/it] 12%|█▏        | 5920/50000 [2:06:29<15:31:08,  1.27s/it]                                                         {'loss': 128.2547, 'learning_rate': 1.9756998657533807e-05, 'epoch': 0.92}
 12%|█▏        | 5920/50000 [2:06:29<15:31:08,  1.27s/it] 12%|█▏        | 5921/50000 [2:06:30<15:26:31,  1.26s/it] 12%|█▏        | 5922/50000 [2:06:32<15:21:58,  1.26s/it] 12%|█▏        | 5923/50000 [2:06:33<15:18:34,  1.25s/it] 12%|█▏        | 5924/50000 [2:06:34<15:16:12,  1.25s/it] 12%|█▏        | 5925/50000 [2:06:35<15:15:17,  1.25s/it] 12%|█▏        | 5926/50000 [2:06:37<15:14:56,  1.25s/it] 12%|█▏        | 5927/50000 [2:06:38<15:14:29,  1.24s/it] 12%|█▏        | 5928/50000 [2:06:39<15:13:21,  1.24s/it] 12%|█▏        | 5929/50000 [2:06:40<15:13:05,  1.24s/it] 12%|█▏        | 5930/50000 [2:06:42<15:12:56,  1.24s/it]                                                         {'loss': 131.9141, 'learning_rate': 1.9755547347779405e-05, 'epoch': 0.92}
 12%|█▏        | 5930/50000 [2:06:42<15:12:56,  1.24s/it] 12%|█▏        | 5931/50000 [2:06:43<15:13:06,  1.24s/it] 12%|█▏        | 5932/50000 [2:06:44<15:12:38,  1.24s/it] 12%|█▏        | 5933/50000 [2:06:45<15:12:14,  1.24s/it] 12%|█▏        | 5934/50000 [2:06:47<15:12:47,  1.24s/it] 12%|█▏        | 5935/50000 [2:06:48<15:13:03,  1.24s/it] 12%|█▏        | 5936/50000 [2:06:49<15:12:44,  1.24s/it] 12%|█▏        | 5937/50000 [2:06:50<15:13:27,  1.24s/it] 12%|█▏        | 5938/50000 [2:06:52<15:12:54,  1.24s/it] 12%|█▏        | 5939/50000 [2:06:53<15:12:24,  1.24s/it] 12%|█▏        | 5940/50000 [2:06:54<15:11:42,  1.24s/it]                                                         {'loss': 145.5203, 'learning_rate': 1.9754091770617155e-05, 'epoch': 0.92}
 12%|█▏        | 5940/50000 [2:06:54<15:11:42,  1.24s/it] 12%|█▏        | 5941/50000 [2:06:55<15:11:19,  1.24s/it] 12%|█▏        | 5942/50000 [2:06:56<15:11:06,  1.24s/it][2023-07-03 14:07:07,987] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, but hysteresis is 2. Reducing hysteresis to 1
 12%|█▏        | 5943/50000 [2:06:57<14:16:00,  1.17s/it] 12%|█▏        | 5944/50000 [2:06:59<14:32:25,  1.19s/it] 12%|█▏        | 5945/50000 [2:07:00<14:44:39,  1.20s/it] 12%|█▏        | 5946/50000 [2:07:01<15:09:39,  1.24s/it] 12%|█▏        | 5947/50000 [2:07:03<15:10:22,  1.24s/it] 12%|█▏        | 5948/50000 [2:07:04<15:11:04,  1.24s/it] 12%|█▏        | 5949/50000 [2:07:05<15:11:28,  1.24s/it] 12%|█▏        | 5950/50000 [2:07:06<15:13:02,  1.24s/it]                                                         {'loss': 143.475, 'learning_rate': 1.9752778103063626e-05, 'epoch': 0.92}
 12%|█▏        | 5950/50000 [2:07:06<15:13:02,  1.24s/it] 12%|█▏        | 5951/50000 [2:07:08<15:14:14,  1.25s/it] 12%|█▏        | 5952/50000 [2:07:09<15:13:46,  1.24s/it] 12%|█▏        | 5953/50000 [2:07:10<15:13:38,  1.24s/it] 12%|█▏        | 5954/50000 [2:07:11<15:13:05,  1.24s/it] 12%|█▏        | 5955/50000 [2:07:13<15:36:17,  1.28s/it] 12%|█▏        | 5956/50000 [2:07:14<15:29:01,  1.27s/it] 12%|█▏        | 5957/50000 [2:07:15<15:24:41,  1.26s/it] 12%|█▏        | 5958/50000 [2:07:16<15:21:27,  1.26s/it] 12%|█▏        | 5959/50000 [2:07:18<15:18:47,  1.25s/it] 12%|█▏        | 5960/50000 [2:07:19<15:17:39,  1.25s/it]                                                         {'loss': 152.1734, 'learning_rate': 1.9751314419582175e-05, 'epoch': 0.92}
 12%|█▏        | 5960/50000 [2:07:19<15:17:39,  1.25s/it] 12%|█▏        | 5961/50000 [2:07:20<15:16:42,  1.25s/it] 12%|█▏        | 5962/50000 [2:07:21<15:15:36,  1.25s/it] 12%|█▏        | 5963/50000 [2:07:23<15:14:27,  1.25s/it] 12%|█▏        | 5964/50000 [2:07:24<15:14:33,  1.25s/it] 12%|█▏        | 5965/50000 [2:07:25<15:13:36,  1.24s/it] 12%|█▏        | 5966/50000 [2:07:26<15:12:47,  1.24s/it] 12%|█▏        | 5967/50000 [2:07:28<15:12:08,  1.24s/it] 12%|█▏        | 5968/50000 [2:07:29<15:11:41,  1.24s/it] 12%|█▏        | 5969/50000 [2:07:30<15:11:26,  1.24s/it] 12%|█▏        | 5970/50000 [2:07:31<15:11:04,  1.24s/it]                                                         {'loss': 132.6562, 'learning_rate': 1.97498464705445e-05, 'epoch': 0.93}
 12%|█▏        | 5970/50000 [2:07:31<15:11:04,  1.24s/it] 12%|█▏        | 5971/50000 [2:07:32<15:11:01,  1.24s/it] 12%|█▏        | 5972/50000 [2:07:34<15:10:47,  1.24s/it] 12%|█▏        | 5973/50000 [2:07:35<15:10:54,  1.24s/it] 12%|█▏        | 5974/50000 [2:07:36<15:10:31,  1.24s/it] 12%|█▏        | 5975/50000 [2:07:37<15:10:46,  1.24s/it] 12%|█▏        | 5976/50000 [2:07:39<15:10:39,  1.24s/it] 12%|█▏        | 5977/50000 [2:07:40<15:10:32,  1.24s/it] 12%|█▏        | 5978/50000 [2:07:41<15:10:19,  1.24s/it] 12%|█▏        | 5979/50000 [2:07:42<15:10:06,  1.24s/it] 12%|█▏        | 5980/50000 [2:07:44<15:10:11,  1.24s/it]                                                         {'loss': 150.1234, 'learning_rate': 1.974837425659274e-05, 'epoch': 0.93}
 12%|█▏        | 5980/50000 [2:07:44<15:10:11,  1.24s/it] 12%|█▏        | 5981/50000 [2:07:45<15:10:42,  1.24s/it] 12%|█▏        | 5982/50000 [2:07:46<15:10:20,  1.24s/it] 12%|█▏        | 5983/50000 [2:07:47<15:10:12,  1.24s/it] 12%|█▏        | 5984/50000 [2:07:49<15:10:07,  1.24s/it] 12%|█▏        | 5985/50000 [2:07:50<15:10:19,  1.24s/it] 12%|█▏        | 5986/50000 [2:07:51<15:10:18,  1.24s/it] 12%|█▏        | 5987/50000 [2:07:52<15:10:02,  1.24s/it] 12%|█▏        | 5988/50000 [2:07:54<15:09:48,  1.24s/it] 12%|█▏        | 5989/50000 [2:07:55<15:09:45,  1.24s/it] 12%|█▏        | 5990/50000 [2:07:56<15:09:50,  1.24s/it]                                                         {'loss': 134.9859, 'learning_rate': 1.974689777837088e-05, 'epoch': 0.93}
 12%|█▏        | 5990/50000 [2:07:56<15:09:50,  1.24s/it] 12%|█▏        | 5991/50000 [2:07:57<15:10:02,  1.24s/it] 12%|█▏        | 5992/50000 [2:07:59<15:09:49,  1.24s/it] 12%|█▏        | 5993/50000 [2:08:00<15:09:32,  1.24s/it] 12%|█▏        | 5994/50000 [2:08:01<15:09:18,  1.24s/it] 12%|█▏        | 5995/50000 [2:08:02<15:09:23,  1.24s/it] 12%|█▏        | 5996/50000 [2:08:03<15:09:29,  1.24s/it] 12%|█▏        | 5997/50000 [2:08:05<15:09:18,  1.24s/it] 12%|█▏        | 5998/50000 [2:08:06<15:09:20,  1.24s/it] 12%|█▏        | 5999/50000 [2:08:07<15:09:30,  1.24s/it][2023-07-03 14:08:18,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=81, lr=[1.9745417036524795e-05], mom=[(0.9, 0.999)]
[2023-07-03 14:08:18,958] [INFO] [timer.py:199:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=3.2491445659002927, CurrSamplesPerSec=3.2489497740946796, MemAllocated=47.21GB, MaxMemAllocated=59.8GB
 12%|█▏        | 6000/50000 [2:08:08<15:09:27,  1.24s/it]                                                         {'loss': 138.0797, 'learning_rate': 1.9745417036524795e-05, 'epoch': 0.93}
 12%|█▏        | 6000/50000 [2:08:08<15:09:27,  1.24s/it][INFO|trainer.py:3129] 2023-07-03 14:08:18,965 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 14:08:18,965 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 14:08:18,965 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                         
                                             [A{'eval_loss': 148.5, 'eval_accuracy': 0.3954538612072859, 'eval_runtime': 3.108, 'eval_samples_per_second': 8.366, 'eval_steps_per_second': 2.252, 'epoch': 0.93}
 12%|█▏        | 6000/50000 [2:08:12<15:09:27,  1.24s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 14:08:22,075 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000
[INFO|trainer.py:2880] 2023-07-03 14:08:22,086 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 14:08:32,631 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 14:08:32,632 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/special_tokens_map.json
[2023-07-03 14:08:32,634] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 14:08:32,657] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/global_step6000/mp_rank_00_model_states.pt
[2023-07-03 14:08:32,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/global_step6000/mp_rank_00_model_states.pt...
[2023-07-03 14:08:43,634] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/global_step6000/mp_rank_00_model_states.pt.
[2023-07-03 14:08:43,637] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 14:09:01,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 14:09:01,267] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 14:09:01,267] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 14:09:01,280 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-5000] due to args.save_total_limit
 12%|█▏        | 6001/50000 [2:09:01<203:36:25, 16.66s/it] 12%|█▏        | 6002/50000 [2:09:02<147:04:42, 12.03s/it] 12%|█▏        | 6003/50000 [2:09:04<107:55:48,  8.83s/it] 12%|█▏        | 6004/50000 [2:09:05<80:05:45,  6.55s/it]  12%|█▏        | 6005/50000 [2:09:06<60:37:02,  4.96s/it] 12%|█▏        | 6006/50000 [2:09:07<46:58:31,  3.84s/it] 12%|█▏        | 6007/50000 [2:09:09<37:42:34,  3.09s/it] 12%|█▏        | 6008/50000 [2:09:10<30:57:00,  2.53s/it] 12%|█▏        | 6009/50000 [2:09:11<26:12:32,  2.14s/it] 12%|█▏        | 6010/50000 [2:09:12<22:53:24,  1.87s/it]                                                         {'loss': 146.1172, 'learning_rate': 1.9743932031702204e-05, 'epoch': 0.93}
 12%|█▏        | 6010/50000 [2:09:12<22:53:24,  1.87s/it] 12%|█▏        | 6011/50000 [2:09:14<20:34:03,  1.68s/it] 12%|█▏        | 6012/50000 [2:09:15<18:57:16,  1.55s/it] 12%|█▏        | 6013/50000 [2:09:16<17:48:52,  1.46s/it] 12%|█▏        | 6014/50000 [2:09:17<17:00:47,  1.39s/it] 12%|█▏        | 6015/50000 [2:09:19<16:26:53,  1.35s/it] 12%|█▏        | 6016/50000 [2:09:20<16:03:11,  1.31s/it] 12%|█▏        | 6017/50000 [2:09:21<15:46:41,  1.29s/it] 12%|█▏        | 6018/50000 [2:09:22<15:35:14,  1.28s/it] 12%|█▏        | 6019/50000 [2:09:24<15:26:55,  1.26s/it] 12%|█▏        | 6020/50000 [2:09:25<15:21:13,  1.26s/it]                                                         {'loss': 138.7328, 'learning_rate': 1.97424427645527e-05, 'epoch': 0.93}
 12%|█▏        | 6020/50000 [2:09:25<15:21:13,  1.26s/it] 12%|█▏        | 6021/50000 [2:09:26<15:17:41,  1.25s/it] 12%|█▏        | 6022/50000 [2:09:27<15:14:35,  1.25s/it] 12%|█▏        | 6023/50000 [2:09:29<15:12:30,  1.24s/it] 12%|█▏        | 6024/50000 [2:09:30<15:10:59,  1.24s/it] 12%|█▏        | 6025/50000 [2:09:31<15:09:52,  1.24s/it] 12%|█▏        | 6026/50000 [2:09:32<15:09:10,  1.24s/it] 12%|█▏        | 6027/50000 [2:09:34<15:08:33,  1.24s/it] 12%|█▏        | 6028/50000 [2:09:35<15:08:09,  1.24s/it] 12%|█▏        | 6029/50000 [2:09:36<15:07:53,  1.24s/it] 12%|█▏        | 6030/50000 [2:09:37<15:08:00,  1.24s/it]                                                         {'loss': 139.7094, 'learning_rate': 1.974094923572774e-05, 'epoch': 0.93}
 12%|█▏        | 6030/50000 [2:09:37<15:08:00,  1.24s/it] 12%|█▏        | 6031/50000 [2:09:38<15:08:07,  1.24s/it] 12%|█▏        | 6032/50000 [2:09:40<15:08:17,  1.24s/it] 12%|█▏        | 6033/50000 [2:09:41<15:08:08,  1.24s/it] 12%|█▏        | 6034/50000 [2:09:42<15:07:53,  1.24s/it] 12%|█▏        | 6035/50000 [2:09:43<15:07:37,  1.24s/it] 12%|█▏        | 6036/50000 [2:09:45<15:07:34,  1.24s/it] 12%|█▏        | 6037/50000 [2:09:46<15:07:33,  1.24s/it] 12%|█▏        | 6038/50000 [2:09:47<15:07:34,  1.24s/it] 12%|█▏        | 6039/50000 [2:09:48<15:07:40,  1.24s/it] 12%|█▏        | 6040/50000 [2:09:50<15:07:31,  1.24s/it]                                                         {'loss': 145.3828, 'learning_rate': 1.9739451445880646e-05, 'epoch': 0.94}
 12%|█▏        | 6040/50000 [2:09:50<15:07:31,  1.24s/it] 12%|█▏        | 6041/50000 [2:09:51<15:08:09,  1.24s/it] 12%|█▏        | 6042/50000 [2:09:52<15:08:23,  1.24s/it] 12%|█▏        | 6043/50000 [2:09:53<15:08:33,  1.24s/it] 12%|█▏        | 6044/50000 [2:09:55<15:08:39,  1.24s/it] 12%|█▏        | 6045/50000 [2:09:56<15:08:38,  1.24s/it] 12%|█▏        | 6046/50000 [2:09:57<15:08:48,  1.24s/it] 12%|█▏        | 6047/50000 [2:09:58<15:31:55,  1.27s/it][2023-07-03 14:10:09,904] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
 12%|█▏        | 6048/50000 [2:09:59<14:29:45,  1.19s/it] 12%|█▏        | 6049/50000 [2:10:01<14:42:26,  1.20s/it] 12%|█▏        | 6050/50000 [2:10:02<14:50:25,  1.22s/it]                                                         {'loss': 128.4437, 'learning_rate': 1.9738099792385804e-05, 'epoch': 0.94}
 12%|█▏        | 6050/50000 [2:10:02<14:50:25,  1.22s/it] 12%|█▏        | 6051/50000 [2:10:03<14:56:33,  1.22s/it] 12%|█▏        | 6052/50000 [2:10:04<15:00:06,  1.23s/it] 12%|█▏        | 6053/50000 [2:10:06<15:02:44,  1.23s/it] 12%|█▏        | 6054/50000 [2:10:07<15:05:29,  1.24s/it] 12%|█▏        | 6055/50000 [2:10:08<15:06:22,  1.24s/it] 12%|█▏        | 6056/50000 [2:10:09<15:07:22,  1.24s/it] 12%|█▏        | 6057/50000 [2:10:11<15:07:42,  1.24s/it] 12%|█▏        | 6058/50000 [2:10:12<15:07:52,  1.24s/it] 12%|█▏        | 6059/50000 [2:10:13<15:09:32,  1.24s/it] 12%|█▏        | 6060/50000 [2:10:14<15:37:17,  1.28s/it]                                                         {'loss': 184.782, 'learning_rate': 1.9736593908403226e-05, 'epoch': 0.94}
 12%|█▏        | 6060/50000 [2:10:14<15:37:17,  1.28s/it] 12%|█▏        | 6061/50000 [2:10:16<15:29:55,  1.27s/it] 12%|█▏        | 6062/50000 [2:10:17<15:24:39,  1.26s/it] 12%|█▏        | 6063/50000 [2:10:18<15:20:07,  1.26s/it] 12%|█▏        | 6064/50000 [2:10:19<15:17:07,  1.25s/it] 12%|█▏        | 6065/50000 [2:10:21<15:15:15,  1.25s/it] 12%|█▏        | 6066/50000 [2:10:22<15:13:48,  1.25s/it][2023-07-03 14:10:33,395] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 12%|█▏        | 6067/50000 [2:10:23<14:17:20,  1.17s/it] 12%|█▏        | 6068/50000 [2:10:24<14:33:04,  1.19s/it] 12%|█▏        | 6069/50000 [2:10:25<14:44:23,  1.21s/it] 12%|█▏        | 6070/50000 [2:10:27<14:51:48,  1.22s/it]                                                         {'loss': 143.8984, 'learning_rate': 1.9735234971255083e-05, 'epoch': 0.94}
 12%|█▏        | 6070/50000 [2:10:27<14:51:48,  1.22s/it] 12%|█▏        | 6071/50000 [2:10:28<15:01:53,  1.23s/it] 12%|█▏        | 6072/50000 [2:10:29<15:30:20,  1.27s/it] 12%|█▏        | 6073/50000 [2:10:30<15:24:56,  1.26s/it] 12%|█▏        | 6074/50000 [2:10:32<15:31:30,  1.27s/it] 12%|█▏        | 6075/50000 [2:10:33<15:25:12,  1.26s/it] 12%|█▏        | 6076/50000 [2:10:34<15:21:36,  1.26s/it] 12%|█▏        | 6077/50000 [2:10:36<15:30:37,  1.27s/it] 12%|█▏        | 6078/50000 [2:10:37<15:24:52,  1.26s/it] 12%|█▏        | 6079/50000 [2:10:38<15:20:53,  1.26s/it] 12%|█▏        | 6080/50000 [2:10:39<15:17:55,  1.25s/it]                                                         {'loss': 131.3297, 'learning_rate': 1.9733720995515013e-05, 'epoch': 0.94}
 12%|█▏        | 6080/50000 [2:10:39<15:17:55,  1.25s/it] 12%|█▏        | 6081/50000 [2:10:41<15:15:38,  1.25s/it] 12%|█▏        | 6082/50000 [2:10:42<15:13:30,  1.25s/it] 12%|█▏        | 6083/50000 [2:10:43<15:12:20,  1.25s/it] 12%|█▏        | 6084/50000 [2:10:44<15:10:56,  1.24s/it] 12%|█▏        | 6085/50000 [2:10:46<15:09:55,  1.24s/it] 12%|█▏        | 6086/50000 [2:10:47<15:09:28,  1.24s/it] 12%|█▏        | 6087/50000 [2:10:48<15:09:56,  1.24s/it] 12%|█▏        | 6088/50000 [2:10:49<15:09:54,  1.24s/it] 12%|█▏        | 6089/50000 [2:10:50<15:09:46,  1.24s/it] 12%|█▏        | 6090/50000 [2:10:52<15:09:37,  1.24s/it]                                                         {'loss': 135.2438, 'learning_rate': 1.9732202761914684e-05, 'epoch': 0.94}
 12%|█▏        | 6090/50000 [2:10:52<15:09:37,  1.24s/it] 12%|█▏        | 6091/50000 [2:10:53<15:10:01,  1.24s/it] 12%|█▏        | 6092/50000 [2:10:54<15:09:47,  1.24s/it] 12%|█▏        | 6093/50000 [2:10:55<15:09:20,  1.24s/it] 12%|█▏        | 6094/50000 [2:10:57<15:09:41,  1.24s/it] 12%|█▏        | 6095/50000 [2:10:58<15:09:43,  1.24s/it] 12%|█▏        | 6096/50000 [2:10:59<15:09:54,  1.24s/it] 12%|█▏        | 6097/50000 [2:11:00<15:09:39,  1.24s/it] 12%|█▏        | 6098/50000 [2:11:02<15:09:22,  1.24s/it] 12%|█▏        | 6099/50000 [2:11:03<15:09:25,  1.24s/it] 12%|█▏        | 6100/50000 [2:11:04<15:09:20,  1.24s/it]                                                         {'loss': 121.2641, 'learning_rate': 1.9730680271118227e-05, 'epoch': 0.95}
 12%|█▏        | 6100/50000 [2:11:04<15:09:20,  1.24s/it] 12%|█▏        | 6101/50000 [2:11:05<15:09:54,  1.24s/it] 12%|█▏        | 6102/50000 [2:11:07<15:09:24,  1.24s/it] 12%|█▏        | 6103/50000 [2:11:08<15:09:08,  1.24s/it] 12%|█▏        | 6104/50000 [2:11:09<15:09:20,  1.24s/it] 12%|█▏        | 6105/50000 [2:11:10<15:09:28,  1.24s/it] 12%|█▏        | 6106/50000 [2:11:12<15:09:42,  1.24s/it] 12%|█▏        | 6107/50000 [2:11:13<15:09:33,  1.24s/it] 12%|█▏        | 6108/50000 [2:11:14<15:09:11,  1.24s/it] 12%|█▏        | 6109/50000 [2:11:15<15:09:10,  1.24s/it] 12%|█▏        | 6110/50000 [2:11:17<15:09:28,  1.24s/it]                                                         {'loss': 147.9828, 'learning_rate': 1.9729153523791624e-05, 'epoch': 0.95}
 12%|█▏        | 6110/50000 [2:11:17<15:09:28,  1.24s/it] 12%|█▏        | 6111/50000 [2:11:18<15:09:29,  1.24s/it] 12%|█▏        | 6112/50000 [2:11:19<15:09:18,  1.24s/it] 12%|█▏        | 6113/50000 [2:11:20<15:09:18,  1.24s/it] 12%|█▏        | 6114/50000 [2:11:22<15:08:59,  1.24s/it] 12%|█▏        | 6115/50000 [2:11:23<15:08:57,  1.24s/it] 12%|█▏        | 6116/50000 [2:11:24<15:08:36,  1.24s/it] 12%|█▏        | 6117/50000 [2:11:25<15:08:32,  1.24s/it] 12%|█▏        | 6118/50000 [2:11:27<15:08:14,  1.24s/it] 12%|█▏        | 6119/50000 [2:11:28<15:08:12,  1.24s/it] 12%|█▏        | 6120/50000 [2:11:29<15:08:14,  1.24s/it]                                                         {'loss': 137.3234, 'learning_rate': 1.972762252060273e-05, 'epoch': 0.95}
 12%|█▏        | 6120/50000 [2:11:29<15:08:14,  1.24s/it] 12%|█▏        | 6121/50000 [2:11:30<15:08:35,  1.24s/it] 12%|█▏        | 6122/50000 [2:11:31<15:08:26,  1.24s/it] 12%|█▏        | 6123/50000 [2:11:33<15:08:14,  1.24s/it] 12%|█▏        | 6124/50000 [2:11:34<15:46:58,  1.29s/it] 12%|█▏        | 6125/50000 [2:11:35<15:35:23,  1.28s/it] 12%|█▏        | 6126/50000 [2:11:37<15:27:36,  1.27s/it] 12%|█▏        | 6127/50000 [2:11:38<15:21:59,  1.26s/it] 12%|█▏        | 6128/50000 [2:11:39<15:20:59,  1.26s/it] 12%|█▏        | 6129/50000 [2:11:40<15:17:26,  1.25s/it] 12%|█▏        | 6130/50000 [2:11:42<15:14:36,  1.25s/it]                                                         {'loss': 147.2727, 'learning_rate': 1.9726087262221263e-05, 'epoch': 0.95}
 12%|█▏        | 6130/50000 [2:11:42<15:14:36,  1.25s/it] 12%|█▏        | 6131/50000 [2:11:43<15:13:22,  1.25s/it] 12%|█▏        | 6132/50000 [2:11:44<15:12:16,  1.25s/it] 12%|█▏        | 6133/50000 [2:11:45<15:11:43,  1.25s/it] 12%|█▏        | 6134/50000 [2:11:47<15:10:51,  1.25s/it] 12%|█▏        | 6135/50000 [2:11:48<15:10:04,  1.24s/it] 12%|█▏        | 6136/50000 [2:11:49<15:10:05,  1.24s/it] 12%|█▏        | 6137/50000 [2:11:50<15:09:24,  1.24s/it] 12%|█▏        | 6138/50000 [2:11:52<15:08:58,  1.24s/it] 12%|█▏        | 6139/50000 [2:11:53<15:09:27,  1.24s/it] 12%|█▏        | 6140/50000 [2:11:54<15:10:30,  1.25s/it]                                                         {'loss': 133.6133, 'learning_rate': 1.972454774931879e-05, 'epoch': 0.95}
 12%|█▏        | 6140/50000 [2:11:54<15:10:30,  1.25s/it] 12%|█▏        | 6141/50000 [2:11:55<15:11:34,  1.25s/it] 12%|█▏        | 6142/50000 [2:11:57<15:10:38,  1.25s/it] 12%|█▏        | 6143/50000 [2:11:58<15:09:52,  1.24s/it] 12%|█▏        | 6144/50000 [2:11:59<15:09:02,  1.24s/it] 12%|█▏        | 6145/50000 [2:12:00<15:08:44,  1.24s/it] 12%|█▏        | 6146/50000 [2:12:02<15:08:35,  1.24s/it] 12%|█▏        | 6147/50000 [2:12:03<15:08:52,  1.24s/it] 12%|█▏        | 6148/50000 [2:12:04<15:08:48,  1.24s/it] 12%|█▏        | 6149/50000 [2:12:05<15:09:16,  1.24s/it] 12%|█▏        | 6150/50000 [2:12:07<15:08:40,  1.24s/it]                                                         {'loss': 119.2422, 'learning_rate': 1.9723003982568745e-05, 'epoch': 0.95}
 12%|█▏        | 6150/50000 [2:12:07<15:08:40,  1.24s/it] 12%|█▏        | 6151/50000 [2:12:08<15:08:46,  1.24s/it] 12%|█▏        | 6152/50000 [2:12:09<15:08:15,  1.24s/it] 12%|█▏        | 6153/50000 [2:12:10<15:07:48,  1.24s/it] 12%|█▏        | 6154/50000 [2:12:11<15:08:19,  1.24s/it] 12%|█▏        | 6155/50000 [2:12:13<15:08:39,  1.24s/it] 12%|█▏        | 6156/50000 [2:12:14<15:08:51,  1.24s/it] 12%|█▏        | 6157/50000 [2:12:15<15:09:17,  1.24s/it] 12%|█▏        | 6158/50000 [2:12:16<15:08:39,  1.24s/it] 12%|█▏        | 6159/50000 [2:12:18<15:08:11,  1.24s/it] 12%|█▏        | 6160/50000 [2:12:19<15:07:48,  1.24s/it]                                                         {'loss': 136.8703, 'learning_rate': 1.972145596264643e-05, 'epoch': 0.95}
 12%|█▏        | 6160/50000 [2:12:19<15:07:48,  1.24s/it] 12%|█▏        | 6161/50000 [2:12:20<15:08:47,  1.24s/it] 12%|█▏        | 6162/50000 [2:12:21<15:08:23,  1.24s/it] 12%|█▏        | 6163/50000 [2:12:23<15:08:14,  1.24s/it] 12%|█▏        | 6164/50000 [2:12:24<15:08:16,  1.24s/it] 12%|█▏        | 6165/50000 [2:12:25<15:07:55,  1.24s/it] 12%|█▏        | 6166/50000 [2:12:26<15:07:35,  1.24s/it] 12%|█▏        | 6167/50000 [2:12:28<15:07:33,  1.24s/it] 12%|█▏        | 6168/50000 [2:12:29<15:07:16,  1.24s/it] 12%|█▏        | 6169/50000 [2:12:30<15:07:25,  1.24s/it] 12%|█▏        | 6170/50000 [2:12:31<15:07:23,  1.24s/it]                                                         {'loss': 142.3453, 'learning_rate': 1.9719903690228997e-05, 'epoch': 0.96}
 12%|█▏        | 6170/50000 [2:12:31<15:07:23,  1.24s/it] 12%|█▏        | 6171/50000 [2:12:33<15:07:40,  1.24s/it] 12%|█▏        | 6172/50000 [2:12:34<15:07:36,  1.24s/it] 12%|█▏        | 6173/50000 [2:12:35<15:07:31,  1.24s/it] 12%|█▏        | 6174/50000 [2:12:36<15:07:34,  1.24s/it] 12%|█▏        | 6175/50000 [2:12:38<15:07:10,  1.24s/it] 12%|█▏        | 6176/50000 [2:12:39<15:07:27,  1.24s/it] 12%|█▏        | 6177/50000 [2:12:40<15:07:23,  1.24s/it] 12%|█▏        | 6178/50000 [2:12:41<15:07:09,  1.24s/it] 12%|█▏        | 6179/50000 [2:12:43<15:07:21,  1.24s/it] 12%|█▏        | 6180/50000 [2:12:44<15:07:15,  1.24s/it]                                                         {'loss': 152.4688, 'learning_rate': 1.971834716599547e-05, 'epoch': 0.96}
 12%|█▏        | 6180/50000 [2:12:44<15:07:15,  1.24s/it] 12%|█▏        | 6181/50000 [2:12:45<15:07:37,  1.24s/it] 12%|█▏        | 6182/50000 [2:12:46<15:07:33,  1.24s/it] 12%|█▏        | 6183/50000 [2:12:48<15:08:13,  1.24s/it] 12%|█▏        | 6184/50000 [2:12:49<15:08:34,  1.24s/it] 12%|█▏        | 6185/50000 [2:12:50<15:08:01,  1.24s/it] 12%|█▏        | 6186/50000 [2:12:51<15:08:24,  1.24s/it] 12%|█▏        | 6187/50000 [2:12:52<15:07:50,  1.24s/it] 12%|█▏        | 6188/50000 [2:12:54<15:07:42,  1.24s/it] 12%|█▏        | 6189/50000 [2:12:55<15:07:13,  1.24s/it] 12%|█▏        | 6190/50000 [2:12:56<15:06:45,  1.24s/it]                                                         {'loss': 125.6828, 'learning_rate': 1.9716786390626715e-05, 'epoch': 0.96}
 12%|█▏        | 6190/50000 [2:12:56<15:06:45,  1.24s/it] 12%|█▏        | 6191/50000 [2:12:57<15:07:05,  1.24s/it] 12%|█▏        | 6192/50000 [2:12:59<15:07:09,  1.24s/it] 12%|█▏        | 6193/50000 [2:13:00<15:06:52,  1.24s/it] 12%|█▏        | 6194/50000 [2:13:01<15:06:24,  1.24s/it] 12%|█▏        | 6195/50000 [2:13:02<15:06:27,  1.24s/it] 12%|█▏        | 6196/50000 [2:13:04<15:06:14,  1.24s/it] 12%|█▏        | 6197/50000 [2:13:05<15:06:31,  1.24s/it] 12%|█▏        | 6198/50000 [2:13:06<15:06:34,  1.24s/it] 12%|█▏        | 6199/50000 [2:13:07<15:06:26,  1.24s/it] 12%|█▏        | 6200/50000 [2:13:09<15:06:04,  1.24s/it]                                                         {'loss': 148.0078, 'learning_rate': 1.9715221364805474e-05, 'epoch': 0.96}
 12%|█▏        | 6200/50000 [2:13:09<15:06:04,  1.24s/it][2023-07-03 14:13:20,141] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
 12%|█▏        | 6201/50000 [2:13:10<14:11:41,  1.17s/it] 12%|█▏        | 6202/50000 [2:13:11<14:27:51,  1.19s/it] 12%|█▏        | 6203/50000 [2:13:12<14:39:13,  1.20s/it] 12%|█▏        | 6204/50000 [2:13:13<14:47:07,  1.22s/it] 12%|█▏        | 6205/50000 [2:13:15<14:53:09,  1.22s/it] 12%|█▏        | 6206/50000 [2:13:16<14:57:24,  1.23s/it] 12%|█▏        | 6207/50000 [2:13:17<14:59:32,  1.23s/it] 12%|█▏        | 6208/50000 [2:13:18<15:01:19,  1.23s/it] 12%|█▏        | 6209/50000 [2:13:20<15:07:57,  1.24s/it] 12%|█▏        | 6210/50000 [2:13:21<15:08:07,  1.24s/it]                                                         {'loss': 132.5047, 'learning_rate': 1.971380920799526e-05, 'epoch': 0.96}
 12%|█▏        | 6210/50000 [2:13:21<15:08:07,  1.24s/it] 12%|█▏        | 6211/50000 [2:13:22<15:08:57,  1.25s/it] 12%|█▏        | 6212/50000 [2:13:24<16:10:05,  1.33s/it] 12%|█▏        | 6213/50000 [2:13:25<15:52:20,  1.30s/it] 12%|█▏        | 6214/50000 [2:13:26<15:41:08,  1.29s/it] 12%|█▏        | 6215/50000 [2:13:27<15:30:47,  1.28s/it] 12%|█▏        | 6216/50000 [2:13:29<15:24:57,  1.27s/it] 12%|█▏        | 6217/50000 [2:13:30<15:19:18,  1.26s/it] 12%|█▏        | 6218/50000 [2:13:31<15:15:23,  1.25s/it] 12%|█▏        | 6219/50000 [2:13:32<15:13:07,  1.25s/it] 12%|█▏        | 6220/50000 [2:13:34<15:11:43,  1.25s/it]                                                         {'loss': 133.1656, 'learning_rate': 1.9712236108201895e-05, 'epoch': 0.96}
 12%|█▏        | 6220/50000 [2:13:34<15:11:43,  1.25s/it] 12%|█▏        | 6221/50000 [2:13:35<15:10:26,  1.25s/it] 12%|█▏        | 6222/50000 [2:13:36<15:09:21,  1.25s/it] 12%|█▏        | 6223/50000 [2:13:37<15:08:59,  1.25s/it] 12%|█▏        | 6224/50000 [2:13:39<15:08:23,  1.25s/it] 12%|█▏        | 6225/50000 [2:13:40<15:07:30,  1.24s/it] 12%|█▏        | 6226/50000 [2:13:41<15:07:52,  1.24s/it] 12%|█▏        | 6227/50000 [2:13:42<15:07:16,  1.24s/it] 12%|█▏        | 6228/50000 [2:13:44<15:06:49,  1.24s/it] 12%|█▏        | 6229/50000 [2:13:45<15:06:38,  1.24s/it] 12%|█▏        | 6230/50000 [2:13:46<15:06:17,  1.24s/it]                                                         {'loss': 122.6445, 'learning_rate': 1.9710658759946487e-05, 'epoch': 0.97}
 12%|█▏        | 6230/50000 [2:13:46<15:06:17,  1.24s/it] 12%|█▏        | 6231/50000 [2:13:47<15:06:16,  1.24s/it] 12%|█▏        | 6232/50000 [2:13:48<15:06:21,  1.24s/it] 12%|█▏        | 6233/50000 [2:13:50<15:06:18,  1.24s/it] 12%|█▏        | 6234/50000 [2:13:51<15:06:26,  1.24s/it] 12%|█▏        | 6235/50000 [2:13:52<15:06:26,  1.24s/it] 12%|█▏        | 6236/50000 [2:13:53<15:06:30,  1.24s/it] 12%|█▏        | 6237/50000 [2:13:55<15:06:18,  1.24s/it] 12%|█▏        | 6238/50000 [2:13:56<15:06:08,  1.24s/it] 12%|█▏        | 6239/50000 [2:13:57<15:05:59,  1.24s/it] 12%|█▏        | 6240/50000 [2:13:58<15:06:18,  1.24s/it]                                                         {'loss': 118.8656, 'learning_rate': 1.970907716391903e-05, 'epoch': 0.97}
 12%|█▏        | 6240/50000 [2:13:58<15:06:18,  1.24s/it] 12%|█▏        | 6241/50000 [2:14:00<15:06:25,  1.24s/it] 12%|█▏        | 6242/50000 [2:14:01<15:06:41,  1.24s/it] 12%|█▏        | 6243/50000 [2:14:02<15:06:40,  1.24s/it] 12%|█▏        | 6244/50000 [2:14:03<15:26:13,  1.27s/it] 12%|█▏        | 6245/50000 [2:14:05<15:20:08,  1.26s/it] 12%|█▏        | 6246/50000 [2:14:06<15:16:04,  1.26s/it] 12%|█▏        | 6247/50000 [2:14:07<15:13:34,  1.25s/it] 12%|█▏        | 6248/50000 [2:14:08<15:11:11,  1.25s/it] 12%|█▏        | 6249/50000 [2:14:10<15:12:11,  1.25s/it] 12%|█▎        | 6250/50000 [2:14:11<15:10:32,  1.25s/it]                                                         {'loss': 133.9609, 'learning_rate': 1.970749132081136e-05, 'epoch': 0.97}
 12%|█▎        | 6250/50000 [2:14:11<15:10:32,  1.25s/it] 13%|█▎        | 6251/50000 [2:14:12<15:08:58,  1.25s/it] 13%|█▎        | 6252/50000 [2:14:13<15:07:57,  1.25s/it] 13%|█▎        | 6253/50000 [2:14:15<15:07:03,  1.24s/it] 13%|█▎        | 6254/50000 [2:14:16<15:07:03,  1.24s/it] 13%|█▎        | 6255/50000 [2:14:17<15:07:28,  1.24s/it] 13%|█▎        | 6256/50000 [2:14:18<15:07:13,  1.24s/it] 13%|█▎        | 6257/50000 [2:14:20<15:07:50,  1.25s/it] 13%|█▎        | 6258/50000 [2:14:21<15:07:35,  1.24s/it] 13%|█▎        | 6259/50000 [2:14:22<15:07:26,  1.24s/it] 13%|█▎        | 6260/50000 [2:14:23<15:07:48,  1.25s/it]                                                         {'loss': 140.9406, 'learning_rate': 1.970590123131719e-05, 'epoch': 0.97}
 13%|█▎        | 6260/50000 [2:14:23<15:07:48,  1.25s/it] 13%|█▎        | 6261/50000 [2:14:25<15:07:46,  1.25s/it] 13%|█▎        | 6262/50000 [2:14:26<15:07:09,  1.24s/it] 13%|█▎        | 6263/50000 [2:14:27<15:07:06,  1.24s/it] 13%|█▎        | 6264/50000 [2:14:28<15:06:17,  1.24s/it] 13%|█▎        | 6265/50000 [2:14:30<15:05:54,  1.24s/it] 13%|█▎        | 6266/50000 [2:14:31<15:05:18,  1.24s/it] 13%|█▎        | 6267/50000 [2:14:32<15:05:01,  1.24s/it] 13%|█▎        | 6268/50000 [2:14:33<15:04:53,  1.24s/it] 13%|█▎        | 6269/50000 [2:14:35<15:04:44,  1.24s/it] 13%|█▎        | 6270/50000 [2:14:36<15:04:34,  1.24s/it]                                                         {'loss': 144.6672, 'learning_rate': 1.9704306896132063e-05, 'epoch': 0.97}
 13%|█▎        | 6270/50000 [2:14:36<15:04:34,  1.24s/it] 13%|█▎        | 6271/50000 [2:14:37<15:04:46,  1.24s/it] 13%|█▎        | 6272/50000 [2:14:38<15:05:17,  1.24s/it] 13%|█▎        | 6273/50000 [2:14:40<15:05:17,  1.24s/it] 13%|█▎        | 6274/50000 [2:14:41<15:06:08,  1.24s/it] 13%|█▎        | 6275/50000 [2:14:42<15:05:20,  1.24s/it] 13%|█▎        | 6276/50000 [2:14:43<15:04:53,  1.24s/it] 13%|█▎        | 6277/50000 [2:14:45<15:04:40,  1.24s/it][2023-07-03 14:14:56,014] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 13%|█▎        | 6278/50000 [2:14:45<14:10:09,  1.17s/it] 13%|█▎        | 6279/50000 [2:14:47<14:26:23,  1.19s/it] 13%|█▎        | 6280/50000 [2:14:48<14:37:55,  1.20s/it]                                                         {'loss': 145.8203, 'learning_rate': 1.9702868364976068e-05, 'epoch': 0.97}
 13%|█▎        | 6280/50000 [2:14:48<14:37:55,  1.20s/it] 13%|█▎        | 6281/50000 [2:14:49<14:46:02,  1.22s/it][2023-07-03 14:15:00,733] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
 13%|█▎        | 6282/50000 [2:14:50<13:57:25,  1.15s/it] 13%|█▎        | 6283/50000 [2:14:51<14:17:18,  1.18s/it] 13%|█▎        | 6284/50000 [2:14:53<14:31:03,  1.20s/it] 13%|█▎        | 6285/50000 [2:14:54<14:40:47,  1.21s/it] 13%|█▎        | 6286/50000 [2:14:55<14:47:30,  1.22s/it] 13%|█▎        | 6287/50000 [2:14:56<14:52:27,  1.22s/it] 13%|█▎        | 6288/50000 [2:14:58<14:55:49,  1.23s/it] 13%|█▎        | 6289/50000 [2:14:59<14:58:10,  1.23s/it] 13%|█▎        | 6290/50000 [2:15:00<14:59:57,  1.24s/it]                                                         {'loss': 155.3281, 'learning_rate': 1.970142639588499e-05, 'epoch': 0.98}
 13%|█▎        | 6290/50000 [2:15:00<14:59:57,  1.24s/it] 13%|█▎        | 6291/50000 [2:15:01<15:00:57,  1.24s/it] 13%|█▎        | 6292/50000 [2:15:03<15:01:53,  1.24s/it] 13%|█▎        | 6293/50000 [2:15:04<15:02:15,  1.24s/it] 13%|█▎        | 6294/50000 [2:15:05<15:02:27,  1.24s/it] 13%|█▎        | 6295/50000 [2:15:06<15:02:29,  1.24s/it] 13%|█▎        | 6296/50000 [2:15:08<15:02:26,  1.24s/it] 13%|█▎        | 6297/50000 [2:15:09<15:02:46,  1.24s/it] 13%|█▎        | 6298/50000 [2:15:10<15:02:52,  1.24s/it] 13%|█▎        | 6299/50000 [2:15:11<15:03:04,  1.24s/it] 13%|█▎        | 6300/50000 [2:15:13<15:03:29,  1.24s/it]                                                         {'loss': 167.3891, 'learning_rate': 1.9699820176481394e-05, 'epoch': 0.98}
 13%|█▎        | 6300/50000 [2:15:13<15:03:29,  1.24s/it] 13%|█▎        | 6301/50000 [2:15:14<15:03:34,  1.24s/it] 13%|█▎        | 6302/50000 [2:15:15<15:03:32,  1.24s/it] 13%|█▎        | 6303/50000 [2:15:16<15:03:03,  1.24s/it] 13%|█▎        | 6304/50000 [2:15:18<15:02:56,  1.24s/it] 13%|█▎        | 6305/50000 [2:15:19<15:02:48,  1.24s/it] 13%|█▎        | 6306/50000 [2:15:20<15:02:46,  1.24s/it] 13%|█▎        | 6307/50000 [2:15:21<15:03:03,  1.24s/it] 13%|█▎        | 6308/50000 [2:15:22<15:02:59,  1.24s/it] 13%|█▎        | 6309/50000 [2:15:24<15:03:22,  1.24s/it] 13%|█▎        | 6310/50000 [2:15:25<15:06:26,  1.24s/it]                                                         {'loss': 173.8203, 'learning_rate': 1.969820971404691e-05, 'epoch': 0.98}
 13%|█▎        | 6310/50000 [2:15:25<15:06:26,  1.24s/it] 13%|█▎        | 6311/50000 [2:15:26<15:05:32,  1.24s/it] 13%|█▎        | 6312/50000 [2:15:27<15:04:48,  1.24s/it] 13%|█▎        | 6313/50000 [2:15:29<15:03:56,  1.24s/it] 13%|█▎        | 6314/50000 [2:15:30<15:03:29,  1.24s/it] 13%|█▎        | 6315/50000 [2:15:31<15:02:58,  1.24s/it] 13%|█▎        | 6316/50000 [2:15:32<15:03:04,  1.24s/it] 13%|█▎        | 6317/50000 [2:15:34<15:02:43,  1.24s/it] 13%|█▎        | 6318/50000 [2:15:35<15:02:44,  1.24s/it] 13%|█▎        | 6319/50000 [2:15:36<15:02:37,  1.24s/it] 13%|█▎        | 6320/50000 [2:15:37<15:02:26,  1.24s/it]                                                         {'loss': 138.6445, 'learning_rate': 1.969659500928601e-05, 'epoch': 0.98}
 13%|█▎        | 6320/50000 [2:15:37<15:02:26,  1.24s/it] 13%|█▎        | 6321/50000 [2:15:39<15:02:31,  1.24s/it] 13%|█▎        | 6322/50000 [2:15:40<15:02:25,  1.24s/it] 13%|█▎        | 6323/50000 [2:15:41<15:02:32,  1.24s/it] 13%|█▎        | 6324/50000 [2:15:42<15:02:40,  1.24s/it] 13%|█▎        | 6325/50000 [2:15:44<15:02:36,  1.24s/it] 13%|█▎        | 6326/50000 [2:15:45<15:02:29,  1.24s/it] 13%|█▎        | 6327/50000 [2:15:46<15:02:12,  1.24s/it] 13%|█▎        | 6328/50000 [2:15:47<15:02:15,  1.24s/it] 13%|█▎        | 6329/50000 [2:15:49<15:02:16,  1.24s/it] 13%|█▎        | 6330/50000 [2:15:50<15:02:01,  1.24s/it]                                                         {'loss': 132.5734, 'learning_rate': 1.969497606290502e-05, 'epoch': 0.98}
 13%|█▎        | 6330/50000 [2:15:50<15:02:01,  1.24s/it] 13%|█▎        | 6331/50000 [2:15:51<15:01:57,  1.24s/it] 13%|█▎        | 6332/50000 [2:15:52<15:01:55,  1.24s/it] 13%|█▎        | 6333/50000 [2:15:53<15:01:44,  1.24s/it] 13%|█▎        | 6334/50000 [2:15:55<15:01:31,  1.24s/it] 13%|█▎        | 6335/50000 [2:15:56<15:01:27,  1.24s/it] 13%|█▎        | 6336/50000 [2:15:57<15:01:51,  1.24s/it] 13%|█▎        | 6337/50000 [2:15:58<15:01:44,  1.24s/it] 13%|█▎        | 6338/50000 [2:16:00<15:01:38,  1.24s/it] 13%|█▎        | 6339/50000 [2:16:01<15:01:26,  1.24s/it] 13%|█▎        | 6340/50000 [2:16:02<15:01:14,  1.24s/it]                                                         {'loss': 136.6328, 'learning_rate': 1.9693352875612116e-05, 'epoch': 0.98}
 13%|█▎        | 6340/50000 [2:16:02<15:01:14,  1.24s/it] 13%|█▎        | 6341/50000 [2:16:03<15:01:27,  1.24s/it] 13%|█▎        | 6342/50000 [2:16:05<15:01:19,  1.24s/it] 13%|█▎        | 6343/50000 [2:16:06<15:01:24,  1.24s/it] 13%|█▎        | 6344/50000 [2:16:07<15:01:29,  1.24s/it] 13%|█▎        | 6345/50000 [2:16:08<15:01:35,  1.24s/it] 13%|█▎        | 6346/50000 [2:16:10<15:01:32,  1.24s/it] 13%|█▎        | 6347/50000 [2:16:11<15:01:51,  1.24s/it] 13%|█▎        | 6348/50000 [2:16:12<15:01:27,  1.24s/it] 13%|█▎        | 6349/50000 [2:16:13<15:01:16,  1.24s/it] 13%|█▎        | 6350/50000 [2:16:15<15:01:44,  1.24s/it]                                                         {'loss': 146.6766, 'learning_rate': 1.9691725448117348e-05, 'epoch': 0.98}
 13%|█▎        | 6350/50000 [2:16:15<15:01:44,  1.24s/it] 13%|█▎        | 6351/50000 [2:16:16<15:01:47,  1.24s/it] 13%|█▎        | 6352/50000 [2:16:17<15:01:44,  1.24s/it] 13%|█▎        | 6353/50000 [2:16:18<15:01:20,  1.24s/it] 13%|█▎        | 6354/50000 [2:16:19<15:01:16,  1.24s/it] 13%|█▎        | 6355/50000 [2:16:21<15:01:11,  1.24s/it] 13%|█▎        | 6356/50000 [2:16:22<15:01:31,  1.24s/it] 13%|█▎        | 6357/50000 [2:16:23<15:01:17,  1.24s/it] 13%|█▎        | 6358/50000 [2:16:24<15:01:12,  1.24s/it] 13%|█▎        | 6359/50000 [2:16:26<15:01:04,  1.24s/it] 13%|█▎        | 6360/50000 [2:16:27<15:01:09,  1.24s/it]                                                         {'loss': 136.8859, 'learning_rate': 1.96900937811326e-05, 'epoch': 0.99}
 13%|█▎        | 6360/50000 [2:16:27<15:01:09,  1.24s/it] 13%|█▎        | 6361/50000 [2:16:28<15:01:16,  1.24s/it] 13%|█▎        | 6362/50000 [2:16:29<15:01:18,  1.24s/it] 13%|█▎        | 6363/50000 [2:16:31<15:01:17,  1.24s/it] 13%|█▎        | 6364/50000 [2:16:32<15:00:51,  1.24s/it] 13%|█▎        | 6365/50000 [2:16:33<15:00:39,  1.24s/it] 13%|█▎        | 6366/50000 [2:16:34<15:00:50,  1.24s/it] 13%|█▎        | 6367/50000 [2:16:36<15:00:58,  1.24s/it] 13%|█▎        | 6368/50000 [2:16:37<15:01:07,  1.24s/it] 13%|█▎        | 6369/50000 [2:16:38<15:01:19,  1.24s/it] 13%|█▎        | 6370/50000 [2:16:39<15:01:19,  1.24s/it]                                                         {'loss': 120.9164, 'learning_rate': 1.9688457875371612e-05, 'epoch': 0.99}
 13%|█▎        | 6370/50000 [2:16:39<15:01:19,  1.24s/it] 13%|█▎        | 6371/50000 [2:16:41<15:01:22,  1.24s/it] 13%|█▎        | 6372/50000 [2:16:42<15:00:53,  1.24s/it] 13%|█▎        | 6373/50000 [2:16:43<15:00:41,  1.24s/it] 13%|█▎        | 6374/50000 [2:16:44<15:00:45,  1.24s/it] 13%|█▎        | 6375/50000 [2:16:46<15:00:47,  1.24s/it] 13%|█▎        | 6376/50000 [2:16:47<15:00:46,  1.24s/it] 13%|█▎        | 6377/50000 [2:16:48<15:00:54,  1.24s/it] 13%|█▎        | 6378/50000 [2:16:49<15:01:08,  1.24s/it] 13%|█▎        | 6379/50000 [2:16:50<15:01:14,  1.24s/it] 13%|█▎        | 6380/50000 [2:16:52<15:00:55,  1.24s/it]                                                         {'loss': 128.4172, 'learning_rate': 1.968681773155e-05, 'epoch': 0.99}
 13%|█▎        | 6380/50000 [2:16:52<15:00:55,  1.24s/it] 13%|█▎        | 6381/50000 [2:16:53<15:01:01,  1.24s/it] 13%|█▎        | 6382/50000 [2:16:54<15:00:55,  1.24s/it] 13%|█▎        | 6383/50000 [2:16:55<15:00:45,  1.24s/it] 13%|█▎        | 6384/50000 [2:16:57<15:00:58,  1.24s/it] 13%|█▎        | 6385/50000 [2:16:58<15:00:50,  1.24s/it] 13%|█▎        | 6386/50000 [2:16:59<15:00:39,  1.24s/it] 13%|█▎        | 6387/50000 [2:17:00<15:00:33,  1.24s/it] 13%|█▎        | 6388/50000 [2:17:02<15:00:38,  1.24s/it] 13%|█▎        | 6389/50000 [2:17:03<15:00:30,  1.24s/it] 13%|█▎        | 6390/50000 [2:17:04<15:00:53,  1.24s/it]                                                         {'loss': 150.9422, 'learning_rate': 1.968517335038521e-05, 'epoch': 0.99}
 13%|█▎        | 6390/50000 [2:17:04<15:00:53,  1.24s/it] 13%|█▎        | 6391/50000 [2:17:05<15:01:26,  1.24s/it] 13%|█▎        | 6392/50000 [2:17:07<15:01:16,  1.24s/it] 13%|█▎        | 6393/50000 [2:17:08<15:00:56,  1.24s/it] 13%|█▎        | 6394/50000 [2:17:09<15:00:50,  1.24s/it] 13%|█▎        | 6395/50000 [2:17:10<15:00:34,  1.24s/it] 13%|█▎        | 6396/50000 [2:17:12<15:00:28,  1.24s/it] 13%|█▎        | 6397/50000 [2:17:13<15:00:39,  1.24s/it] 13%|█▎        | 6398/50000 [2:17:14<15:00:40,  1.24s/it] 13%|█▎        | 6399/50000 [2:17:15<15:00:17,  1.24s/it] 13%|█▎        | 6400/50000 [2:17:16<15:00:09,  1.24s/it]                                                         {'loss': 143.5859, 'learning_rate': 1.968352473259655e-05, 'epoch': 0.99}
 13%|█▎        | 6400/50000 [2:17:16<15:00:09,  1.24s/it] 13%|█▎        | 6401/50000 [2:17:18<15:00:24,  1.24s/it] 13%|█▎        | 6402/50000 [2:17:19<15:00:14,  1.24s/it] 13%|█▎        | 6403/50000 [2:17:20<15:00:18,  1.24s/it] 13%|█▎        | 6404/50000 [2:17:21<15:00:43,  1.24s/it] 13%|█▎        | 6405/50000 [2:17:23<15:00:19,  1.24s/it] 13%|█▎        | 6406/50000 [2:17:24<15:00:28,  1.24s/it] 13%|█▎        | 6407/50000 [2:17:25<15:00:18,  1.24s/it] 13%|█▎        | 6408/50000 [2:17:26<15:00:23,  1.24s/it] 13%|█▎        | 6409/50000 [2:17:28<15:00:13,  1.24s/it] 13%|█▎        | 6410/50000 [2:17:29<15:00:16,  1.24s/it]                                                         {'loss': 140.6828, 'learning_rate': 1.9681871878905184e-05, 'epoch': 0.99}
 13%|█▎        | 6410/50000 [2:17:29<15:00:16,  1.24s/it] 13%|█▎        | 6411/50000 [2:17:30<15:00:20,  1.24s/it] 13%|█▎        | 6412/50000 [2:17:31<15:00:15,  1.24s/it] 13%|█▎        | 6413/50000 [2:17:33<15:00:07,  1.24s/it] 13%|█▎        | 6414/50000 [2:17:34<15:00:01,  1.24s/it] 13%|█▎        | 6415/50000 [2:17:35<14:59:52,  1.24s/it] 13%|█▎        | 6416/50000 [2:17:36<14:59:38,  1.24s/it] 13%|█▎        | 6417/50000 [2:17:38<14:59:54,  1.24s/it] 13%|█▎        | 6418/50000 [2:17:39<15:00:02,  1.24s/it] 13%|█▎        | 6419/50000 [2:17:40<15:00:28,  1.24s/it] 13%|█▎        | 6420/50000 [2:17:41<15:00:28,  1.24s/it]                                                         {'loss': 131.6703, 'learning_rate': 1.968021479003412e-05, 'epoch': 1.0}
 13%|█▎        | 6420/50000 [2:17:41<15:00:28,  1.24s/it] 13%|█▎        | 6421/50000 [2:17:43<15:00:27,  1.24s/it] 13%|█▎        | 6422/50000 [2:17:44<15:00:17,  1.24s/it] 13%|█▎        | 6423/50000 [2:17:45<15:00:37,  1.24s/it] 13%|█▎        | 6424/50000 [2:17:46<15:00:40,  1.24s/it] 13%|█▎        | 6425/50000 [2:17:47<15:00:26,  1.24s/it] 13%|█▎        | 6426/50000 [2:17:49<15:00:57,  1.24s/it] 13%|█▎        | 6427/50000 [2:17:50<15:00:31,  1.24s/it] 13%|█▎        | 6428/50000 [2:17:51<15:00:16,  1.24s/it] 13%|█▎        | 6429/50000 [2:17:52<15:00:14,  1.24s/it] 13%|█▎        | 6430/50000 [2:17:54<15:00:12,  1.24s/it]                                                         {'loss': 133.1609, 'learning_rate': 1.9678553466708235e-05, 'epoch': 1.0}
 13%|█▎        | 6430/50000 [2:17:54<15:00:12,  1.24s/it] 13%|█▎        | 6431/50000 [2:17:55<15:00:01,  1.24s/it] 13%|█▎        | 6432/50000 [2:17:56<14:59:58,  1.24s/it] 13%|█▎        | 6433/50000 [2:17:57<14:59:48,  1.24s/it] 13%|█▎        | 6434/50000 [2:17:59<14:59:35,  1.24s/it] 13%|█▎        | 6435/50000 [2:18:00<14:59:28,  1.24s/it] 13%|█▎        | 6436/50000 [2:18:01<15:00:16,  1.24s/it] 13%|█▎        | 6437/50000 [2:18:02<14:59:56,  1.24s/it] 13%|█▎        | 6438/50000 [2:18:04<14:59:39,  1.24s/it] 13%|█▎        | 6439/50000 [2:18:05<14:59:51,  1.24s/it] 13%|█▎        | 6440/50000 [2:18:06<14:59:53,  1.24s/it]                                                         {'loss': 136.4406, 'learning_rate': 1.9676887909654245e-05, 'epoch': 1.0}
 13%|█▎        | 6440/50000 [2:18:06<14:59:53,  1.24s/it] 13%|█▎        | 6441/50000 [2:18:07<14:59:59,  1.24s/it] 13%|█▎        | 6442/50000 [2:18:09<14:59:49,  1.24s/it] 13%|█▎        | 6443/50000 [2:18:10<14:59:34,  1.24s/it] 13%|█▎        | 6444/50000 [2:18:11<14:59:14,  1.24s/it] 13%|█▎        | 6445/50000 [2:18:12<14:59:20,  1.24s/it] 13%|█▎        | 6446/50000 [2:18:14<14:59:27,  1.24s/it] 13%|█▎        | 6447/50000 [2:18:15<14:58:57,  1.24s/it] 13%|█▎        | 6448/50000 [2:18:16<14:59:21,  1.24s/it] 13%|█▎        | 6449/50000 [2:18:17<14:59:27,  1.24s/it] 13%|█▎        | 6450/50000 [2:18:18<14:59:23,  1.24s/it]                                                         {'loss': 143.925, 'learning_rate': 1.9675218119600714e-05, 'epoch': 1.0}
 13%|█▎        | 6450/50000 [2:18:18<14:59:23,  1.24s/it] 13%|█▎        | 6451/50000 [2:18:20<14:59:46,  1.24s/it] 13%|█▎        | 6452/50000 [2:18:21<15:00:04,  1.24s/it] 13%|█▎        | 6453/50000 [2:18:22<14:59:53,  1.24s/it] 13%|█▎        | 6454/50000 [2:18:23<14:59:45,  1.24s/it] 13%|█▎        | 6455/50000 [2:18:25<14:59:35,  1.24s/it] 13%|█▎        | 6456/50000 [2:18:26<14:59:28,  1.24s/it] 13%|█▎        | 6457/50000 [2:18:27<14:59:19,  1.24s/it] 13%|█▎        | 6458/50000 [2:18:28<14:59:32,  1.24s/it] 13%|█▎        | 6459/50000 [2:18:30<15:16:36,  1.26s/it] 13%|█▎        | 6460/50000 [2:18:31<15:11:25,  1.26s/it]                                                         {'loss': 133.0297, 'learning_rate': 1.9673544097278072e-05, 'epoch': 1.0}
 13%|█▎        | 6460/50000 [2:18:31<15:11:25,  1.26s/it] 13%|█▎        | 6461/50000 [2:18:32<15:07:44,  1.25s/it] 13%|█▎        | 6462/50000 [2:18:33<15:05:01,  1.25s/it] 13%|█▎        | 6463/50000 [2:18:35<15:02:57,  1.24s/it] 13%|█▎        | 6464/50000 [2:18:36<15:02:08,  1.24s/it] 13%|█▎        | 6465/50000 [2:18:37<15:01:13,  1.24s/it] 13%|█▎        | 6466/50000 [2:18:38<15:00:53,  1.24s/it] 13%|█▎        | 6467/50000 [2:18:40<15:00:25,  1.24s/it] 13%|█▎        | 6468/50000 [2:18:41<15:00:05,  1.24s/it] 13%|█▎        | 6469/50000 [2:18:42<14:59:37,  1.24s/it] 13%|█▎        | 6470/50000 [2:18:43<14:59:37,  1.24s/it]                                                         {'loss': 121.7031, 'learning_rate': 1.9671865843418595e-05, 'epoch': 1.0}
 13%|█▎        | 6470/50000 [2:18:43<14:59:37,  1.24s/it] 13%|█▎        | 6471/50000 [2:18:45<14:59:38,  1.24s/it] 13%|█▎        | 6472/50000 [2:18:46<14:59:21,  1.24s/it] 13%|█▎        | 6473/50000 [2:18:47<14:59:10,  1.24s/it] 13%|█▎        | 6474/50000 [2:18:48<14:59:33,  1.24s/it] 13%|█▎        | 6475/50000 [2:18:50<14:59:17,  1.24s/it] 13%|█▎        | 6476/50000 [2:18:51<14:59:06,  1.24s/it] 13%|█▎        | 6477/50000 [2:18:52<14:59:01,  1.24s/it] 13%|█▎        | 6478/50000 [2:18:53<14:58:44,  1.24s/it] 13%|█▎        | 6479/50000 [2:18:54<14:58:33,  1.24s/it] 13%|█▎        | 6480/50000 [2:18:56<14:58:38,  1.24s/it]                                                         {'loss': 109.6117, 'learning_rate': 1.9670183358756404e-05, 'epoch': 1.0}
 13%|█▎        | 6480/50000 [2:18:56<14:58:38,  1.24s/it] 13%|█▎        | 6481/50000 [2:18:57<14:58:38,  1.24s/it] 13%|█▎        | 6482/50000 [2:18:58<14:58:26,  1.24s/it] 13%|█▎        | 6483/50000 [2:18:59<14:58:30,  1.24s/it] 13%|█▎        | 6484/50000 [2:19:01<14:58:18,  1.24s/it] 13%|█▎        | 6485/50000 [2:19:02<14:58:18,  1.24s/it] 13%|█▎        | 6486/50000 [2:19:03<14:58:25,  1.24s/it] 13%|█▎        | 6487/50000 [2:19:04<14:58:26,  1.24s/it] 13%|█▎        | 6488/50000 [2:19:06<14:57:53,  1.24s/it] 13%|█▎        | 6489/50000 [2:19:07<14:58:15,  1.24s/it] 13%|█▎        | 6490/50000 [2:19:08<14:58:12,  1.24s/it]                                                         {'loss': 111.2109, 'learning_rate': 1.966849664402747e-05, 'epoch': 1.01}
 13%|█▎        | 6490/50000 [2:19:08<14:58:12,  1.24s/it] 13%|█▎        | 6491/50000 [2:19:09<14:58:01,  1.24s/it] 13%|█▎        | 6492/50000 [2:19:11<14:58:14,  1.24s/it] 13%|█▎        | 6493/50000 [2:19:12<14:58:11,  1.24s/it] 13%|█▎        | 6494/50000 [2:19:13<14:58:05,  1.24s/it] 13%|█▎        | 6495/50000 [2:19:14<14:57:58,  1.24s/it] 13%|█▎        | 6496/50000 [2:19:16<14:58:21,  1.24s/it] 13%|█▎        | 6497/50000 [2:19:17<14:58:24,  1.24s/it] 13%|█▎        | 6498/50000 [2:19:18<14:58:09,  1.24s/it] 13%|█▎        | 6499/50000 [2:19:19<14:58:10,  1.24s/it] 13%|█▎        | 6500/50000 [2:19:20<14:58:05,  1.24s/it]                                                         {'loss': 111.1813, 'learning_rate': 1.966680569996963e-05, 'epoch': 1.01}
 13%|█▎        | 6500/50000 [2:19:20<14:58:05,  1.24s/it] 13%|█▎        | 6501/50000 [2:19:22<14:58:16,  1.24s/it] 13%|█▎        | 6502/50000 [2:19:23<14:58:19,  1.24s/it] 13%|█▎        | 6503/50000 [2:19:24<14:57:54,  1.24s/it] 13%|█▎        | 6504/50000 [2:19:25<14:57:34,  1.24s/it] 13%|█▎        | 6505/50000 [2:19:27<14:58:02,  1.24s/it] 13%|█▎        | 6506/50000 [2:19:28<14:58:14,  1.24s/it] 13%|█▎        | 6507/50000 [2:19:29<14:58:02,  1.24s/it] 13%|█▎        | 6508/50000 [2:19:30<14:57:58,  1.24s/it] 13%|█▎        | 6509/50000 [2:19:32<14:58:03,  1.24s/it] 13%|█▎        | 6510/50000 [2:19:33<14:58:07,  1.24s/it]                                                         {'loss': 115.4242, 'learning_rate': 1.9665110527322557e-05, 'epoch': 1.01}
 13%|█▎        | 6510/50000 [2:19:33<14:58:07,  1.24s/it] 13%|█▎        | 6511/50000 [2:19:34<14:58:12,  1.24s/it] 13%|█▎        | 6512/50000 [2:19:35<14:58:24,  1.24s/it] 13%|█▎        | 6513/50000 [2:19:37<14:57:53,  1.24s/it] 13%|█▎        | 6514/50000 [2:19:38<14:57:31,  1.24s/it] 13%|█▎        | 6515/50000 [2:19:39<14:57:40,  1.24s/it] 13%|█▎        | 6516/50000 [2:19:40<14:57:26,  1.24s/it] 13%|█▎        | 6517/50000 [2:19:42<14:57:08,  1.24s/it] 13%|█▎        | 6518/50000 [2:19:43<14:57:43,  1.24s/it] 13%|█▎        | 6519/50000 [2:19:44<14:57:42,  1.24s/it] 13%|█▎        | 6520/50000 [2:19:45<14:57:29,  1.24s/it]                                                         {'loss': 118.5891, 'learning_rate': 1.9663411126827774e-05, 'epoch': 1.01}
 13%|█▎        | 6520/50000 [2:19:45<14:57:29,  1.24s/it] 13%|█▎        | 6521/50000 [2:19:47<14:57:42,  1.24s/it] 13%|█▎        | 6522/50000 [2:19:48<14:57:36,  1.24s/it] 13%|█▎        | 6523/50000 [2:19:49<14:57:42,  1.24s/it] 13%|█▎        | 6524/50000 [2:19:50<14:57:48,  1.24s/it] 13%|█▎        | 6525/50000 [2:19:51<14:57:46,  1.24s/it] 13%|█▎        | 6526/50000 [2:19:53<14:57:27,  1.24s/it] 13%|█▎        | 6527/50000 [2:19:54<14:57:40,  1.24s/it] 13%|█▎        | 6528/50000 [2:19:55<14:57:15,  1.24s/it] 13%|█▎        | 6529/50000 [2:19:56<14:56:48,  1.24s/it] 13%|█▎        | 6530/50000 [2:19:58<14:57:53,  1.24s/it]                                                         {'loss': 112.875, 'learning_rate': 1.9661707499228657e-05, 'epoch': 1.01}
 13%|█▎        | 6530/50000 [2:19:58<14:57:53,  1.24s/it] 13%|█▎        | 6531/50000 [2:19:59<14:58:04,  1.24s/it] 13%|█▎        | 6532/50000 [2:20:00<14:57:40,  1.24s/it] 13%|█▎        | 6533/50000 [2:20:01<14:57:28,  1.24s/it] 13%|█▎        | 6534/50000 [2:20:03<14:57:42,  1.24s/it] 13%|█▎        | 6535/50000 [2:20:04<14:57:27,  1.24s/it] 13%|█▎        | 6536/50000 [2:20:05<14:57:05,  1.24s/it] 13%|█▎        | 6537/50000 [2:20:06<14:56:38,  1.24s/it] 13%|█▎        | 6538/50000 [2:20:08<14:56:31,  1.24s/it] 13%|█▎        | 6539/50000 [2:20:09<14:56:23,  1.24s/it] 13%|█▎        | 6540/50000 [2:20:10<14:56:01,  1.24s/it]                                                         {'loss': 111.6063, 'learning_rate': 1.965999964527043e-05, 'epoch': 1.01}
 13%|█▎        | 6540/50000 [2:20:10<14:56:01,  1.24s/it] 13%|█▎        | 6541/50000 [2:20:11<14:56:31,  1.24s/it] 13%|█▎        | 6542/50000 [2:20:13<14:56:46,  1.24s/it] 13%|█▎        | 6543/50000 [2:20:14<14:56:52,  1.24s/it] 13%|█▎        | 6544/50000 [2:20:15<14:56:38,  1.24s/it] 13%|█▎        | 6545/50000 [2:20:16<14:55:19,  1.24s/it] 13%|█▎        | 6546/50000 [2:20:17<14:54:30,  1.24s/it] 13%|█▎        | 6547/50000 [2:20:19<14:53:51,  1.23s/it] 13%|█▎        | 6548/50000 [2:20:20<14:53:09,  1.23s/it] 13%|█▎        | 6549/50000 [2:20:21<14:52:55,  1.23s/it] 13%|█▎        | 6550/50000 [2:20:22<14:53:45,  1.23s/it]                                                         {'loss': 119.4578, 'learning_rate': 1.9658287565700172e-05, 'epoch': 1.02}
 13%|█▎        | 6550/50000 [2:20:22<14:53:45,  1.23s/it] 13%|█▎        | 6551/50000 [2:20:24<14:54:57,  1.24s/it] 13%|█▎        | 6552/50000 [2:20:25<14:55:41,  1.24s/it] 13%|█▎        | 6553/50000 [2:20:26<14:55:30,  1.24s/it] 13%|█▎        | 6554/50000 [2:20:27<14:56:16,  1.24s/it] 13%|█▎        | 6555/50000 [2:20:29<14:56:22,  1.24s/it] 13%|█▎        | 6556/50000 [2:20:30<14:56:43,  1.24s/it] 13%|█▎        | 6557/50000 [2:20:31<14:56:55,  1.24s/it] 13%|█▎        | 6558/50000 [2:20:32<14:56:37,  1.24s/it] 13%|█▎        | 6559/50000 [2:20:34<14:56:37,  1.24s/it] 13%|█▎        | 6560/50000 [2:20:35<14:56:46,  1.24s/it]                                                         {'loss': 106.7422, 'learning_rate': 1.96565712612668e-05, 'epoch': 1.02}
 13%|█▎        | 6560/50000 [2:20:35<14:56:46,  1.24s/it] 13%|█▎        | 6561/50000 [2:20:36<14:56:04,  1.24s/it] 13%|█▎        | 6562/50000 [2:20:37<14:55:09,  1.24s/it] 13%|█▎        | 6563/50000 [2:20:38<14:54:39,  1.24s/it] 13%|█▎        | 6564/50000 [2:20:40<14:54:18,  1.24s/it] 13%|█▎        | 6565/50000 [2:20:41<14:53:49,  1.23s/it] 13%|█▎        | 6566/50000 [2:20:42<14:53:19,  1.23s/it] 13%|█▎        | 6567/50000 [2:20:43<14:52:56,  1.23s/it] 13%|█▎        | 6568/50000 [2:20:45<14:52:52,  1.23s/it] 13%|█▎        | 6569/50000 [2:20:46<14:52:38,  1.23s/it] 13%|█▎        | 6570/50000 [2:20:47<14:52:29,  1.23s/it]                                                         {'loss': 118.9516, 'learning_rate': 1.965485073272108e-05, 'epoch': 1.02}
 13%|█▎        | 6570/50000 [2:20:47<14:52:29,  1.23s/it] 13%|█▎        | 6571/50000 [2:20:48<14:52:24,  1.23s/it] 13%|█▎        | 6572/50000 [2:20:50<14:52:29,  1.23s/it] 13%|█▎        | 6573/50000 [2:20:51<14:52:19,  1.23s/it][2023-07-03 14:21:02,307] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
 13%|█▎        | 6574/50000 [2:20:52<13:58:14,  1.16s/it] 13%|█▎        | 6575/50000 [2:20:53<14:13:54,  1.18s/it] 13%|█▎        | 6576/50000 [2:20:54<14:25:18,  1.20s/it] 13%|█▎        | 6577/50000 [2:20:55<14:33:27,  1.21s/it][2023-07-03 14:21:06,987] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 13%|█▎        | 6578/50000 [2:20:56<13:45:03,  1.14s/it] 13%|█▎        | 6579/50000 [2:20:58<14:04:38,  1.17s/it] 13%|█▎        | 6580/50000 [2:20:59<14:18:39,  1.19s/it]                                                         {'loss': 119.2219, 'learning_rate': 1.9653471269029317e-05, 'epoch': 1.02}
 13%|█▎        | 6580/50000 [2:20:59<14:18:39,  1.19s/it] 13%|█▎        | 6581/50000 [2:21:00<14:28:24,  1.20s/it] 13%|█▎        | 6582/50000 [2:21:01<14:35:15,  1.21s/it] 13%|█▎        | 6583/50000 [2:21:03<14:39:47,  1.22s/it] 13%|█▎        | 6584/50000 [2:21:04<14:43:08,  1.22s/it] 13%|█▎        | 6585/50000 [2:21:05<14:45:18,  1.22s/it] 13%|█▎        | 6586/50000 [2:21:06<14:46:55,  1.23s/it] 13%|█▎        | 6587/50000 [2:21:08<14:48:03,  1.23s/it] 13%|█▎        | 6588/50000 [2:21:09<14:48:50,  1.23s/it] 13%|█▎        | 6589/50000 [2:21:10<14:49:14,  1.23s/it] 13%|█▎        | 6590/50000 [2:21:11<14:49:42,  1.23s/it]                                                         {'loss': 121.3406, 'learning_rate': 1.9651743138979222e-05, 'epoch': 1.02}
 13%|█▎        | 6590/50000 [2:21:11<14:49:42,  1.23s/it] 13%|█▎        | 6591/50000 [2:21:12<14:50:03,  1.23s/it] 13%|█▎        | 6592/50000 [2:21:14<14:50:29,  1.23s/it] 13%|█▎        | 6593/50000 [2:21:15<14:50:50,  1.23s/it] 13%|█▎        | 6594/50000 [2:21:16<14:50:57,  1.23s/it] 13%|█▎        | 6595/50000 [2:21:17<14:51:07,  1.23s/it] 13%|█▎        | 6596/50000 [2:21:19<14:51:07,  1.23s/it] 13%|█▎        | 6597/50000 [2:21:20<14:51:08,  1.23s/it] 13%|█▎        | 6598/50000 [2:21:21<14:51:07,  1.23s/it] 13%|█▎        | 6599/50000 [2:21:22<14:51:04,  1.23s/it] 13%|█▎        | 6600/50000 [2:21:24<14:51:21,  1.23s/it]                                                         {'loss': 112.0656, 'learning_rate': 1.9650010786928767e-05, 'epoch': 1.02}
 13%|█▎        | 6600/50000 [2:21:24<14:51:21,  1.23s/it] 13%|█▎        | 6601/50000 [2:21:25<14:52:35,  1.23s/it] 13%|█▎        | 6602/50000 [2:21:26<14:53:14,  1.23s/it] 13%|█▎        | 6603/50000 [2:21:27<14:54:13,  1.24s/it] 13%|█▎        | 6604/50000 [2:21:29<14:54:12,  1.24s/it] 13%|█▎        | 6605/50000 [2:21:30<14:54:23,  1.24s/it] 13%|█▎        | 6606/50000 [2:21:31<14:54:43,  1.24s/it] 13%|█▎        | 6607/50000 [2:21:32<14:54:59,  1.24s/it] 13%|█▎        | 6608/50000 [2:21:33<14:54:50,  1.24s/it] 13%|█▎        | 6609/50000 [2:21:35<14:55:03,  1.24s/it] 13%|█▎        | 6610/50000 [2:21:36<14:55:01,  1.24s/it]                                                         {'loss': 120.5102, 'learning_rate': 1.9648274213635745e-05, 'epoch': 1.02}
 13%|█▎        | 6610/50000 [2:21:36<14:55:01,  1.24s/it] 13%|█▎        | 6611/50000 [2:21:37<14:55:00,  1.24s/it] 13%|█▎        | 6612/50000 [2:21:38<14:55:08,  1.24s/it] 13%|█▎        | 6613/50000 [2:21:40<14:55:00,  1.24s/it] 13%|█▎        | 6614/50000 [2:21:41<14:55:07,  1.24s/it] 13%|█▎        | 6615/50000 [2:21:42<14:55:07,  1.24s/it] 13%|█▎        | 6616/50000 [2:21:43<14:55:23,  1.24s/it] 13%|█▎        | 6617/50000 [2:21:45<14:55:34,  1.24s/it] 13%|█▎        | 6618/50000 [2:21:46<14:55:18,  1.24s/it] 13%|█▎        | 6619/50000 [2:21:47<14:55:09,  1.24s/it] 13%|█▎        | 6620/50000 [2:21:48<14:54:55,  1.24s/it]                                                         {'loss': 114.2859, 'learning_rate': 1.964653341985979e-05, 'epoch': 1.03}
 13%|█▎        | 6620/50000 [2:21:48<14:54:55,  1.24s/it] 13%|█▎        | 6621/50000 [2:21:50<14:55:16,  1.24s/it] 13%|█▎        | 6622/50000 [2:21:51<14:55:20,  1.24s/it] 13%|█▎        | 6623/50000 [2:21:52<14:55:07,  1.24s/it] 13%|█▎        | 6624/50000 [2:21:53<14:55:36,  1.24s/it] 13%|█▎        | 6625/50000 [2:21:55<14:55:18,  1.24s/it] 13%|█▎        | 6626/50000 [2:21:56<14:55:10,  1.24s/it] 13%|█▎        | 6627/50000 [2:21:57<14:54:59,  1.24s/it] 13%|█▎        | 6628/50000 [2:21:58<14:55:01,  1.24s/it] 13%|█▎        | 6629/50000 [2:21:59<14:54:55,  1.24s/it] 13%|█▎        | 6630/50000 [2:22:01<14:54:57,  1.24s/it]                                                         {'loss': 122.5547, 'learning_rate': 1.9644788406362382e-05, 'epoch': 1.03}
 13%|█▎        | 6630/50000 [2:22:01<14:54:57,  1.24s/it] 13%|█▎        | 6631/50000 [2:22:02<14:55:09,  1.24s/it] 13%|█▎        | 6632/50000 [2:22:03<14:55:00,  1.24s/it] 13%|█▎        | 6633/50000 [2:22:04<14:54:53,  1.24s/it] 13%|█▎        | 6634/50000 [2:22:06<14:55:01,  1.24s/it] 13%|█▎        | 6635/50000 [2:22:07<14:54:44,  1.24s/it] 13%|█▎        | 6636/50000 [2:22:08<14:54:44,  1.24s/it] 13%|█▎        | 6637/50000 [2:22:09<14:54:40,  1.24s/it] 13%|█▎        | 6638/50000 [2:22:11<14:55:06,  1.24s/it] 13%|█▎        | 6639/50000 [2:22:12<14:54:44,  1.24s/it] 13%|█▎        | 6640/50000 [2:22:13<14:54:45,  1.24s/it]                                                         {'loss': 126.9938, 'learning_rate': 1.9643039173906853e-05, 'epoch': 1.03}
 13%|█▎        | 6640/50000 [2:22:13<14:54:45,  1.24s/it] 13%|█▎        | 6641/50000 [2:22:14<14:54:48,  1.24s/it] 13%|█▎        | 6642/50000 [2:22:16<14:54:31,  1.24s/it] 13%|█▎        | 6643/50000 [2:22:17<14:54:24,  1.24s/it] 13%|█▎        | 6644/50000 [2:22:18<14:54:35,  1.24s/it] 13%|█▎        | 6645/50000 [2:22:19<14:54:42,  1.24s/it][2023-07-03 14:22:30,784] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
 13%|█▎        | 6646/50000 [2:22:20<14:00:30,  1.16s/it] 13%|█▎        | 6647/50000 [2:22:22<14:16:30,  1.19s/it] 13%|█▎        | 6648/50000 [2:22:23<14:28:17,  1.20s/it] 13%|█▎        | 6649/50000 [2:22:24<14:36:06,  1.21s/it] 13%|█▎        | 6650/50000 [2:22:25<14:41:35,  1.22s/it]                                                         {'loss': 119.8438, 'learning_rate': 1.964146125812006e-05, 'epoch': 1.03}
 13%|█▎        | 6650/50000 [2:22:25<14:41:35,  1.22s/it] 13%|█▎        | 6651/50000 [2:22:26<14:45:50,  1.23s/it] 13%|█▎        | 6652/50000 [2:22:28<14:48:19,  1.23s/it] 13%|█▎        | 6653/50000 [2:22:29<14:49:48,  1.23s/it] 13%|█▎        | 6654/50000 [2:22:30<14:51:19,  1.23s/it] 13%|█▎        | 6655/50000 [2:22:31<14:52:19,  1.24s/it] 13%|█▎        | 6656/50000 [2:22:33<14:52:58,  1.24s/it] 13%|█▎        | 6657/50000 [2:22:34<14:53:16,  1.24s/it] 13%|█▎        | 6658/50000 [2:22:35<14:53:24,  1.24s/it] 13%|█▎        | 6659/50000 [2:22:36<14:53:19,  1.24s/it] 13%|█▎        | 6660/50000 [2:22:38<14:53:31,  1.24s/it]                                                         {'loss': 108.1391, 'learning_rate': 1.963970401175368e-05, 'epoch': 1.03}
 13%|█▎        | 6660/50000 [2:22:38<14:53:31,  1.24s/it] 13%|█▎        | 6661/50000 [2:22:39<14:54:12,  1.24s/it] 13%|█▎        | 6662/50000 [2:22:40<14:54:03,  1.24s/it] 13%|█▎        | 6663/50000 [2:22:41<14:54:15,  1.24s/it] 13%|█▎        | 6664/50000 [2:22:43<14:54:04,  1.24s/it] 13%|█▎        | 6665/50000 [2:22:44<14:53:55,  1.24s/it] 13%|█▎        | 6666/50000 [2:22:45<14:53:46,  1.24s/it] 13%|█▎        | 6667/50000 [2:22:46<14:53:57,  1.24s/it] 13%|█▎        | 6668/50000 [2:22:48<14:53:44,  1.24s/it] 13%|█▎        | 6669/50000 [2:22:49<14:53:46,  1.24s/it] 13%|█▎        | 6670/50000 [2:22:50<14:53:48,  1.24s/it]                                                         {'loss': 110.8797, 'learning_rate': 1.963794254865326e-05, 'epoch': 1.03}
 13%|█▎        | 6670/50000 [2:22:50<14:53:48,  1.24s/it] 13%|█▎        | 6671/50000 [2:22:51<14:54:02,  1.24s/it] 13%|█▎        | 6672/50000 [2:22:52<14:53:46,  1.24s/it] 13%|█▎        | 6673/50000 [2:22:54<14:53:46,  1.24s/it] 13%|█▎        | 6674/50000 [2:22:55<14:53:45,  1.24s/it] 13%|█▎        | 6675/50000 [2:22:56<14:53:40,  1.24s/it] 13%|█▎        | 6676/50000 [2:22:57<14:53:39,  1.24s/it] 13%|█▎        | 6677/50000 [2:22:59<14:53:45,  1.24s/it] 13%|█▎        | 6678/50000 [2:23:00<14:53:39,  1.24s/it] 13%|█▎        | 6679/50000 [2:23:01<14:53:32,  1.24s/it] 13%|█▎        | 6680/50000 [2:23:02<14:53:25,  1.24s/it]                                                         {'loss': 133.6281, 'learning_rate': 1.963617686958933e-05, 'epoch': 1.04}
 13%|█▎        | 6680/50000 [2:23:02<14:53:25,  1.24s/it] 13%|█▎        | 6681/50000 [2:23:04<14:53:21,  1.24s/it] 13%|█▎        | 6682/50000 [2:23:05<14:53:43,  1.24s/it] 13%|█▎        | 6683/50000 [2:23:06<14:53:49,  1.24s/it] 13%|█▎        | 6684/50000 [2:23:07<14:53:33,  1.24s/it] 13%|█▎        | 6685/50000 [2:23:09<14:53:54,  1.24s/it] 13%|█▎        | 6686/50000 [2:23:10<14:54:09,  1.24s/it] 13%|█▎        | 6687/50000 [2:23:11<14:54:23,  1.24s/it] 13%|█▎        | 6688/50000 [2:23:12<14:54:06,  1.24s/it] 13%|█▎        | 6689/50000 [2:23:14<14:54:14,  1.24s/it] 13%|█▎        | 6690/50000 [2:23:15<14:54:19,  1.24s/it]                                                         {'loss': 127.8547, 'learning_rate': 1.9634406975334257e-05, 'epoch': 1.04}
 13%|█▎        | 6690/50000 [2:23:15<14:54:19,  1.24s/it] 13%|█▎        | 6691/50000 [2:23:16<14:54:20,  1.24s/it] 13%|█▎        | 6692/50000 [2:23:17<14:54:31,  1.24s/it] 13%|█▎        | 6693/50000 [2:23:18<14:54:32,  1.24s/it] 13%|█▎        | 6694/50000 [2:23:20<14:54:22,  1.24s/it] 13%|█▎        | 6695/50000 [2:23:21<14:54:30,  1.24s/it] 13%|█▎        | 6696/50000 [2:23:22<14:54:34,  1.24s/it] 13%|█▎        | 6697/50000 [2:23:23<14:54:17,  1.24s/it] 13%|█▎        | 6698/50000 [2:23:25<14:54:20,  1.24s/it] 13%|█▎        | 6699/50000 [2:23:26<14:54:13,  1.24s/it] 13%|█▎        | 6700/50000 [2:23:27<14:54:12,  1.24s/it]                                                         {'loss': 109.6281, 'learning_rate': 1.9632632866662255e-05, 'epoch': 1.04}
 13%|█▎        | 6700/50000 [2:23:27<14:54:12,  1.24s/it] 13%|█▎        | 6701/50000 [2:23:28<14:54:29,  1.24s/it] 13%|█▎        | 6702/50000 [2:23:30<14:55:46,  1.24s/it] 13%|█▎        | 6703/50000 [2:23:31<14:55:20,  1.24s/it] 13%|█▎        | 6704/50000 [2:23:32<14:54:59,  1.24s/it] 13%|█▎        | 6705/50000 [2:23:33<14:54:48,  1.24s/it] 13%|█▎        | 6706/50000 [2:23:35<14:54:07,  1.24s/it] 13%|█▎        | 6707/50000 [2:23:36<14:54:10,  1.24s/it] 13%|█▎        | 6708/50000 [2:23:37<14:54:13,  1.24s/it] 13%|█▎        | 6709/50000 [2:23:38<14:53:45,  1.24s/it] 13%|█▎        | 6710/50000 [2:23:40<14:53:17,  1.24s/it]                                                         {'loss': 126.3141, 'learning_rate': 1.9630854544349372e-05, 'epoch': 1.04}
 13%|█▎        | 6710/50000 [2:23:40<14:53:17,  1.24s/it] 13%|█▎        | 6711/50000 [2:23:41<14:53:21,  1.24s/it] 13%|█▎        | 6712/50000 [2:23:42<14:53:27,  1.24s/it] 13%|█▎        | 6713/50000 [2:23:43<14:53:11,  1.24s/it] 13%|█▎        | 6714/50000 [2:23:44<14:53:00,  1.24s/it] 13%|█▎        | 6715/50000 [2:23:46<14:52:52,  1.24s/it] 13%|█▎        | 6716/50000 [2:23:47<14:52:50,  1.24s/it] 13%|█▎        | 6717/50000 [2:23:48<14:52:55,  1.24s/it] 13%|█▎        | 6718/50000 [2:23:50<15:21:25,  1.28s/it] 13%|█▎        | 6719/50000 [2:23:51<15:13:35,  1.27s/it] 13%|█▎        | 6720/50000 [2:23:52<15:08:50,  1.26s/it]                                                         {'loss': 107.8297, 'learning_rate': 1.9629072009173514e-05, 'epoch': 1.04}
 13%|█▎        | 6720/50000 [2:23:52<15:08:50,  1.26s/it] 13%|█▎        | 6721/50000 [2:23:53<15:08:12,  1.26s/it] 13%|█▎        | 6722/50000 [2:23:55<15:06:52,  1.26s/it] 13%|█▎        | 6723/50000 [2:23:56<15:03:50,  1.25s/it] 13%|█▎        | 6724/50000 [2:23:57<15:01:31,  1.25s/it] 13%|█▎        | 6725/50000 [2:23:58<14:59:47,  1.25s/it] 13%|█▎        | 6726/50000 [2:24:00<14:59:38,  1.25s/it] 13%|█▎        | 6727/50000 [2:24:01<14:59:52,  1.25s/it] 13%|█▎        | 6728/50000 [2:24:02<14:59:27,  1.25s/it] 13%|█▎        | 6729/50000 [2:24:03<14:58:22,  1.25s/it] 13%|█▎        | 6730/50000 [2:24:05<14:57:39,  1.24s/it]                                                         {'loss': 116.9547, 'learning_rate': 1.962728526191442e-05, 'epoch': 1.04}
 13%|█▎        | 6730/50000 [2:24:05<14:57:39,  1.24s/it] 13%|█▎        | 6731/50000 [2:24:06<14:57:05,  1.24s/it] 13%|█▎        | 6732/50000 [2:24:07<14:56:10,  1.24s/it] 13%|█▎        | 6733/50000 [2:24:08<14:56:30,  1.24s/it] 13%|█▎        | 6734/50000 [2:24:09<14:55:41,  1.24s/it] 13%|█▎        | 6735/50000 [2:24:11<14:55:52,  1.24s/it] 13%|█▎        | 6736/50000 [2:24:12<14:55:57,  1.24s/it] 13%|█▎        | 6737/50000 [2:24:13<14:55:50,  1.24s/it] 13%|█▎        | 6738/50000 [2:24:14<14:55:44,  1.24s/it] 13%|█▎        | 6739/50000 [2:24:16<14:56:06,  1.24s/it] 13%|█▎        | 6740/50000 [2:24:17<14:55:31,  1.24s/it]                                                         {'loss': 111.2695, 'learning_rate': 1.9625494303353673e-05, 'epoch': 1.04}
 13%|█▎        | 6740/50000 [2:24:17<14:55:31,  1.24s/it] 13%|█▎        | 6741/50000 [2:24:18<14:55:25,  1.24s/it] 13%|█▎        | 6742/50000 [2:24:19<14:54:45,  1.24s/it] 13%|█▎        | 6743/50000 [2:24:21<14:54:06,  1.24s/it] 13%|█▎        | 6744/50000 [2:24:22<14:54:34,  1.24s/it] 13%|█▎        | 6745/50000 [2:24:23<14:55:03,  1.24s/it] 13%|█▎        | 6746/50000 [2:24:24<14:55:27,  1.24s/it] 13%|█▎        | 6747/50000 [2:24:26<14:55:22,  1.24s/it] 13%|█▎        | 6748/50000 [2:24:27<14:54:51,  1.24s/it] 13%|█▎        | 6749/50000 [2:24:28<14:54:03,  1.24s/it] 14%|█▎        | 6750/50000 [2:24:29<14:53:19,  1.24s/it]                                                         {'loss': 126.4367, 'learning_rate': 1.9623699134274702e-05, 'epoch': 1.05}
 14%|█▎        | 6750/50000 [2:24:29<14:53:19,  1.24s/it] 14%|█▎        | 6751/50000 [2:24:31<14:53:11,  1.24s/it] 14%|█▎        | 6752/50000 [2:24:32<14:54:08,  1.24s/it] 14%|█▎        | 6753/50000 [2:24:33<14:54:12,  1.24s/it] 14%|█▎        | 6754/50000 [2:24:34<14:54:08,  1.24s/it] 14%|█▎        | 6755/50000 [2:24:36<14:54:46,  1.24s/it] 14%|█▎        | 6756/50000 [2:24:37<14:54:46,  1.24s/it] 14%|█▎        | 6757/50000 [2:24:38<14:54:49,  1.24s/it] 14%|█▎        | 6758/50000 [2:24:39<14:54:32,  1.24s/it] 14%|█▎        | 6759/50000 [2:24:41<14:54:21,  1.24s/it] 14%|█▎        | 6760/50000 [2:24:42<14:54:38,  1.24s/it]                                                         {'loss': 125.9594, 'learning_rate': 1.962189975546277e-05, 'epoch': 1.05}
 14%|█▎        | 6760/50000 [2:24:42<14:54:38,  1.24s/it] 14%|█▎        | 6761/50000 [2:24:43<14:54:34,  1.24s/it] 14%|█▎        | 6762/50000 [2:24:44<14:54:38,  1.24s/it] 14%|█▎        | 6763/50000 [2:24:45<14:55:19,  1.24s/it] 14%|█▎        | 6764/50000 [2:24:47<14:55:09,  1.24s/it] 14%|█▎        | 6765/50000 [2:24:48<14:54:53,  1.24s/it] 14%|█▎        | 6766/50000 [2:24:49<14:55:02,  1.24s/it] 14%|█▎        | 6767/50000 [2:24:50<14:55:00,  1.24s/it] 14%|█▎        | 6768/50000 [2:24:52<14:54:30,  1.24s/it] 14%|█▎        | 6769/50000 [2:24:53<14:54:26,  1.24s/it] 14%|█▎        | 6770/50000 [2:24:54<14:54:18,  1.24s/it]                                                         {'loss': 110.6555, 'learning_rate': 1.962009616770499e-05, 'epoch': 1.05}
 14%|█▎        | 6770/50000 [2:24:54<14:54:18,  1.24s/it] 14%|█▎        | 6771/50000 [2:24:55<14:54:29,  1.24s/it] 14%|█▎        | 6772/50000 [2:24:57<14:55:05,  1.24s/it] 14%|█▎        | 6773/50000 [2:24:58<14:55:08,  1.24s/it] 14%|█▎        | 6774/50000 [2:24:59<14:56:17,  1.24s/it] 14%|█▎        | 6775/50000 [2:25:00<14:58:54,  1.25s/it] 14%|█▎        | 6776/50000 [2:25:02<14:57:38,  1.25s/it] 14%|█▎        | 6777/50000 [2:25:03<14:56:37,  1.24s/it] 14%|█▎        | 6778/50000 [2:25:04<14:57:14,  1.25s/it] 14%|█▎        | 6779/50000 [2:25:05<14:59:34,  1.25s/it] 14%|█▎        | 6780/50000 [2:25:07<14:59:11,  1.25s/it]                                                         {'loss': 114.4156, 'learning_rate': 1.961828837179031e-05, 'epoch': 1.05}
 14%|█▎        | 6780/50000 [2:25:07<14:59:11,  1.25s/it] 14%|█▎        | 6781/50000 [2:25:08<14:58:40,  1.25s/it] 14%|█▎        | 6782/50000 [2:25:09<14:58:18,  1.25s/it] 14%|█▎        | 6783/50000 [2:25:10<15:00:36,  1.25s/it] 14%|█▎        | 6784/50000 [2:25:12<14:59:27,  1.25s/it] 14%|█▎        | 6785/50000 [2:25:13<14:57:49,  1.25s/it] 14%|█▎        | 6786/50000 [2:25:14<14:56:33,  1.24s/it] 14%|█▎        | 6787/50000 [2:25:15<14:56:13,  1.24s/it] 14%|█▎        | 6788/50000 [2:25:17<14:55:21,  1.24s/it] 14%|█▎        | 6789/50000 [2:25:18<14:55:41,  1.24s/it] 14%|█▎        | 6790/50000 [2:25:19<14:56:06,  1.24s/it]                                                         {'loss': 106.7977, 'learning_rate': 1.9616476368509526e-05, 'epoch': 1.05}
 14%|█▎        | 6790/50000 [2:25:19<14:56:06,  1.24s/it] 14%|█▎        | 6791/50000 [2:25:20<14:56:15,  1.24s/it] 14%|█▎        | 6792/50000 [2:25:22<14:55:55,  1.24s/it] 14%|█▎        | 6793/50000 [2:25:23<14:55:44,  1.24s/it] 14%|█▎        | 6794/50000 [2:25:24<14:57:20,  1.25s/it] 14%|█▎        | 6795/50000 [2:25:25<14:55:59,  1.24s/it] 14%|█▎        | 6796/50000 [2:25:27<14:55:38,  1.24s/it] 14%|█▎        | 6797/50000 [2:25:28<14:55:28,  1.24s/it] 14%|█▎        | 6798/50000 [2:25:29<14:55:13,  1.24s/it] 14%|█▎        | 6799/50000 [2:25:30<14:55:05,  1.24s/it] 14%|█▎        | 6800/50000 [2:25:32<14:56:03,  1.24s/it]                                                         {'loss': 116.7469, 'learning_rate': 1.961466015865527e-05, 'epoch': 1.05}
 14%|█▎        | 6800/50000 [2:25:32<14:56:03,  1.24s/it] 14%|█▎        | 6801/50000 [2:25:33<14:56:12,  1.24s/it] 14%|█▎        | 6802/50000 [2:25:34<14:55:27,  1.24s/it] 14%|█▎        | 6803/50000 [2:25:35<14:54:27,  1.24s/it] 14%|█▎        | 6804/50000 [2:25:36<14:54:00,  1.24s/it] 14%|█▎        | 6805/50000 [2:25:38<14:54:24,  1.24s/it] 14%|█▎        | 6806/50000 [2:25:39<14:54:37,  1.24s/it] 14%|█▎        | 6807/50000 [2:25:40<14:54:30,  1.24s/it] 14%|█▎        | 6808/50000 [2:25:41<14:54:25,  1.24s/it] 14%|█▎        | 6809/50000 [2:25:43<14:55:20,  1.24s/it] 14%|█▎        | 6810/50000 [2:25:44<14:55:25,  1.24s/it]                                                         {'loss': 115.1156, 'learning_rate': 1.9612839743022004e-05, 'epoch': 1.06}
 14%|█▎        | 6810/50000 [2:25:44<14:55:25,  1.24s/it] 14%|█▎        | 6811/50000 [2:25:45<14:55:18,  1.24s/it] 14%|█▎        | 6812/50000 [2:25:46<14:55:05,  1.24s/it] 14%|█▎        | 6813/50000 [2:25:48<14:55:24,  1.24s/it] 14%|█▎        | 6814/50000 [2:25:49<14:54:50,  1.24s/it] 14%|█▎        | 6815/50000 [2:25:50<14:55:01,  1.24s/it] 14%|█▎        | 6816/50000 [2:25:51<14:54:24,  1.24s/it] 14%|█▎        | 6817/50000 [2:25:53<14:53:59,  1.24s/it] 14%|█▎        | 6818/50000 [2:25:54<14:53:39,  1.24s/it] 14%|█▎        | 6819/50000 [2:25:55<14:53:23,  1.24s/it] 14%|█▎        | 6820/50000 [2:25:56<14:53:04,  1.24s/it]                                                         {'loss': 112.0906, 'learning_rate': 1.961101512240605e-05, 'epoch': 1.06}
 14%|█▎        | 6820/50000 [2:25:56<14:53:04,  1.24s/it] 14%|█▎        | 6821/50000 [2:25:58<14:53:15,  1.24s/it] 14%|█▎        | 6822/50000 [2:25:59<14:52:56,  1.24s/it] 14%|█▎        | 6823/50000 [2:26:00<14:53:03,  1.24s/it] 14%|█▎        | 6824/50000 [2:26:01<14:52:44,  1.24s/it] 14%|█▎        | 6825/50000 [2:26:03<14:52:52,  1.24s/it] 14%|█▎        | 6826/50000 [2:26:04<14:52:50,  1.24s/it] 14%|█▎        | 6827/50000 [2:26:05<14:52:57,  1.24s/it] 14%|█▎        | 6828/50000 [2:26:06<14:52:48,  1.24s/it] 14%|█▎        | 6829/50000 [2:26:08<14:52:51,  1.24s/it] 14%|█▎        | 6830/50000 [2:26:09<14:52:47,  1.24s/it]                                                         {'loss': 112.4625, 'learning_rate': 1.9609186297605558e-05, 'epoch': 1.06}
 14%|█▎        | 6830/50000 [2:26:09<14:52:47,  1.24s/it] 14%|█▎        | 6831/50000 [2:26:10<14:53:03,  1.24s/it] 14%|█▎        | 6832/50000 [2:26:11<14:53:04,  1.24s/it] 14%|█▎        | 6833/50000 [2:26:13<14:52:53,  1.24s/it] 14%|█▎        | 6834/50000 [2:26:14<14:53:00,  1.24s/it] 14%|█▎        | 6835/50000 [2:26:15<14:52:52,  1.24s/it] 14%|█▎        | 6836/50000 [2:26:16<14:52:44,  1.24s/it] 14%|█▎        | 6837/50000 [2:26:17<14:52:26,  1.24s/it] 14%|█▎        | 6838/50000 [2:26:19<14:52:14,  1.24s/it] 14%|█▎        | 6839/50000 [2:26:20<14:53:23,  1.24s/it] 14%|█▎        | 6840/50000 [2:26:21<14:53:17,  1.24s/it]                                                         {'loss': 118.7164, 'learning_rate': 1.960735326942051e-05, 'epoch': 1.06}
 14%|█▎        | 6840/50000 [2:26:21<14:53:17,  1.24s/it] 14%|█▎        | 6841/50000 [2:26:22<14:53:04,  1.24s/it] 14%|█▎        | 6842/50000 [2:26:24<14:52:52,  1.24s/it] 14%|█▎        | 6843/50000 [2:26:25<14:52:31,  1.24s/it] 14%|█▎        | 6844/50000 [2:26:26<14:52:51,  1.24s/it] 14%|█▎        | 6845/50000 [2:26:27<14:52:30,  1.24s/it] 14%|█▎        | 6846/50000 [2:26:29<14:53:04,  1.24s/it] 14%|█▎        | 6847/50000 [2:26:30<14:52:27,  1.24s/it] 14%|█▎        | 6848/50000 [2:26:31<14:53:15,  1.24s/it] 14%|█▎        | 6849/50000 [2:26:32<14:55:48,  1.25s/it] 14%|█▎        | 6850/50000 [2:26:34<14:55:36,  1.25s/it]                                                         {'loss': 111.55, 'learning_rate': 1.9605516038652747e-05, 'epoch': 1.06}
 14%|█▎        | 6850/50000 [2:26:34<14:55:36,  1.25s/it] 14%|█▎        | 6851/50000 [2:26:35<14:55:38,  1.25s/it] 14%|█▎        | 6852/50000 [2:26:36<14:54:49,  1.24s/it] 14%|█▎        | 6853/50000 [2:26:37<14:55:15,  1.24s/it] 14%|█▎        | 6854/50000 [2:26:39<14:54:15,  1.24s/it] 14%|█▎        | 6855/50000 [2:26:40<14:53:41,  1.24s/it] 14%|█▎        | 6856/50000 [2:26:41<14:53:19,  1.24s/it] 14%|█▎        | 6857/50000 [2:26:42<14:53:11,  1.24s/it] 14%|█▎        | 6858/50000 [2:26:44<14:53:27,  1.24s/it] 14%|█▎        | 6859/50000 [2:26:45<14:53:08,  1.24s/it] 14%|█▎        | 6860/50000 [2:26:46<14:53:44,  1.24s/it]                                                         {'loss': 117.925, 'learning_rate': 1.9603674606105924e-05, 'epoch': 1.06}
 14%|█▎        | 6860/50000 [2:26:46<14:53:44,  1.24s/it] 14%|█▎        | 6861/50000 [2:26:47<14:53:23,  1.24s/it] 14%|█▎        | 6862/50000 [2:26:49<14:53:17,  1.24s/it] 14%|█▎        | 6863/50000 [2:26:50<14:52:44,  1.24s/it] 14%|█▎        | 6864/50000 [2:26:51<14:52:20,  1.24s/it] 14%|█▎        | 6865/50000 [2:26:52<14:52:31,  1.24s/it] 14%|█▎        | 6866/50000 [2:26:54<14:52:36,  1.24s/it] 14%|█▎        | 6867/50000 [2:26:55<14:52:51,  1.24s/it] 14%|█▎        | 6868/50000 [2:26:56<14:52:28,  1.24s/it] 14%|█▎        | 6869/50000 [2:26:57<14:52:29,  1.24s/it] 14%|█▎        | 6870/50000 [2:26:58<14:53:02,  1.24s/it]                                                         {'loss': 116.5445, 'learning_rate': 1.9601828972585554e-05, 'epoch': 1.06}
 14%|█▎        | 6870/50000 [2:26:58<14:53:02,  1.24s/it] 14%|█▎        | 6871/50000 [2:27:00<14:53:48,  1.24s/it] 14%|█▎        | 6872/50000 [2:27:01<14:53:27,  1.24s/it] 14%|█▎        | 6873/50000 [2:27:02<14:53:18,  1.24s/it] 14%|█▎        | 6874/50000 [2:27:03<14:52:59,  1.24s/it] 14%|█▍        | 6875/50000 [2:27:05<14:54:17,  1.24s/it] 14%|█▍        | 6876/50000 [2:27:06<14:56:12,  1.25s/it] 14%|█▍        | 6877/50000 [2:27:07<14:56:01,  1.25s/it] 14%|█▍        | 6878/50000 [2:27:08<14:55:41,  1.25s/it] 14%|█▍        | 6879/50000 [2:27:10<15:07:20,  1.26s/it] 14%|█▍        | 6880/50000 [2:27:11<15:03:41,  1.26s/it]                                                         {'loss': 101.4742, 'learning_rate': 1.9599979138898978e-05, 'epoch': 1.07}
 14%|█▍        | 6880/50000 [2:27:11<15:03:41,  1.26s/it] 14%|█▍        | 6881/50000 [2:27:12<15:02:32,  1.26s/it] 14%|█▍        | 6882/50000 [2:27:13<15:01:30,  1.25s/it] 14%|█▍        | 6883/50000 [2:27:15<15:00:43,  1.25s/it] 14%|█▍        | 6884/50000 [2:27:16<14:59:14,  1.25s/it] 14%|█▍        | 6885/50000 [2:27:17<14:59:27,  1.25s/it] 14%|█▍        | 6886/50000 [2:27:18<14:58:40,  1.25s/it] 14%|█▍        | 6887/50000 [2:27:20<14:58:15,  1.25s/it] 14%|█▍        | 6888/50000 [2:27:21<14:57:24,  1.25s/it] 14%|█▍        | 6889/50000 [2:27:22<14:56:51,  1.25s/it] 14%|█▍        | 6890/50000 [2:27:23<14:56:42,  1.25s/it]                                                         {'loss': 119.35, 'learning_rate': 1.9598125105855374e-05, 'epoch': 1.07}
 14%|█▍        | 6890/50000 [2:27:23<14:56:42,  1.25s/it] 14%|█▍        | 6891/50000 [2:27:25<14:57:31,  1.25s/it] 14%|█▍        | 6892/50000 [2:27:26<14:57:20,  1.25s/it] 14%|█▍        | 6893/50000 [2:27:27<14:56:15,  1.25s/it] 14%|█▍        | 6894/50000 [2:27:28<14:56:06,  1.25s/it] 14%|█▍        | 6895/50000 [2:27:30<14:55:00,  1.25s/it] 14%|█▍        | 6896/50000 [2:27:31<14:54:33,  1.25s/it] 14%|█▍        | 6897/50000 [2:27:32<14:54:58,  1.25s/it] 14%|█▍        | 6898/50000 [2:27:33<14:54:08,  1.24s/it] 14%|█▍        | 6899/50000 [2:27:35<14:54:10,  1.24s/it] 14%|█▍        | 6900/50000 [2:27:36<14:53:45,  1.24s/it]                                                         {'loss': 118.6477, 'learning_rate': 1.9596266874265762e-05, 'epoch': 1.07}
 14%|█▍        | 6900/50000 [2:27:36<14:53:45,  1.24s/it] 14%|█▍        | 6901/50000 [2:27:37<14:53:14,  1.24s/it] 14%|█▍        | 6902/50000 [2:27:38<14:53:01,  1.24s/it] 14%|█▍        | 6903/50000 [2:27:40<14:53:15,  1.24s/it] 14%|█▍        | 6904/50000 [2:27:41<14:54:46,  1.25s/it] 14%|█▍        | 6905/50000 [2:27:42<14:54:29,  1.25s/it] 14%|█▍        | 6906/50000 [2:27:43<14:54:21,  1.25s/it] 14%|█▍        | 6907/50000 [2:27:45<14:53:46,  1.24s/it] 14%|█▍        | 6908/50000 [2:27:46<14:53:17,  1.24s/it] 14%|█▍        | 6909/50000 [2:27:47<14:53:07,  1.24s/it] 14%|█▍        | 6910/50000 [2:27:48<14:53:11,  1.24s/it]                                                         {'loss': 116.3953, 'learning_rate': 1.9594404444942996e-05, 'epoch': 1.07}
 14%|█▍        | 6910/50000 [2:27:48<14:53:11,  1.24s/it] 14%|█▍        | 6911/50000 [2:27:50<14:53:27,  1.24s/it] 14%|█▍        | 6912/50000 [2:27:51<14:53:21,  1.24s/it] 14%|█▍        | 6913/50000 [2:27:52<14:53:40,  1.24s/it] 14%|█▍        | 6914/50000 [2:27:53<14:53:08,  1.24s/it] 14%|█▍        | 6915/50000 [2:27:55<14:53:26,  1.24s/it] 14%|█▍        | 6916/50000 [2:27:56<14:52:26,  1.24s/it] 14%|█▍        | 6917/50000 [2:27:57<14:52:46,  1.24s/it] 14%|█▍        | 6918/50000 [2:27:58<14:52:39,  1.24s/it] 14%|█▍        | 6919/50000 [2:28:00<14:52:19,  1.24s/it] 14%|█▍        | 6920/50000 [2:28:01<14:51:53,  1.24s/it]                                                         {'loss': 112.7047, 'learning_rate': 1.9592537818701762e-05, 'epoch': 1.07}
 14%|█▍        | 6920/50000 [2:28:01<14:51:53,  1.24s/it] 14%|█▍        | 6921/50000 [2:28:02<14:51:57,  1.24s/it] 14%|█▍        | 6922/50000 [2:28:03<14:51:44,  1.24s/it] 14%|█▍        | 6923/50000 [2:28:05<14:51:53,  1.24s/it] 14%|█▍        | 6924/50000 [2:28:06<14:51:19,  1.24s/it] 14%|█▍        | 6925/50000 [2:28:07<14:51:23,  1.24s/it] 14%|█▍        | 6926/50000 [2:28:08<14:52:48,  1.24s/it] 14%|█▍        | 6927/50000 [2:28:10<14:54:10,  1.25s/it] 14%|█▍        | 6928/50000 [2:28:11<14:54:00,  1.25s/it] 14%|█▍        | 6929/50000 [2:28:12<14:53:16,  1.24s/it] 14%|█▍        | 6930/50000 [2:28:13<14:52:57,  1.24s/it]                                                         {'loss': 122.568, 'learning_rate': 1.959066699635859e-05, 'epoch': 1.07}
 14%|█▍        | 6930/50000 [2:28:13<14:52:57,  1.24s/it] 14%|█▍        | 6931/50000 [2:28:14<14:53:40,  1.25s/it] 14%|█▍        | 6932/50000 [2:28:16<14:52:53,  1.24s/it] 14%|█▍        | 6933/50000 [2:28:17<14:52:13,  1.24s/it] 14%|█▍        | 6934/50000 [2:28:18<14:51:43,  1.24s/it] 14%|█▍        | 6935/50000 [2:28:19<14:52:01,  1.24s/it] 14%|█▍        | 6936/50000 [2:28:21<14:51:26,  1.24s/it] 14%|█▍        | 6937/50000 [2:28:22<14:50:57,  1.24s/it] 14%|█▍        | 6938/50000 [2:28:23<14:50:49,  1.24s/it] 14%|█▍        | 6939/50000 [2:28:24<14:51:16,  1.24s/it] 14%|█▍        | 6940/50000 [2:28:26<14:52:43,  1.24s/it]                                                         {'loss': 107.9586, 'learning_rate': 1.958879197873184e-05, 'epoch': 1.08}
 14%|█▍        | 6940/50000 [2:28:26<14:52:43,  1.24s/it] 14%|█▍        | 6941/50000 [2:28:27<14:52:37,  1.24s/it] 14%|█▍        | 6942/50000 [2:28:28<14:52:31,  1.24s/it] 14%|█▍        | 6943/50000 [2:28:29<14:53:04,  1.24s/it] 14%|█▍        | 6944/50000 [2:28:31<14:53:09,  1.24s/it] 14%|█▍        | 6945/50000 [2:28:32<14:52:38,  1.24s/it] 14%|█▍        | 6946/50000 [2:28:33<14:52:27,  1.24s/it] 14%|█▍        | 6947/50000 [2:28:34<14:51:30,  1.24s/it] 14%|█▍        | 6948/50000 [2:28:36<14:51:33,  1.24s/it] 14%|█▍        | 6949/50000 [2:28:37<14:50:58,  1.24s/it] 14%|█▍        | 6950/50000 [2:28:38<14:51:20,  1.24s/it]                                                         {'loss': 112.6242, 'learning_rate': 1.9586912766641704e-05, 'epoch': 1.08}
 14%|█▍        | 6950/50000 [2:28:38<14:51:20,  1.24s/it] 14%|█▍        | 6951/50000 [2:28:39<14:50:57,  1.24s/it] 14%|█▍        | 6952/50000 [2:28:41<14:51:08,  1.24s/it] 14%|█▍        | 6953/50000 [2:28:42<14:50:27,  1.24s/it] 14%|█▍        | 6954/50000 [2:28:43<14:50:18,  1.24s/it] 14%|█▍        | 6955/50000 [2:28:44<14:50:18,  1.24s/it] 14%|█▍        | 6956/50000 [2:28:46<14:50:17,  1.24s/it] 14%|█▍        | 6957/50000 [2:28:47<14:50:23,  1.24s/it] 14%|█▍        | 6958/50000 [2:28:48<14:50:47,  1.24s/it] 14%|█▍        | 6959/50000 [2:28:49<14:50:20,  1.24s/it] 14%|█▍        | 6960/50000 [2:28:51<14:50:43,  1.24s/it]                                                         {'loss': 106.9313, 'learning_rate': 1.9585029360910218e-05, 'epoch': 1.08}
 14%|█▍        | 6960/50000 [2:28:51<14:50:43,  1.24s/it] 14%|█▍        | 6961/50000 [2:28:52<14:50:36,  1.24s/it] 14%|█▍        | 6962/50000 [2:28:53<14:50:09,  1.24s/it] 14%|█▍        | 6963/50000 [2:28:54<14:50:09,  1.24s/it] 14%|█▍        | 6964/50000 [2:28:55<14:50:06,  1.24s/it] 14%|█▍        | 6965/50000 [2:28:57<14:49:46,  1.24s/it] 14%|█▍        | 6966/50000 [2:28:58<14:49:40,  1.24s/it] 14%|█▍        | 6967/50000 [2:28:59<14:49:41,  1.24s/it] 14%|█▍        | 6968/50000 [2:29:00<14:49:42,  1.24s/it] 14%|█▍        | 6969/50000 [2:29:02<14:49:58,  1.24s/it] 14%|█▍        | 6970/50000 [2:29:03<14:49:44,  1.24s/it]                                                         {'loss': 112.4703, 'learning_rate': 1.9583141762361248e-05, 'epoch': 1.08}
 14%|█▍        | 6970/50000 [2:29:03<14:49:44,  1.24s/it] 14%|█▍        | 6971/50000 [2:29:04<14:49:43,  1.24s/it] 14%|█▍        | 6972/50000 [2:29:05<14:49:33,  1.24s/it] 14%|█▍        | 6973/50000 [2:29:07<14:49:23,  1.24s/it] 14%|█▍        | 6974/50000 [2:29:08<14:49:23,  1.24s/it] 14%|█▍        | 6975/50000 [2:29:09<14:49:14,  1.24s/it] 14%|█▍        | 6976/50000 [2:29:10<14:49:44,  1.24s/it] 14%|█▍        | 6977/50000 [2:29:12<14:49:58,  1.24s/it] 14%|█▍        | 6978/50000 [2:29:13<14:49:36,  1.24s/it] 14%|█▍        | 6979/50000 [2:29:14<14:49:48,  1.24s/it] 14%|█▍        | 6980/50000 [2:29:15<14:49:42,  1.24s/it]                                                         {'loss': 120.0953, 'learning_rate': 1.958124997182049e-05, 'epoch': 1.08}
 14%|█▍        | 6980/50000 [2:29:15<14:49:42,  1.24s/it][2023-07-03 14:29:26,834] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, but hysteresis is 2. Reducing hysteresis to 1
 14%|█▍        | 6981/50000 [2:29:16<13:56:19,  1.17s/it] 14%|█▍        | 6982/50000 [2:29:18<14:12:26,  1.19s/it] 14%|█▍        | 6983/50000 [2:29:19<14:23:40,  1.20s/it] 14%|█▍        | 6984/50000 [2:29:20<14:31:37,  1.22s/it] 14%|█▍        | 6985/50000 [2:29:21<14:36:32,  1.22s/it] 14%|█▍        | 6986/50000 [2:29:23<14:40:13,  1.23s/it] 14%|█▍        | 6987/50000 [2:29:24<14:42:56,  1.23s/it] 14%|█▍        | 6988/50000 [2:29:25<14:44:49,  1.23s/it] 14%|█▍        | 6989/50000 [2:29:26<14:46:16,  1.24s/it] 14%|█▍        | 6990/50000 [2:29:27<14:47:19,  1.24s/it]                                                         {'loss': 124.1711, 'learning_rate': 1.957954377686475e-05, 'epoch': 1.08}
 14%|█▍        | 6990/50000 [2:29:27<14:47:19,  1.24s/it] 14%|█▍        | 6991/50000 [2:29:29<14:47:53,  1.24s/it][2023-07-03 14:29:40,232] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
 14%|█▍        | 6992/50000 [2:29:30<13:54:34,  1.16s/it] 14%|█▍        | 6993/50000 [2:29:31<14:10:36,  1.19s/it] 14%|█▍        | 6994/50000 [2:29:32<14:22:19,  1.20s/it] 14%|█▍        | 6995/50000 [2:29:33<14:30:16,  1.21s/it] 14%|█▍        | 6996/50000 [2:29:35<14:35:54,  1.22s/it] 14%|█▍        | 6997/50000 [2:29:36<14:39:44,  1.23s/it] 14%|█▍        | 6998/50000 [2:29:37<14:43:29,  1.23s/it] 14%|█▍        | 6999/50000 [2:29:38<14:45:37,  1.24s/it] 14%|█▍        | 7000/50000 [2:29:40<14:46:49,  1.24s/it]                                                         {'loss': 109.9461, 'learning_rate': 1.9577834187670484e-05, 'epoch': 1.09}
 14%|█▍        | 7000/50000 [2:29:40<14:46:49,  1.24s/it][INFO|trainer.py:3129] 2023-07-03 14:29:50,168 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 14:29:50,169 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 14:29:50,169 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                         
                                             [A{'eval_loss': 151.5, 'eval_accuracy': 0.3971850067740479, 'eval_runtime': 3.1015, 'eval_samples_per_second': 8.383, 'eval_steps_per_second': 2.257, 'epoch': 1.09}
 14%|█▍        | 7000/50000 [2:29:43<14:46:49,  1.24s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 14:29:53,271 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000
[INFO|trainer.py:2880] 2023-07-03 14:29:53,285 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 14:30:03,471 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 14:30:03,471 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/special_tokens_map.json
[2023-07-03 14:30:03,474] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 14:30:03,497] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/global_step7000/mp_rank_00_model_states.pt
[2023-07-03 14:30:03,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/global_step7000/mp_rank_00_model_states.pt...
[2023-07-03 14:30:13,854] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/global_step7000/mp_rank_00_model_states.pt.
[2023-07-03 14:30:13,857] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/global_step7000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 14:30:31,903] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/global_step7000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 14:30:31,903] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-7000/global_step7000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 14:30:31,903] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 14:30:31,957 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-6000] due to args.save_total_limit
 14%|█▍        | 7001/50000 [2:30:32<198:01:20, 16.58s/it] 14%|█▍        | 7002/50000 [2:30:33<143:25:51, 12.01s/it] 14%|█▍        | 7003/50000 [2:30:35<104:51:30,  8.78s/it] 14%|█▍        | 7004/50000 [2:30:36<77:51:03,  6.52s/it]  14%|█▍        | 7005/50000 [2:30:37<59:12:38,  4.96s/it] 14%|█▍        | 7006/50000 [2:30:38<45:52:25,  3.84s/it] 14%|█▍        | 7007/50000 [2:30:40<36:35:12,  3.06s/it] 14%|█▍        | 7008/50000 [2:30:41<30:06:10,  2.52s/it] 14%|█▍        | 7009/50000 [2:30:42<25:32:12,  2.14s/it] 14%|█▍        | 7010/50000 [2:30:43<22:19:37,  1.87s/it]                                                         {'loss': 113.9953, 'learning_rate': 1.9575930663961055e-05, 'epoch': 1.09}
 14%|█▍        | 7010/50000 [2:30:43<22:19:37,  1.87s/it] 14%|█▍        | 7011/50000 [2:30:45<20:05:02,  1.68s/it] 14%|█▍        | 7012/50000 [2:30:46<18:29:19,  1.55s/it] 14%|█▍        | 7013/50000 [2:30:47<17:22:35,  1.46s/it] 14%|█▍        | 7014/50000 [2:30:48<16:35:36,  1.39s/it] 14%|█▍        | 7015/50000 [2:30:50<16:02:45,  1.34s/it] 14%|█▍        | 7016/50000 [2:30:51<15:40:25,  1.31s/it] 14%|█▍        | 7017/50000 [2:30:52<15:24:53,  1.29s/it] 14%|█▍        | 7018/50000 [2:30:53<15:13:23,  1.28s/it] 14%|█▍        | 7019/50000 [2:30:55<15:05:22,  1.26s/it] 14%|█▍        | 7020/50000 [2:30:56<15:00:23,  1.26s/it]                                                         {'loss': 115.3695, 'learning_rate': 1.9574022951414223e-05, 'epoch': 1.09}
 14%|█▍        | 7020/50000 [2:30:56<15:00:23,  1.26s/it] 14%|█▍        | 7021/50000 [2:30:57<14:56:33,  1.25s/it] 14%|█▍        | 7022/50000 [2:30:58<14:53:18,  1.25s/it] 14%|█▍        | 7023/50000 [2:31:00<14:51:06,  1.24s/it] 14%|█▍        | 7024/50000 [2:31:01<14:50:22,  1.24s/it] 14%|█▍        | 7025/50000 [2:31:02<14:49:03,  1.24s/it] 14%|█▍        | 7026/50000 [2:31:03<14:48:24,  1.24s/it] 14%|█▍        | 7027/50000 [2:31:04<14:47:46,  1.24s/it] 14%|█▍        | 7028/50000 [2:31:06<14:47:29,  1.24s/it] 14%|█▍        | 7029/50000 [2:31:07<14:47:39,  1.24s/it] 14%|█▍        | 7030/50000 [2:31:08<14:46:55,  1.24s/it]                                                         {'loss': 115.9266, 'learning_rate': 1.9572111050864483e-05, 'epoch': 1.09}
 14%|█▍        | 7030/50000 [2:31:08<14:46:55,  1.24s/it] 14%|█▍        | 7031/50000 [2:31:09<14:46:52,  1.24s/it] 14%|█▍        | 7032/50000 [2:31:11<14:46:48,  1.24s/it] 14%|█▍        | 7033/50000 [2:31:12<14:46:30,  1.24s/it] 14%|█▍        | 7034/50000 [2:31:13<14:47:04,  1.24s/it] 14%|█▍        | 7035/50000 [2:31:14<14:46:54,  1.24s/it] 14%|█▍        | 7036/50000 [2:31:16<14:47:03,  1.24s/it] 14%|█▍        | 7037/50000 [2:31:17<14:46:43,  1.24s/it] 14%|█▍        | 7038/50000 [2:31:18<14:46:43,  1.24s/it] 14%|█▍        | 7039/50000 [2:31:19<14:46:36,  1.24s/it] 14%|█▍        | 7040/50000 [2:31:21<14:47:33,  1.24s/it]                                                         {'loss': 103.5156, 'learning_rate': 1.9570194963148162e-05, 'epoch': 1.09}
 14%|█▍        | 7040/50000 [2:31:21<14:47:33,  1.24s/it] 14%|█▍        | 7041/50000 [2:31:22<14:48:05,  1.24s/it] 14%|█▍        | 7042/50000 [2:31:23<14:48:32,  1.24s/it] 14%|█▍        | 7043/50000 [2:31:24<15:16:36,  1.28s/it] 14%|█▍        | 7044/50000 [2:31:26<15:09:12,  1.27s/it] 14%|█▍        | 7045/50000 [2:31:27<15:03:12,  1.26s/it] 14%|█▍        | 7046/50000 [2:31:28<14:59:36,  1.26s/it] 14%|█▍        | 7047/50000 [2:31:29<14:56:52,  1.25s/it] 14%|█▍        | 7048/50000 [2:31:31<14:54:15,  1.25s/it] 14%|█▍        | 7049/50000 [2:31:32<14:53:50,  1.25s/it] 14%|█▍        | 7050/50000 [2:31:33<14:52:07,  1.25s/it]                                                         {'loss': 116.5859, 'learning_rate': 1.9568274689103426e-05, 'epoch': 1.09}
 14%|█▍        | 7050/50000 [2:31:33<14:52:07,  1.25s/it] 14%|█▍        | 7051/50000 [2:31:34<14:51:32,  1.25s/it] 14%|█▍        | 7052/50000 [2:31:36<14:51:06,  1.24s/it] 14%|█▍        | 7053/50000 [2:31:37<14:50:22,  1.24s/it] 14%|█▍        | 7054/50000 [2:31:38<14:49:57,  1.24s/it] 14%|█▍        | 7055/50000 [2:31:39<14:49:31,  1.24s/it] 14%|█▍        | 7056/50000 [2:31:41<14:50:02,  1.24s/it] 14%|█▍        | 7057/50000 [2:31:42<14:49:35,  1.24s/it] 14%|█▍        | 7058/50000 [2:31:43<14:49:39,  1.24s/it] 14%|█▍        | 7059/50000 [2:31:44<14:49:10,  1.24s/it] 14%|█▍        | 7060/50000 [2:31:46<14:49:06,  1.24s/it]                                                         {'loss': 124.3898, 'learning_rate': 1.956635022957027e-05, 'epoch': 1.09}
 14%|█▍        | 7060/50000 [2:31:46<14:49:06,  1.24s/it] 14%|█▍        | 7061/50000 [2:31:47<14:49:07,  1.24s/it] 14%|█▍        | 7062/50000 [2:31:48<14:48:51,  1.24s/it] 14%|█▍        | 7063/50000 [2:31:49<14:48:58,  1.24s/it] 14%|█▍        | 7064/50000 [2:31:51<14:49:17,  1.24s/it] 14%|█▍        | 7065/50000 [2:31:52<14:49:00,  1.24s/it] 14%|█▍        | 7066/50000 [2:31:53<14:48:58,  1.24s/it] 14%|█▍        | 7067/50000 [2:31:54<14:48:45,  1.24s/it] 14%|█▍        | 7068/50000 [2:31:56<14:54:01,  1.25s/it] 14%|█▍        | 7069/50000 [2:31:57<14:53:17,  1.25s/it] 14%|█▍        | 7070/50000 [2:31:58<14:53:23,  1.25s/it]                                                         {'loss': 110.5828, 'learning_rate': 1.956442158539051e-05, 'epoch': 1.1}
 14%|█▍        | 7070/50000 [2:31:58<14:53:23,  1.25s/it] 14%|█▍        | 7071/50000 [2:31:59<14:53:31,  1.25s/it] 14%|█▍        | 7072/50000 [2:32:00<14:52:45,  1.25s/it] 14%|█▍        | 7073/50000 [2:32:02<14:52:56,  1.25s/it] 14%|█▍        | 7074/50000 [2:32:03<14:52:13,  1.25s/it] 14%|█▍        | 7075/50000 [2:32:04<14:51:20,  1.25s/it] 14%|█▍        | 7076/50000 [2:32:05<14:50:51,  1.25s/it] 14%|█▍        | 7077/50000 [2:32:07<14:50:04,  1.24s/it] 14%|█▍        | 7078/50000 [2:32:08<14:49:44,  1.24s/it] 14%|█▍        | 7079/50000 [2:32:09<14:49:00,  1.24s/it] 14%|█▍        | 7080/50000 [2:32:10<14:48:51,  1.24s/it]                                                         {'loss': 117.693, 'learning_rate': 1.956248875740781e-05, 'epoch': 1.1}
 14%|█▍        | 7080/50000 [2:32:10<14:48:51,  1.24s/it] 14%|█▍        | 7081/50000 [2:32:12<14:49:35,  1.24s/it] 14%|█▍        | 7082/50000 [2:32:13<14:49:20,  1.24s/it] 14%|█▍        | 7083/50000 [2:32:14<14:48:59,  1.24s/it] 14%|█▍        | 7084/50000 [2:32:15<14:48:36,  1.24s/it] 14%|█▍        | 7085/50000 [2:32:17<14:48:29,  1.24s/it] 14%|█▍        | 7086/50000 [2:32:18<14:48:47,  1.24s/it] 14%|█▍        | 7087/50000 [2:32:19<14:48:18,  1.24s/it] 14%|█▍        | 7088/50000 [2:32:20<14:47:58,  1.24s/it] 14%|█▍        | 7089/50000 [2:32:22<14:47:58,  1.24s/it] 14%|█▍        | 7090/50000 [2:32:23<14:47:41,  1.24s/it]                                                         {'loss': 105.5633, 'learning_rate': 1.956055174646765e-05, 'epoch': 1.1}
 14%|█▍        | 7090/50000 [2:32:23<14:47:41,  1.24s/it] 14%|█▍        | 7091/50000 [2:32:24<14:48:22,  1.24s/it] 14%|█▍        | 7092/50000 [2:32:25<14:47:56,  1.24s/it] 14%|█▍        | 7093/50000 [2:32:27<14:47:44,  1.24s/it] 14%|█▍        | 7094/50000 [2:32:28<14:47:26,  1.24s/it] 14%|█▍        | 7095/50000 [2:32:29<14:47:28,  1.24s/it] 14%|█▍        | 7096/50000 [2:32:30<14:47:16,  1.24s/it] 14%|█▍        | 7097/50000 [2:32:32<14:47:04,  1.24s/it] 14%|█▍        | 7098/50000 [2:32:33<14:47:47,  1.24s/it] 14%|█▍        | 7099/50000 [2:32:34<14:47:37,  1.24s/it] 14%|█▍        | 7100/50000 [2:32:35<14:47:28,  1.24s/it]                                                         {'loss': 109.7406, 'learning_rate': 1.955861055341734e-05, 'epoch': 1.1}
 14%|█▍        | 7100/50000 [2:32:35<14:47:28,  1.24s/it] 14%|█▍        | 7101/50000 [2:32:37<14:47:30,  1.24s/it] 14%|█▍        | 7102/50000 [2:32:38<14:47:27,  1.24s/it] 14%|█▍        | 7103/50000 [2:32:39<14:47:52,  1.24s/it] 14%|█▍        | 7104/50000 [2:32:40<14:47:32,  1.24s/it] 14%|█▍        | 7105/50000 [2:32:41<14:47:22,  1.24s/it] 14%|█▍        | 7106/50000 [2:32:43<14:47:09,  1.24s/it] 14%|█▍        | 7107/50000 [2:32:44<14:47:26,  1.24s/it] 14%|█▍        | 7108/50000 [2:32:45<14:47:20,  1.24s/it] 14%|█▍        | 7109/50000 [2:32:46<14:47:08,  1.24s/it] 14%|█▍        | 7110/50000 [2:32:48<14:47:09,  1.24s/it]                                                         {'loss': 123.1891, 'learning_rate': 1.9556665179106034e-05, 'epoch': 1.1}
 14%|█▍        | 7110/50000 [2:32:48<14:47:09,  1.24s/it] 14%|█▍        | 7111/50000 [2:32:49<14:47:22,  1.24s/it] 14%|█▍        | 7112/50000 [2:32:50<14:47:15,  1.24s/it] 14%|█▍        | 7113/50000 [2:32:51<14:46:58,  1.24s/it] 14%|█▍        | 7114/50000 [2:32:53<14:46:59,  1.24s/it] 14%|█▍        | 7115/50000 [2:32:54<14:46:57,  1.24s/it] 14%|█▍        | 7116/50000 [2:32:55<14:46:51,  1.24s/it] 14%|█▍        | 7117/50000 [2:32:56<14:46:51,  1.24s/it] 14%|█▍        | 7118/50000 [2:32:58<14:46:40,  1.24s/it] 14%|█▍        | 7119/50000 [2:32:59<14:46:40,  1.24s/it] 14%|█▍        | 7120/50000 [2:33:00<14:46:24,  1.24s/it]                                                         {'loss': 129.3547, 'learning_rate': 1.9554715624384698e-05, 'epoch': 1.1}
 14%|█▍        | 7120/50000 [2:33:00<14:46:24,  1.24s/it] 14%|█▍        | 7121/50000 [2:33:01<14:46:43,  1.24s/it] 14%|█▍        | 7122/50000 [2:33:03<14:46:36,  1.24s/it] 14%|█▍        | 7123/50000 [2:33:04<14:46:47,  1.24s/it] 14%|█▍        | 7124/50000 [2:33:05<14:46:56,  1.24s/it] 14%|█▍        | 7125/50000 [2:33:06<14:47:19,  1.24s/it] 14%|█▍        | 7126/50000 [2:33:08<14:47:03,  1.24s/it] 14%|█▍        | 7127/50000 [2:33:09<14:47:00,  1.24s/it] 14%|█▍        | 7128/50000 [2:33:10<14:46:57,  1.24s/it] 14%|█▍        | 7129/50000 [2:33:11<14:47:29,  1.24s/it] 14%|█▍        | 7130/50000 [2:33:13<14:47:18,  1.24s/it]                                                         {'loss': 115.7742, 'learning_rate': 1.9552761890106135e-05, 'epoch': 1.11}
 14%|█▍        | 7130/50000 [2:33:13<14:47:18,  1.24s/it] 14%|█▍        | 7131/50000 [2:33:14<14:47:32,  1.24s/it] 14%|█▍        | 7132/50000 [2:33:15<15:05:49,  1.27s/it] 14%|█▍        | 7133/50000 [2:33:16<15:00:49,  1.26s/it] 14%|█▍        | 7134/50000 [2:33:18<14:57:11,  1.26s/it] 14%|█▍        | 7135/50000 [2:33:19<14:54:18,  1.25s/it][2023-07-03 14:33:30,332] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, but hysteresis is 2. Reducing hysteresis to 1
 14%|█▍        | 7136/50000 [2:33:20<14:00:29,  1.18s/it] 14%|█▍        | 7137/50000 [2:33:21<14:15:09,  1.20s/it] 14%|█▍        | 7138/50000 [2:33:22<14:25:22,  1.21s/it] 14%|█▍        | 7139/50000 [2:33:24<14:31:53,  1.22s/it] 14%|█▍        | 7140/50000 [2:33:25<14:37:16,  1.23s/it]                                                         {'loss': 114.8391, 'learning_rate': 1.955099995644031e-05, 'epoch': 1.11}
 14%|█▍        | 7140/50000 [2:33:25<14:37:16,  1.23s/it] 14%|█▍        | 7141/50000 [2:33:26<14:40:44,  1.23s/it] 14%|█▍        | 7142/50000 [2:33:27<14:42:25,  1.24s/it] 14%|█▍        | 7143/50000 [2:33:29<14:43:30,  1.24s/it] 14%|█▍        | 7144/50000 [2:33:30<14:45:21,  1.24s/it] 14%|█▍        | 7145/50000 [2:33:31<14:45:34,  1.24s/it] 14%|█▍        | 7146/50000 [2:33:32<14:45:43,  1.24s/it] 14%|█▍        | 7147/50000 [2:33:33<14:46:20,  1.24s/it] 14%|█▍        | 7148/50000 [2:33:35<14:46:49,  1.24s/it] 14%|█▍        | 7149/50000 [2:33:36<14:48:22,  1.24s/it] 14%|█▍        | 7150/50000 [2:33:37<14:50:03,  1.25s/it]                                                         {'loss': 112.007, 'learning_rate': 1.954903828335903e-05, 'epoch': 1.11}
 14%|█▍        | 7150/50000 [2:33:37<14:50:03,  1.25s/it] 14%|█▍        | 7151/50000 [2:33:38<14:51:41,  1.25s/it] 14%|█▍        | 7152/50000 [2:33:40<14:51:20,  1.25s/it] 14%|█▍        | 7153/50000 [2:33:41<14:50:20,  1.25s/it] 14%|█▍        | 7154/50000 [2:33:42<14:50:46,  1.25s/it] 14%|█▍        | 7155/50000 [2:33:43<14:50:07,  1.25s/it] 14%|█▍        | 7156/50000 [2:33:45<14:49:47,  1.25s/it] 14%|█▍        | 7157/50000 [2:33:46<14:57:23,  1.26s/it] 14%|█▍        | 7158/50000 [2:33:47<14:54:07,  1.25s/it] 14%|█▍        | 7159/50000 [2:33:48<14:51:55,  1.25s/it] 14%|█▍        | 7160/50000 [2:33:50<14:50:28,  1.25s/it]                                                         {'loss': 115.7539, 'learning_rate': 1.9547072433203987e-05, 'epoch': 1.11}
 14%|█▍        | 7160/50000 [2:33:50<14:50:28,  1.25s/it] 14%|█▍        | 7161/50000 [2:33:51<14:50:28,  1.25s/it] 14%|█▍        | 7162/50000 [2:33:52<14:48:55,  1.25s/it] 14%|█▍        | 7163/50000 [2:33:53<14:48:48,  1.24s/it] 14%|█▍        | 7164/50000 [2:33:55<14:49:21,  1.25s/it] 14%|█▍        | 7165/50000 [2:33:56<14:50:07,  1.25s/it] 14%|█▍        | 7166/50000 [2:33:57<14:49:32,  1.25s/it] 14%|█▍        | 7167/50000 [2:33:58<14:49:43,  1.25s/it][2023-07-03 14:34:09,957] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
 14%|█▍        | 7168/50000 [2:33:59<13:56:14,  1.17s/it] 14%|█▍        | 7169/50000 [2:34:01<14:11:15,  1.19s/it] 14%|█▍        | 7170/50000 [2:34:02<14:22:32,  1.21s/it]                                                         {'loss': 104.3031, 'learning_rate': 1.954529959737707e-05, 'epoch': 1.11}
 14%|█▍        | 7170/50000 [2:34:02<14:22:32,  1.21s/it] 14%|█▍        | 7171/50000 [2:34:03<14:30:55,  1.22s/it] 14%|█▍        | 7172/50000 [2:34:04<14:36:39,  1.23s/it] 14%|█▍        | 7173/50000 [2:34:06<14:39:51,  1.23s/it] 14%|█▍        | 7174/50000 [2:34:07<14:42:02,  1.24s/it] 14%|█▍        | 7175/50000 [2:34:08<14:43:09,  1.24s/it] 14%|█▍        | 7176/50000 [2:34:09<14:44:45,  1.24s/it] 14%|█▍        | 7177/50000 [2:34:11<14:45:29,  1.24s/it] 14%|█▍        | 7178/50000 [2:34:12<14:45:41,  1.24s/it] 14%|█▍        | 7179/50000 [2:34:13<14:45:52,  1.24s/it] 14%|█▍        | 7180/50000 [2:34:14<14:46:18,  1.24s/it]                                                         {'loss': 118.0438, 'learning_rate': 1.9543325813152485e-05, 'epoch': 1.11}
 14%|█▍        | 7180/50000 [2:34:14<14:46:18,  1.24s/it] 14%|█▍        | 7181/50000 [2:34:16<14:46:52,  1.24s/it] 14%|█▍        | 7182/50000 [2:34:17<14:46:32,  1.24s/it] 14%|█▍        | 7183/50000 [2:34:18<14:46:08,  1.24s/it] 14%|█▍        | 7184/50000 [2:34:19<14:46:09,  1.24s/it] 14%|█▍        | 7185/50000 [2:34:21<14:45:51,  1.24s/it] 14%|█▍        | 7186/50000 [2:34:22<14:46:41,  1.24s/it] 14%|█▍        | 7187/50000 [2:34:23<14:46:30,  1.24s/it] 14%|█▍        | 7188/50000 [2:34:24<14:46:35,  1.24s/it] 14%|█▍        | 7189/50000 [2:34:26<15:07:33,  1.27s/it] 14%|█▍        | 7190/50000 [2:34:27<15:00:48,  1.26s/it]                                                         {'loss': 127.0625, 'learning_rate': 1.9541347854352963e-05, 'epoch': 1.11}
 14%|█▍        | 7190/50000 [2:34:27<15:00:48,  1.26s/it] 14%|█▍        | 7191/50000 [2:34:28<14:56:43,  1.26s/it] 14%|█▍        | 7192/50000 [2:34:29<14:53:47,  1.25s/it] 14%|█▍        | 7193/50000 [2:34:31<14:51:29,  1.25s/it] 14%|█▍        | 7194/50000 [2:34:32<14:49:36,  1.25s/it] 14%|█▍        | 7195/50000 [2:34:33<14:48:07,  1.24s/it] 14%|█▍        | 7196/50000 [2:34:34<14:47:24,  1.24s/it] 14%|█▍        | 7197/50000 [2:34:36<14:48:36,  1.25s/it] 14%|█▍        | 7198/50000 [2:34:37<14:49:54,  1.25s/it] 14%|█▍        | 7199/50000 [2:34:38<14:49:19,  1.25s/it] 14%|█▍        | 7200/50000 [2:34:39<14:48:33,  1.25s/it]                                                         {'loss': 115.9859, 'learning_rate': 1.9539365721843736e-05, 'epoch': 1.12}
 14%|█▍        | 7200/50000 [2:34:39<14:48:33,  1.25s/it] 14%|█▍        | 7201/50000 [2:34:41<14:48:34,  1.25s/it] 14%|█▍        | 7202/50000 [2:34:42<14:48:05,  1.25s/it] 14%|█▍        | 7203/50000 [2:34:43<14:47:34,  1.24s/it] 14%|█▍        | 7204/50000 [2:34:44<14:46:46,  1.24s/it] 14%|█▍        | 7205/50000 [2:34:46<14:47:16,  1.24s/it] 14%|█▍        | 7206/50000 [2:34:47<14:47:14,  1.24s/it] 14%|█▍        | 7207/50000 [2:34:48<14:46:27,  1.24s/it] 14%|█▍        | 7208/50000 [2:34:49<14:46:21,  1.24s/it][2023-07-03 14:35:00,782] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 14%|█▍        | 7209/50000 [2:34:50<13:53:25,  1.17s/it] 14%|█▍        | 7210/50000 [2:34:52<14:08:49,  1.19s/it]                                                         {'loss': 125.8141, 'learning_rate': 1.9537578234780203e-05, 'epoch': 1.12}
 14%|█▍        | 7210/50000 [2:34:52<14:08:49,  1.19s/it] 14%|█▍        | 7211/50000 [2:34:53<14:19:59,  1.21s/it] 14%|█▍        | 7212/50000 [2:34:54<14:28:18,  1.22s/it] 14%|█▍        | 7213/50000 [2:34:55<14:33:23,  1.22s/it] 14%|█▍        | 7214/50000 [2:34:56<14:37:20,  1.23s/it] 14%|█▍        | 7215/50000 [2:34:58<14:40:50,  1.24s/it] 14%|█▍        | 7216/50000 [2:34:59<14:42:22,  1.24s/it] 14%|█▍        | 7217/50000 [2:35:00<14:43:34,  1.24s/it] 14%|█▍        | 7218/50000 [2:35:01<14:43:45,  1.24s/it] 14%|█▍        | 7219/50000 [2:35:03<14:43:48,  1.24s/it] 14%|█▍        | 7220/50000 [2:35:04<14:44:49,  1.24s/it]                                                         {'loss': 113.8781, 'learning_rate': 1.953558817461276e-05, 'epoch': 1.12}
 14%|█▍        | 7220/50000 [2:35:04<14:44:49,  1.24s/it] 14%|█▍        | 7221/50000 [2:35:05<14:44:52,  1.24s/it] 14%|█▍        | 7222/50000 [2:35:06<14:45:10,  1.24s/it] 14%|█▍        | 7223/50000 [2:35:08<14:45:08,  1.24s/it] 14%|█▍        | 7224/50000 [2:35:09<14:45:09,  1.24s/it] 14%|█▍        | 7225/50000 [2:35:10<14:44:53,  1.24s/it] 14%|█▍        | 7226/50000 [2:35:11<14:45:49,  1.24s/it] 14%|█▍        | 7227/50000 [2:35:13<14:45:42,  1.24s/it] 14%|█▍        | 7228/50000 [2:35:14<14:52:24,  1.25s/it] 14%|█▍        | 7229/50000 [2:35:15<14:50:41,  1.25s/it] 14%|█▍        | 7230/50000 [2:35:16<14:50:20,  1.25s/it]                                                         {'loss': 116.4141, 'learning_rate': 1.9533593943255087e-05, 'epoch': 1.12}
 14%|█▍        | 7230/50000 [2:35:16<14:50:20,  1.25s/it] 14%|█▍        | 7231/50000 [2:35:18<14:50:14,  1.25s/it] 14%|█▍        | 7232/50000 [2:35:19<14:50:48,  1.25s/it] 14%|█▍        | 7233/50000 [2:35:20<14:49:12,  1.25s/it] 14%|█▍        | 7234/50000 [2:35:21<14:47:45,  1.25s/it] 14%|█▍        | 7235/50000 [2:35:23<14:46:54,  1.24s/it] 14%|█▍        | 7236/50000 [2:35:24<14:47:13,  1.24s/it] 14%|█▍        | 7237/50000 [2:35:25<14:49:12,  1.25s/it] 14%|█▍        | 7238/50000 [2:35:26<14:47:38,  1.25s/it] 14%|█▍        | 7239/50000 [2:35:28<14:46:56,  1.24s/it] 14%|█▍        | 7240/50000 [2:35:29<14:46:00,  1.24s/it]                                                         {'loss': 110.6813, 'learning_rate': 1.9531595541579525e-05, 'epoch': 1.12}
 14%|█▍        | 7240/50000 [2:35:29<14:46:00,  1.24s/it] 14%|█▍        | 7241/50000 [2:35:30<14:45:33,  1.24s/it] 14%|█▍        | 7242/50000 [2:35:31<14:44:44,  1.24s/it] 14%|█▍        | 7243/50000 [2:35:33<14:44:13,  1.24s/it] 14%|█▍        | 7244/50000 [2:35:34<14:44:39,  1.24s/it] 14%|█▍        | 7245/50000 [2:35:35<14:45:47,  1.24s/it] 14%|█▍        | 7246/50000 [2:35:36<14:45:54,  1.24s/it] 14%|█▍        | 7247/50000 [2:35:38<14:45:28,  1.24s/it] 14%|█▍        | 7248/50000 [2:35:39<14:44:56,  1.24s/it] 14%|█▍        | 7249/50000 [2:35:40<14:44:23,  1.24s/it] 14%|█▍        | 7250/50000 [2:35:41<14:44:06,  1.24s/it]                                                         {'loss': 146.4328, 'learning_rate': 1.952959297046025e-05, 'epoch': 1.12}
 14%|█▍        | 7250/50000 [2:35:41<14:44:06,  1.24s/it] 15%|█▍        | 7251/50000 [2:35:43<14:44:01,  1.24s/it] 15%|█▍        | 7252/50000 [2:35:44<14:43:53,  1.24s/it] 15%|█▍        | 7253/50000 [2:35:45<14:49:33,  1.25s/it] 15%|█▍        | 7254/50000 [2:35:46<14:48:29,  1.25s/it] 15%|█▍        | 7255/50000 [2:35:47<14:47:21,  1.25s/it] 15%|█▍        | 7256/50000 [2:35:49<14:46:02,  1.24s/it] 15%|█▍        | 7257/50000 [2:35:50<14:45:11,  1.24s/it] 15%|█▍        | 7258/50000 [2:35:51<14:44:30,  1.24s/it] 15%|█▍        | 7259/50000 [2:35:52<14:44:03,  1.24s/it] 15%|█▍        | 7260/50000 [2:35:54<14:44:16,  1.24s/it]                                                         {'loss': 109.0742, 'learning_rate': 1.9527586230773243e-05, 'epoch': 1.13}
 15%|█▍        | 7260/50000 [2:35:54<14:44:16,  1.24s/it] 15%|█▍        | 7261/50000 [2:35:55<14:44:43,  1.24s/it] 15%|█▍        | 7262/50000 [2:35:56<14:44:30,  1.24s/it] 15%|█▍        | 7263/50000 [2:35:57<14:44:06,  1.24s/it] 15%|█▍        | 7264/50000 [2:35:59<14:43:50,  1.24s/it] 15%|█▍        | 7265/50000 [2:36:00<14:43:28,  1.24s/it] 15%|█▍        | 7266/50000 [2:36:01<14:43:33,  1.24s/it] 15%|█▍        | 7267/50000 [2:36:02<14:44:14,  1.24s/it] 15%|█▍        | 7268/50000 [2:36:04<14:44:17,  1.24s/it] 15%|█▍        | 7269/50000 [2:36:05<14:44:19,  1.24s/it] 15%|█▍        | 7270/50000 [2:36:06<14:43:52,  1.24s/it]                                                         {'loss': 115.5625, 'learning_rate': 1.9525575323396332e-05, 'epoch': 1.13}
 15%|█▍        | 7270/50000 [2:36:06<14:43:52,  1.24s/it] 15%|█▍        | 7271/50000 [2:36:07<14:44:13,  1.24s/it] 15%|█▍        | 7272/50000 [2:36:09<14:44:17,  1.24s/it] 15%|█▍        | 7273/50000 [2:36:10<14:44:16,  1.24s/it] 15%|█▍        | 7274/50000 [2:36:11<14:44:02,  1.24s/it] 15%|█▍        | 7275/50000 [2:36:12<14:43:49,  1.24s/it] 15%|█▍        | 7276/50000 [2:36:14<14:43:25,  1.24s/it] 15%|█▍        | 7277/50000 [2:36:15<14:43:23,  1.24s/it] 15%|█▍        | 7278/50000 [2:36:16<14:43:05,  1.24s/it] 15%|█▍        | 7279/50000 [2:36:17<14:43:04,  1.24s/it] 15%|█▍        | 7280/50000 [2:36:19<14:42:59,  1.24s/it]                                                         {'loss': 121.0555, 'learning_rate': 1.952356024920915e-05, 'epoch': 1.13}
 15%|█▍        | 7280/50000 [2:36:19<14:42:59,  1.24s/it] 15%|█▍        | 7281/50000 [2:36:20<14:43:16,  1.24s/it] 15%|█▍        | 7282/50000 [2:36:21<14:43:46,  1.24s/it] 15%|█▍        | 7283/50000 [2:36:22<14:44:07,  1.24s/it] 15%|█▍        | 7284/50000 [2:36:23<14:43:32,  1.24s/it] 15%|█▍        | 7285/50000 [2:36:25<14:43:46,  1.24s/it] 15%|█▍        | 7286/50000 [2:36:26<14:44:12,  1.24s/it] 15%|█▍        | 7287/50000 [2:36:27<14:43:45,  1.24s/it] 15%|█▍        | 7288/50000 [2:36:28<14:43:13,  1.24s/it] 15%|█▍        | 7289/50000 [2:36:30<14:42:59,  1.24s/it] 15%|█▍        | 7290/50000 [2:36:31<14:42:53,  1.24s/it]                                                         {'loss': 119.8406, 'learning_rate': 1.9521541009093158e-05, 'epoch': 1.13}
 15%|█▍        | 7290/50000 [2:36:31<14:42:53,  1.24s/it] 15%|█▍        | 7291/50000 [2:36:32<14:43:27,  1.24s/it] 15%|█▍        | 7292/50000 [2:36:33<14:43:01,  1.24s/it] 15%|█▍        | 7293/50000 [2:36:35<14:43:25,  1.24s/it] 15%|█▍        | 7294/50000 [2:36:36<14:43:05,  1.24s/it] 15%|█▍        | 7295/50000 [2:36:37<14:42:56,  1.24s/it] 15%|█▍        | 7296/50000 [2:36:38<14:42:54,  1.24s/it] 15%|█▍        | 7297/50000 [2:36:40<14:42:36,  1.24s/it] 15%|█▍        | 7298/50000 [2:36:41<14:42:44,  1.24s/it] 15%|█▍        | 7299/50000 [2:36:42<14:42:46,  1.24s/it] 15%|█▍        | 7300/50000 [2:36:43<14:42:48,  1.24s/it]                                                         {'loss': 115.6641, 'learning_rate': 1.9519517603931647e-05, 'epoch': 1.13}
 15%|█▍        | 7300/50000 [2:36:43<14:42:48,  1.24s/it] 15%|█▍        | 7301/50000 [2:36:45<14:43:02,  1.24s/it] 15%|█▍        | 7302/50000 [2:36:46<14:42:34,  1.24s/it] 15%|█▍        | 7303/50000 [2:36:47<14:42:45,  1.24s/it] 15%|█▍        | 7304/50000 [2:36:48<14:43:03,  1.24s/it] 15%|█▍        | 7305/50000 [2:36:50<14:42:51,  1.24s/it] 15%|█▍        | 7306/50000 [2:36:51<14:42:42,  1.24s/it] 15%|█▍        | 7307/50000 [2:36:52<14:42:25,  1.24s/it] 15%|█▍        | 7308/50000 [2:36:53<14:42:29,  1.24s/it] 15%|█▍        | 7309/50000 [2:36:54<14:42:16,  1.24s/it] 15%|█▍        | 7310/50000 [2:36:56<14:42:13,  1.24s/it]                                                         {'loss': 117.4078, 'learning_rate': 1.9517490034609715e-05, 'epoch': 1.13}
 15%|█▍        | 7310/50000 [2:36:56<14:42:13,  1.24s/it] 15%|█▍        | 7311/50000 [2:36:57<14:42:17,  1.24s/it] 15%|█▍        | 7312/50000 [2:36:58<14:42:09,  1.24s/it] 15%|█▍        | 7313/50000 [2:36:59<14:42:00,  1.24s/it] 15%|█▍        | 7314/50000 [2:37:01<14:41:54,  1.24s/it] 15%|█▍        | 7315/50000 [2:37:02<14:41:47,  1.24s/it] 15%|█▍        | 7316/50000 [2:37:03<14:41:47,  1.24s/it] 15%|█▍        | 7317/50000 [2:37:04<14:41:40,  1.24s/it] 15%|█▍        | 7318/50000 [2:37:06<14:41:36,  1.24s/it] 15%|█▍        | 7319/50000 [2:37:07<14:41:38,  1.24s/it] 15%|█▍        | 7320/50000 [2:37:08<14:41:38,  1.24s/it]                                                         {'loss': 115.3953, 'learning_rate': 1.9515458302014295e-05, 'epoch': 1.13}
 15%|█▍        | 7320/50000 [2:37:08<14:41:38,  1.24s/it] 15%|█▍        | 7321/50000 [2:37:09<14:42:07,  1.24s/it] 15%|█▍        | 7322/50000 [2:37:11<14:42:00,  1.24s/it] 15%|█▍        | 7323/50000 [2:37:12<14:42:02,  1.24s/it] 15%|█▍        | 7324/50000 [2:37:13<14:41:50,  1.24s/it] 15%|█▍        | 7325/50000 [2:37:14<14:41:41,  1.24s/it] 15%|█▍        | 7326/50000 [2:37:16<14:41:30,  1.24s/it] 15%|█▍        | 7327/50000 [2:37:17<14:41:32,  1.24s/it] 15%|█▍        | 7328/50000 [2:37:18<14:41:24,  1.24s/it] 15%|█▍        | 7329/50000 [2:37:19<14:41:19,  1.24s/it] 15%|█▍        | 7330/50000 [2:37:21<14:41:20,  1.24s/it]                                                         {'loss': 119.6477, 'learning_rate': 1.9513422407034136e-05, 'epoch': 1.14}
 15%|█▍        | 7330/50000 [2:37:21<14:41:20,  1.24s/it] 15%|█▍        | 7331/50000 [2:37:22<14:41:40,  1.24s/it] 15%|█▍        | 7332/50000 [2:37:23<14:41:33,  1.24s/it] 15%|█▍        | 7333/50000 [2:37:24<14:41:19,  1.24s/it] 15%|█▍        | 7334/50000 [2:37:25<14:41:24,  1.24s/it] 15%|█▍        | 7335/50000 [2:37:27<14:41:28,  1.24s/it] 15%|█▍        | 7336/50000 [2:37:28<14:41:20,  1.24s/it] 15%|█▍        | 7337/50000 [2:37:29<14:41:25,  1.24s/it] 15%|█▍        | 7338/50000 [2:37:30<14:41:18,  1.24s/it] 15%|█▍        | 7339/50000 [2:37:32<14:41:15,  1.24s/it] 15%|█▍        | 7340/50000 [2:37:33<14:41:16,  1.24s/it]                                                         {'loss': 131.6516, 'learning_rate': 1.9511382350559803e-05, 'epoch': 1.14}
 15%|█▍        | 7340/50000 [2:37:33<14:41:16,  1.24s/it] 15%|█▍        | 7341/50000 [2:37:34<14:41:21,  1.24s/it] 15%|█▍        | 7342/50000 [2:37:35<14:41:20,  1.24s/it] 15%|█▍        | 7343/50000 [2:37:37<14:41:30,  1.24s/it] 15%|█▍        | 7344/50000 [2:37:38<14:41:24,  1.24s/it] 15%|█▍        | 7345/50000 [2:37:39<14:41:24,  1.24s/it] 15%|█▍        | 7346/50000 [2:37:40<14:41:08,  1.24s/it] 15%|█▍        | 7347/50000 [2:37:42<14:41:04,  1.24s/it] 15%|█▍        | 7348/50000 [2:37:43<14:41:01,  1.24s/it] 15%|█▍        | 7349/50000 [2:37:44<14:41:07,  1.24s/it] 15%|█▍        | 7350/50000 [2:37:45<14:41:07,  1.24s/it]                                                         {'loss': 132.4516, 'learning_rate': 1.9509338133483692e-05, 'epoch': 1.14}
 15%|█▍        | 7350/50000 [2:37:45<14:41:07,  1.24s/it] 15%|█▍        | 7351/50000 [2:37:47<14:41:38,  1.24s/it] 15%|█▍        | 7352/50000 [2:37:48<14:41:20,  1.24s/it] 15%|█▍        | 7353/50000 [2:37:49<14:41:17,  1.24s/it] 15%|█▍        | 7354/50000 [2:37:50<14:41:04,  1.24s/it] 15%|█▍        | 7355/50000 [2:37:52<14:40:50,  1.24s/it] 15%|█▍        | 7356/50000 [2:37:53<14:40:49,  1.24s/it] 15%|█▍        | 7357/50000 [2:37:54<14:40:53,  1.24s/it] 15%|█▍        | 7358/50000 [2:37:55<14:40:56,  1.24s/it] 15%|█▍        | 7359/50000 [2:37:56<14:40:51,  1.24s/it] 15%|█▍        | 7360/50000 [2:37:58<14:40:48,  1.24s/it]                                                         {'loss': 118.7914, 'learning_rate': 1.9507289756700006e-05, 'epoch': 1.14}
 15%|█▍        | 7360/50000 [2:37:58<14:40:48,  1.24s/it] 15%|█▍        | 7361/50000 [2:37:59<14:40:52,  1.24s/it] 15%|█▍        | 7362/50000 [2:38:00<14:40:50,  1.24s/it] 15%|█▍        | 7363/50000 [2:38:01<14:40:54,  1.24s/it] 15%|█▍        | 7364/50000 [2:38:03<14:40:46,  1.24s/it] 15%|█▍        | 7365/50000 [2:38:04<14:40:48,  1.24s/it] 15%|█▍        | 7366/50000 [2:38:05<14:41:13,  1.24s/it] 15%|█▍        | 7367/50000 [2:38:06<14:40:56,  1.24s/it] 15%|█▍        | 7368/50000 [2:38:08<14:41:42,  1.24s/it] 15%|█▍        | 7369/50000 [2:38:09<14:41:22,  1.24s/it] 15%|█▍        | 7370/50000 [2:38:10<14:41:24,  1.24s/it]                                                         {'loss': 109.8664, 'learning_rate': 1.9505237221104783e-05, 'epoch': 1.14}
 15%|█▍        | 7370/50000 [2:38:10<14:41:24,  1.24s/it] 15%|█▍        | 7371/50000 [2:38:11<14:41:25,  1.24s/it] 15%|█▍        | 7372/50000 [2:38:13<14:41:15,  1.24s/it] 15%|█▍        | 7373/50000 [2:38:14<14:41:04,  1.24s/it] 15%|█▍        | 7374/50000 [2:38:15<14:40:52,  1.24s/it] 15%|█▍        | 7375/50000 [2:38:16<14:40:47,  1.24s/it] 15%|█▍        | 7376/50000 [2:38:18<14:40:31,  1.24s/it] 15%|█▍        | 7377/50000 [2:38:19<14:40:19,  1.24s/it] 15%|█▍        | 7378/50000 [2:38:20<14:40:21,  1.24s/it] 15%|█▍        | 7379/50000 [2:38:21<14:40:23,  1.24s/it] 15%|█▍        | 7380/50000 [2:38:23<14:40:03,  1.24s/it]                                                         {'loss': 102.943, 'learning_rate': 1.9503180527595866e-05, 'epoch': 1.14}
 15%|█▍        | 7380/50000 [2:38:23<14:40:03,  1.24s/it] 15%|█▍        | 7381/50000 [2:38:24<14:40:15,  1.24s/it] 15%|█▍        | 7382/50000 [2:38:25<14:40:16,  1.24s/it] 15%|█▍        | 7383/50000 [2:38:26<14:40:11,  1.24s/it] 15%|█▍        | 7384/50000 [2:38:27<14:40:04,  1.24s/it] 15%|█▍        | 7385/50000 [2:38:29<14:39:58,  1.24s/it] 15%|█▍        | 7386/50000 [2:38:30<14:39:58,  1.24s/it] 15%|█▍        | 7387/50000 [2:38:31<14:39:56,  1.24s/it] 15%|█▍        | 7388/50000 [2:38:32<14:40:08,  1.24s/it] 15%|█▍        | 7389/50000 [2:38:34<14:40:17,  1.24s/it] 15%|█▍        | 7390/50000 [2:38:35<14:39:57,  1.24s/it]                                                         {'loss': 119.0906, 'learning_rate': 1.950111967707292e-05, 'epoch': 1.15}
 15%|█▍        | 7390/50000 [2:38:35<14:39:57,  1.24s/it] 15%|█▍        | 7391/50000 [2:38:36<14:40:05,  1.24s/it] 15%|█▍        | 7392/50000 [2:38:37<14:40:02,  1.24s/it] 15%|█▍        | 7393/50000 [2:38:39<14:39:51,  1.24s/it] 15%|█▍        | 7394/50000 [2:38:40<14:40:00,  1.24s/it] 15%|█▍        | 7395/50000 [2:38:41<14:40:10,  1.24s/it] 15%|█▍        | 7396/50000 [2:38:42<14:40:05,  1.24s/it] 15%|█▍        | 7397/50000 [2:38:44<14:40:10,  1.24s/it] 15%|█▍        | 7398/50000 [2:38:45<14:40:08,  1.24s/it] 15%|█▍        | 7399/50000 [2:38:46<14:39:56,  1.24s/it] 15%|█▍        | 7400/50000 [2:38:47<14:39:56,  1.24s/it]                                                         {'loss': 110.4367, 'learning_rate': 1.949905467043744e-05, 'epoch': 1.15}
 15%|█▍        | 7400/50000 [2:38:47<14:39:56,  1.24s/it] 15%|█▍        | 7401/50000 [2:38:49<14:39:56,  1.24s/it] 15%|█▍        | 7402/50000 [2:38:50<14:40:03,  1.24s/it] 15%|█▍        | 7403/50000 [2:38:51<14:40:16,  1.24s/it] 15%|█▍        | 7404/50000 [2:38:52<14:40:20,  1.24s/it] 15%|█▍        | 7405/50000 [2:38:53<14:40:18,  1.24s/it] 15%|█▍        | 7406/50000 [2:38:55<14:40:08,  1.24s/it] 15%|█▍        | 7407/50000 [2:38:56<14:40:06,  1.24s/it] 15%|█▍        | 7408/50000 [2:38:57<14:40:00,  1.24s/it] 15%|█▍        | 7409/50000 [2:38:58<14:39:55,  1.24s/it] 15%|█▍        | 7410/50000 [2:39:00<14:40:02,  1.24s/it]                                                         {'loss': 105.8289, 'learning_rate': 1.9496985508592723e-05, 'epoch': 1.15}
 15%|█▍        | 7410/50000 [2:39:00<14:40:02,  1.24s/it] 15%|█▍        | 7411/50000 [2:39:01<14:40:15,  1.24s/it] 15%|█▍        | 7412/50000 [2:39:02<14:40:07,  1.24s/it] 15%|█▍        | 7413/50000 [2:39:03<14:40:03,  1.24s/it] 15%|█▍        | 7414/50000 [2:39:05<14:39:50,  1.24s/it] 15%|█▍        | 7415/50000 [2:39:06<14:39:44,  1.24s/it] 15%|█▍        | 7416/50000 [2:39:07<14:39:50,  1.24s/it] 15%|█▍        | 7417/50000 [2:39:08<14:40:05,  1.24s/it] 15%|█▍        | 7418/50000 [2:39:10<14:39:57,  1.24s/it] 15%|█▍        | 7419/50000 [2:39:11<14:39:57,  1.24s/it][2023-07-03 14:39:22,360] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, but hysteresis is 2. Reducing hysteresis to 1
 15%|█▍        | 7420/50000 [2:39:12<13:47:01,  1.17s/it]                                                         {'loss': 115.1836, 'learning_rate': 1.9495119710976628e-05, 'epoch': 1.15}
 15%|█▍        | 7420/50000 [2:39:12<13:47:01,  1.17s/it] 15%|█▍        | 7421/50000 [2:39:13<14:02:48,  1.19s/it] 15%|█▍        | 7422/50000 [2:39:14<14:13:48,  1.20s/it] 15%|█▍        | 7423/50000 [2:39:16<14:21:59,  1.21s/it] 15%|█▍        | 7424/50000 [2:39:17<14:27:34,  1.22s/it] 15%|█▍        | 7425/50000 [2:39:18<14:31:20,  1.23s/it] 15%|█▍        | 7426/50000 [2:39:19<14:34:18,  1.23s/it] 15%|█▍        | 7427/50000 [2:39:21<14:37:03,  1.24s/it] 15%|█▍        | 7428/50000 [2:39:22<14:38:25,  1.24s/it] 15%|█▍        | 7429/50000 [2:39:23<14:38:46,  1.24s/it] 15%|█▍        | 7430/50000 [2:39:24<14:39:00,  1.24s/it]                                                         {'loss': 103.75, 'learning_rate': 1.9493042656729475e-05, 'epoch': 1.15}
 15%|█▍        | 7430/50000 [2:39:24<14:39:00,  1.24s/it] 15%|█▍        | 7431/50000 [2:39:26<14:40:06,  1.24s/it] 15%|█▍        | 7432/50000 [2:39:27<14:39:55,  1.24s/it] 15%|█▍        | 7433/50000 [2:39:28<14:39:41,  1.24s/it] 15%|█▍        | 7434/50000 [2:39:29<14:39:17,  1.24s/it] 15%|█▍        | 7435/50000 [2:39:30<14:39:24,  1.24s/it] 15%|█▍        | 7436/50000 [2:39:32<14:39:11,  1.24s/it] 15%|█▍        | 7437/50000 [2:39:33<14:39:13,  1.24s/it] 15%|█▍        | 7438/50000 [2:39:34<14:39:24,  1.24s/it] 15%|█▍        | 7439/50000 [2:39:35<14:39:42,  1.24s/it] 15%|█▍        | 7440/50000 [2:39:37<14:39:36,  1.24s/it]                                                         {'loss': 126.7203, 'learning_rate': 1.9490961449902946e-05, 'epoch': 1.15}
 15%|█▍        | 7440/50000 [2:39:37<14:39:36,  1.24s/it] 15%|█▍        | 7441/50000 [2:39:38<14:40:07,  1.24s/it] 15%|█▍        | 7442/50000 [2:39:39<14:40:00,  1.24s/it] 15%|█▍        | 7443/50000 [2:39:40<14:39:50,  1.24s/it] 15%|█▍        | 7444/50000 [2:39:42<14:39:48,  1.24s/it] 15%|█▍        | 7445/50000 [2:39:43<14:39:49,  1.24s/it] 15%|█▍        | 7446/50000 [2:39:44<14:39:38,  1.24s/it] 15%|█▍        | 7447/50000 [2:39:45<14:39:36,  1.24s/it] 15%|█▍        | 7448/50000 [2:39:47<14:39:51,  1.24s/it] 15%|█▍        | 7449/50000 [2:39:48<14:39:35,  1.24s/it] 15%|█▍        | 7450/50000 [2:39:49<14:39:46,  1.24s/it]                                                         {'loss': 118.4656, 'learning_rate': 1.9488876091407435e-05, 'epoch': 1.15}
 15%|█▍        | 7450/50000 [2:39:49<14:39:46,  1.24s/it] 15%|█▍        | 7451/50000 [2:39:50<14:39:53,  1.24s/it] 15%|█▍        | 7452/50000 [2:39:52<14:39:39,  1.24s/it][2023-07-03 14:40:03,050] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
 15%|█▍        | 7453/50000 [2:39:53<13:46:19,  1.17s/it] 15%|█▍        | 7454/50000 [2:39:54<14:02:10,  1.19s/it] 15%|█▍        | 7455/50000 [2:39:55<14:13:23,  1.20s/it] 15%|█▍        | 7456/50000 [2:39:56<14:21:02,  1.21s/it] 15%|█▍        | 7457/50000 [2:39:57<14:26:36,  1.22s/it] 15%|█▍        | 7458/50000 [2:39:59<14:30:18,  1.23s/it] 15%|█▍        | 7459/50000 [2:40:00<14:32:56,  1.23s/it] 15%|█▍        | 7460/50000 [2:40:01<14:34:41,  1.23s/it]                                                         {'loss': 115.4898, 'learning_rate': 1.9486995719838392e-05, 'epoch': 1.16}
 15%|█▍        | 7460/50000 [2:40:01<14:34:41,  1.23s/it] 15%|█▍        | 7461/50000 [2:40:02<14:36:12,  1.24s/it] 15%|█▍        | 7462/50000 [2:40:04<14:36:58,  1.24s/it] 15%|█▍        | 7463/50000 [2:40:05<14:37:47,  1.24s/it] 15%|█▍        | 7464/50000 [2:40:06<14:38:33,  1.24s/it] 15%|█▍        | 7465/50000 [2:40:07<14:38:31,  1.24s/it] 15%|█▍        | 7466/50000 [2:40:09<14:38:52,  1.24s/it] 15%|█▍        | 7467/50000 [2:40:10<14:38:47,  1.24s/it] 15%|█▍        | 7468/50000 [2:40:11<14:38:49,  1.24s/it] 15%|█▍        | 7469/50000 [2:40:12<14:38:44,  1.24s/it] 15%|█▍        | 7470/50000 [2:40:14<14:38:41,  1.24s/it]                                                         {'loss': 117.3363, 'learning_rate': 1.9484902475686438e-05, 'epoch': 1.16}
 15%|█▍        | 7470/50000 [2:40:14<14:38:41,  1.24s/it] 15%|█▍        | 7471/50000 [2:40:15<14:38:54,  1.24s/it] 15%|█▍        | 7472/50000 [2:40:16<14:38:54,  1.24s/it] 15%|█▍        | 7473/50000 [2:40:17<14:38:47,  1.24s/it] 15%|█▍        | 7474/50000 [2:40:19<14:38:39,  1.24s/it] 15%|█▍        | 7475/50000 [2:40:20<14:38:23,  1.24s/it] 15%|█▍        | 7476/50000 [2:40:21<14:38:31,  1.24s/it] 15%|█▍        | 7477/50000 [2:40:22<14:38:53,  1.24s/it] 15%|█▍        | 7478/50000 [2:40:24<14:38:55,  1.24s/it] 15%|█▍        | 7479/50000 [2:40:25<14:38:46,  1.24s/it] 15%|█▍        | 7480/50000 [2:40:26<14:38:58,  1.24s/it]                                                         {'loss': 122.8937, 'learning_rate': 1.94828050825159e-05, 'epoch': 1.16}
 15%|█▍        | 7480/50000 [2:40:26<14:38:58,  1.24s/it] 15%|█▍        | 7481/50000 [2:40:27<14:39:06,  1.24s/it] 15%|█▍        | 7482/50000 [2:40:28<14:38:51,  1.24s/it] 15%|█▍        | 7483/50000 [2:40:30<14:38:35,  1.24s/it] 15%|█▍        | 7484/50000 [2:40:31<14:38:35,  1.24s/it] 15%|█▍        | 7485/50000 [2:40:32<14:38:37,  1.24s/it] 15%|█▍        | 7486/50000 [2:40:33<14:38:44,  1.24s/it] 15%|█▍        | 7487/50000 [2:40:35<14:38:59,  1.24s/it] 15%|█▍        | 7488/50000 [2:40:36<14:52:51,  1.26s/it] 15%|█▍        | 7489/50000 [2:40:37<14:48:43,  1.25s/it] 15%|█▍        | 7490/50000 [2:40:38<14:45:43,  1.25s/it]                                                         {'loss': 123.1344, 'learning_rate': 1.9480703541244244e-05, 'epoch': 1.16}
 15%|█▍        | 7490/50000 [2:40:38<14:45:43,  1.25s/it] 15%|█▍        | 7491/50000 [2:40:40<14:44:11,  1.25s/it] 15%|█▍        | 7492/50000 [2:40:41<14:42:41,  1.25s/it] 15%|█▍        | 7493/50000 [2:40:42<14:41:29,  1.24s/it] 15%|█▍        | 7494/50000 [2:40:43<14:40:49,  1.24s/it] 15%|█▍        | 7495/50000 [2:40:45<14:40:04,  1.24s/it] 15%|█▍        | 7496/50000 [2:40:46<14:39:32,  1.24s/it] 15%|█▍        | 7497/50000 [2:40:47<14:39:14,  1.24s/it] 15%|█▍        | 7498/50000 [2:40:48<14:38:57,  1.24s/it] 15%|█▍        | 7499/50000 [2:40:50<14:38:49,  1.24s/it] 15%|█▌        | 7500/50000 [2:40:51<14:38:41,  1.24s/it]                                                         {'loss': 108.8844, 'learning_rate': 1.9478597852790768e-05, 'epoch': 1.16}
 15%|█▌        | 7500/50000 [2:40:51<14:38:41,  1.24s/it] 15%|█▌        | 7501/50000 [2:40:52<14:39:08,  1.24s/it] 15%|█▌        | 7502/50000 [2:40:53<14:38:53,  1.24s/it] 15%|█▌        | 7503/50000 [2:40:55<14:38:40,  1.24s/it] 15%|█▌        | 7504/50000 [2:40:56<14:38:32,  1.24s/it] 15%|█▌        | 7505/50000 [2:40:57<14:38:29,  1.24s/it] 15%|█▌        | 7506/50000 [2:40:58<14:38:10,  1.24s/it] 15%|█▌        | 7507/50000 [2:41:00<14:37:52,  1.24s/it] 15%|█▌        | 7508/50000 [2:41:01<14:37:48,  1.24s/it] 15%|█▌        | 7509/50000 [2:41:02<14:38:20,  1.24s/it][2023-07-03 14:41:13,561] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 15%|█▌        | 7510/50000 [2:41:03<13:45:32,  1.17s/it]                                                         {'loss': 120.0078, 'learning_rate': 1.9476699188103426e-05, 'epoch': 1.16}
 15%|█▌        | 7510/50000 [2:41:03<13:45:32,  1.17s/it] 15%|█▌        | 7511/50000 [2:41:04<14:01:12,  1.19s/it] 15%|█▌        | 7512/50000 [2:41:06<14:12:09,  1.20s/it] 15%|█▌        | 7513/50000 [2:41:07<14:20:02,  1.21s/it] 15%|█▌        | 7514/50000 [2:41:08<14:25:20,  1.22s/it] 15%|█▌        | 7515/50000 [2:41:09<14:29:02,  1.23s/it] 15%|█▌        | 7516/50000 [2:41:10<14:31:51,  1.23s/it] 15%|█▌        | 7517/50000 [2:41:12<14:34:03,  1.23s/it] 15%|█▌        | 7518/50000 [2:41:13<14:35:11,  1.24s/it] 15%|█▌        | 7519/50000 [2:41:14<14:36:23,  1.24s/it] 15%|█▌        | 7520/50000 [2:41:15<14:36:41,  1.24s/it]                                                         {'loss': 127.4148, 'learning_rate': 1.94745856225436e-05, 'epoch': 1.17}
 15%|█▌        | 7520/50000 [2:41:15<14:36:41,  1.24s/it] 15%|█▌        | 7521/50000 [2:41:17<14:37:03,  1.24s/it] 15%|█▌        | 7522/50000 [2:41:18<14:37:02,  1.24s/it] 15%|█▌        | 7523/50000 [2:41:19<14:37:09,  1.24s/it] 15%|█▌        | 7524/50000 [2:41:20<14:37:38,  1.24s/it] 15%|█▌        | 7525/50000 [2:41:22<14:37:34,  1.24s/it] 15%|█▌        | 7526/50000 [2:41:23<14:37:25,  1.24s/it] 15%|█▌        | 7527/50000 [2:41:24<14:37:29,  1.24s/it] 15%|█▌        | 7528/50000 [2:41:25<14:37:21,  1.24s/it] 15%|█▌        | 7529/50000 [2:41:27<14:37:18,  1.24s/it] 15%|█▌        | 7530/50000 [2:41:28<14:37:12,  1.24s/it]                                                         {'loss': 107.332, 'learning_rate': 1.9472467912478135e-05, 'epoch': 1.17}
 15%|█▌        | 7530/50000 [2:41:28<14:37:12,  1.24s/it] 15%|█▌        | 7531/50000 [2:41:29<14:37:19,  1.24s/it] 15%|█▌        | 7532/50000 [2:41:30<14:37:56,  1.24s/it] 15%|█▌        | 7533/50000 [2:41:32<14:37:53,  1.24s/it] 15%|█▌        | 7534/50000 [2:41:33<14:37:42,  1.24s/it] 15%|█▌        | 7535/50000 [2:41:34<14:37:36,  1.24s/it] 15%|█▌        | 7536/50000 [2:41:35<14:38:10,  1.24s/it] 15%|█▌        | 7537/50000 [2:41:37<14:37:53,  1.24s/it] 15%|█▌        | 7538/50000 [2:41:38<14:37:37,  1.24s/it] 15%|█▌        | 7539/50000 [2:41:39<14:37:38,  1.24s/it] 15%|█▌        | 7540/50000 [2:41:40<14:37:40,  1.24s/it]                                                         {'loss': 114.0516, 'learning_rate': 1.947034605883339e-05, 'epoch': 1.17}
 15%|█▌        | 7540/50000 [2:41:40<14:37:40,  1.24s/it] 15%|█▌        | 7541/50000 [2:41:41<14:38:07,  1.24s/it] 15%|█▌        | 7542/50000 [2:41:43<14:37:51,  1.24s/it] 15%|█▌        | 7543/50000 [2:41:44<14:37:47,  1.24s/it] 15%|█▌        | 7544/50000 [2:41:45<14:38:14,  1.24s/it] 15%|█▌        | 7545/50000 [2:41:46<14:38:13,  1.24s/it] 15%|█▌        | 7546/50000 [2:41:48<14:38:11,  1.24s/it] 15%|█▌        | 7547/50000 [2:41:49<14:38:30,  1.24s/it] 15%|█▌        | 7548/50000 [2:41:50<14:38:20,  1.24s/it] 15%|█▌        | 7549/50000 [2:41:51<14:38:01,  1.24s/it] 15%|█▌        | 7550/50000 [2:41:53<14:37:37,  1.24s/it]                                                         {'loss': 114.6227, 'learning_rate': 1.9468220062537522e-05, 'epoch': 1.17}
 15%|█▌        | 7550/50000 [2:41:53<14:37:37,  1.24s/it] 15%|█▌        | 7551/50000 [2:41:54<14:37:35,  1.24s/it] 15%|█▌        | 7552/50000 [2:41:55<14:37:27,  1.24s/it] 15%|█▌        | 7553/50000 [2:41:56<14:37:11,  1.24s/it] 15%|█▌        | 7554/50000 [2:41:58<14:37:09,  1.24s/it] 15%|█▌        | 7555/50000 [2:41:59<14:37:23,  1.24s/it] 15%|█▌        | 7556/50000 [2:42:00<14:37:37,  1.24s/it] 15%|█▌        | 7557/50000 [2:42:01<14:37:31,  1.24s/it] 15%|█▌        | 7558/50000 [2:42:03<14:37:15,  1.24s/it] 15%|█▌        | 7559/50000 [2:42:04<14:37:04,  1.24s/it] 15%|█▌        | 7560/50000 [2:42:05<14:36:52,  1.24s/it]                                                         {'loss': 117.5883, 'learning_rate': 1.946608992452053e-05, 'epoch': 1.17}
 15%|█▌        | 7560/50000 [2:42:05<14:36:52,  1.24s/it] 15%|█▌        | 7561/50000 [2:42:06<14:37:29,  1.24s/it] 15%|█▌        | 7562/50000 [2:42:08<14:37:22,  1.24s/it] 15%|█▌        | 7563/50000 [2:42:09<14:36:59,  1.24s/it] 15%|█▌        | 7564/50000 [2:42:10<14:36:56,  1.24s/it] 15%|█▌        | 7565/50000 [2:42:11<14:37:10,  1.24s/it] 15%|█▌        | 7566/50000 [2:42:13<14:36:59,  1.24s/it] 15%|█▌        | 7567/50000 [2:42:14<14:36:48,  1.24s/it] 15%|█▌        | 7568/50000 [2:42:15<14:36:57,  1.24s/it] 15%|█▌        | 7569/50000 [2:42:16<14:36:44,  1.24s/it] 15%|█▌        | 7570/50000 [2:42:17<14:36:45,  1.24s/it]                                                         {'loss': 107.6242, 'learning_rate': 1.9463955645714204e-05, 'epoch': 1.17}
 15%|█▌        | 7570/50000 [2:42:17<14:36:45,  1.24s/it] 15%|█▌        | 7571/50000 [2:42:19<14:37:40,  1.24s/it] 15%|█▌        | 7572/50000 [2:42:20<14:37:07,  1.24s/it] 15%|█▌        | 7573/50000 [2:42:21<14:36:55,  1.24s/it] 15%|█▌        | 7574/50000 [2:42:22<14:36:43,  1.24s/it] 15%|█▌        | 7575/50000 [2:42:24<14:36:47,  1.24s/it] 15%|█▌        | 7576/50000 [2:42:25<14:36:38,  1.24s/it] 15%|█▌        | 7577/50000 [2:42:26<14:36:45,  1.24s/it] 15%|█▌        | 7578/50000 [2:42:27<14:37:16,  1.24s/it] 15%|█▌        | 7579/50000 [2:42:29<14:37:16,  1.24s/it] 15%|█▌        | 7580/50000 [2:42:30<14:36:47,  1.24s/it]                                                         {'loss': 119.5344, 'learning_rate': 1.9461817227052146e-05, 'epoch': 1.18}
 15%|█▌        | 7580/50000 [2:42:30<14:36:47,  1.24s/it] 15%|█▌        | 7581/50000 [2:42:31<14:37:36,  1.24s/it] 15%|█▌        | 7582/50000 [2:42:32<14:37:25,  1.24s/it] 15%|█▌        | 7583/50000 [2:42:34<14:37:02,  1.24s/it] 15%|█▌        | 7584/50000 [2:42:35<14:36:53,  1.24s/it] 15%|█▌        | 7585/50000 [2:42:36<14:37:28,  1.24s/it] 15%|█▌        | 7586/50000 [2:42:37<14:37:08,  1.24s/it] 15%|█▌        | 7587/50000 [2:42:39<14:37:11,  1.24s/it] 15%|█▌        | 7588/50000 [2:42:40<14:37:09,  1.24s/it] 15%|█▌        | 7589/50000 [2:42:41<14:37:12,  1.24s/it] 15%|█▌        | 7590/50000 [2:42:42<14:37:06,  1.24s/it]                                                         {'loss': 123.5422, 'learning_rate': 1.9459674669469778e-05, 'epoch': 1.18}
 15%|█▌        | 7590/50000 [2:42:42<14:37:06,  1.24s/it] 15%|█▌        | 7591/50000 [2:42:44<14:37:19,  1.24s/it] 15%|█▌        | 7592/50000 [2:42:45<14:36:56,  1.24s/it] 15%|█▌        | 7593/50000 [2:42:46<14:36:44,  1.24s/it] 15%|█▌        | 7594/50000 [2:42:47<14:36:42,  1.24s/it] 15%|█▌        | 7595/50000 [2:42:48<14:36:25,  1.24s/it] 15%|█▌        | 7596/50000 [2:42:50<14:36:22,  1.24s/it] 15%|█▌        | 7597/50000 [2:42:51<14:36:09,  1.24s/it] 15%|█▌        | 7598/50000 [2:42:52<14:36:36,  1.24s/it] 15%|█▌        | 7599/50000 [2:42:53<14:36:29,  1.24s/it] 15%|█▌        | 7600/50000 [2:42:55<14:36:13,  1.24s/it]                                                         {'loss': 121.2703, 'learning_rate': 1.9457527973904326e-05, 'epoch': 1.18}
 15%|█▌        | 7600/50000 [2:42:55<14:36:13,  1.24s/it] 15%|█▌        | 7601/50000 [2:42:56<14:36:23,  1.24s/it] 15%|█▌        | 7602/50000 [2:42:57<14:35:59,  1.24s/it] 15%|█▌        | 7603/50000 [2:42:58<14:36:13,  1.24s/it] 15%|█▌        | 7604/50000 [2:43:00<14:35:59,  1.24s/it] 15%|█▌        | 7605/50000 [2:43:01<14:35:55,  1.24s/it] 15%|█▌        | 7606/50000 [2:43:02<14:36:12,  1.24s/it] 15%|█▌        | 7607/50000 [2:43:03<14:36:09,  1.24s/it] 15%|█▌        | 7608/50000 [2:43:05<14:35:24,  1.24s/it] 15%|█▌        | 7609/50000 [2:43:06<14:35:04,  1.24s/it] 15%|█▌        | 7610/50000 [2:43:07<14:35:04,  1.24s/it]                                                         {'loss': 119.7656, 'learning_rate': 1.9455377141294827e-05, 'epoch': 1.18}
 15%|█▌        | 7610/50000 [2:43:07<14:35:04,  1.24s/it] 15%|█▌        | 7611/50000 [2:43:08<14:35:26,  1.24s/it] 15%|█▌        | 7612/50000 [2:43:10<14:35:09,  1.24s/it] 15%|█▌        | 7613/50000 [2:43:11<14:35:02,  1.24s/it] 15%|█▌        | 7614/50000 [2:43:12<14:35:41,  1.24s/it] 15%|█▌        | 7615/50000 [2:43:13<14:35:15,  1.24s/it] 15%|█▌        | 7616/50000 [2:43:15<14:35:28,  1.24s/it] 15%|█▌        | 7617/50000 [2:43:16<14:35:37,  1.24s/it] 15%|█▌        | 7618/50000 [2:43:17<14:35:34,  1.24s/it] 15%|█▌        | 7619/50000 [2:43:18<14:35:23,  1.24s/it] 15%|█▌        | 7620/50000 [2:43:19<14:34:32,  1.24s/it]                                                         {'loss': 114.6977, 'learning_rate': 1.9453222172582125e-05, 'epoch': 1.18}
 15%|█▌        | 7620/50000 [2:43:19<14:34:32,  1.24s/it] 15%|█▌        | 7621/50000 [2:43:21<14:34:05,  1.24s/it] 15%|█▌        | 7622/50000 [2:43:22<14:33:46,  1.24s/it] 15%|█▌        | 7623/50000 [2:43:23<14:33:08,  1.24s/it] 15%|█▌        | 7624/50000 [2:43:24<14:32:49,  1.24s/it] 15%|█▌        | 7625/50000 [2:43:26<14:33:03,  1.24s/it] 15%|█▌        | 7626/50000 [2:43:27<14:33:42,  1.24s/it] 15%|█▌        | 7627/50000 [2:43:28<14:33:45,  1.24s/it] 15%|█▌        | 7628/50000 [2:43:29<14:33:08,  1.24s/it] 15%|█▌        | 7629/50000 [2:43:31<14:33:07,  1.24s/it] 15%|█▌        | 7630/50000 [2:43:32<14:33:09,  1.24s/it]                                                         {'loss': 107.3047, 'learning_rate': 1.9451063068708883e-05, 'epoch': 1.18}
 15%|█▌        | 7630/50000 [2:43:32<14:33:09,  1.24s/it] 15%|█▌        | 7631/50000 [2:43:33<14:33:36,  1.24s/it] 15%|█▌        | 7632/50000 [2:43:34<14:33:47,  1.24s/it] 15%|█▌        | 7633/50000 [2:43:36<14:33:30,  1.24s/it] 15%|█▌        | 7634/50000 [2:43:37<14:33:18,  1.24s/it] 15%|█▌        | 7635/50000 [2:43:38<14:33:15,  1.24s/it] 15%|█▌        | 7636/50000 [2:43:39<14:33:03,  1.24s/it] 15%|█▌        | 7637/50000 [2:43:40<14:32:57,  1.24s/it] 15%|█▌        | 7638/50000 [2:43:42<14:33:08,  1.24s/it] 15%|█▌        | 7639/50000 [2:43:43<14:33:00,  1.24s/it] 15%|█▌        | 7640/50000 [2:43:44<14:32:42,  1.24s/it]                                                         {'loss': 122.7266, 'learning_rate': 1.944889983061956e-05, 'epoch': 1.18}
 15%|█▌        | 7640/50000 [2:43:44<14:32:42,  1.24s/it] 15%|█▌        | 7641/50000 [2:43:45<14:32:56,  1.24s/it] 15%|█▌        | 7642/50000 [2:43:47<14:32:53,  1.24s/it] 15%|█▌        | 7643/50000 [2:43:48<14:32:55,  1.24s/it] 15%|█▌        | 7644/50000 [2:43:49<14:32:49,  1.24s/it] 15%|█▌        | 7645/50000 [2:43:50<14:32:46,  1.24s/it] 15%|█▌        | 7646/50000 [2:43:52<14:32:40,  1.24s/it] 15%|█▌        | 7647/50000 [2:43:53<14:32:46,  1.24s/it] 15%|█▌        | 7648/50000 [2:43:54<14:32:21,  1.24s/it] 15%|█▌        | 7649/50000 [2:43:55<14:32:19,  1.24s/it] 15%|█▌        | 7650/50000 [2:43:57<14:32:33,  1.24s/it]                                                         {'loss': 116.2539, 'learning_rate': 1.9446732459260436e-05, 'epoch': 1.19}
 15%|█▌        | 7650/50000 [2:43:57<14:32:33,  1.24s/it] 15%|█▌        | 7651/50000 [2:43:58<14:34:20,  1.24s/it] 15%|█▌        | 7652/50000 [2:43:59<14:33:30,  1.24s/it] 15%|█▌        | 7653/50000 [2:44:00<14:32:54,  1.24s/it] 15%|█▌        | 7654/50000 [2:44:02<14:32:46,  1.24s/it] 15%|█▌        | 7655/50000 [2:44:03<14:32:46,  1.24s/it] 15%|█▌        | 7656/50000 [2:44:04<14:32:34,  1.24s/it] 15%|█▌        | 7657/50000 [2:44:05<14:32:25,  1.24s/it] 15%|█▌        | 7658/50000 [2:44:06<14:32:17,  1.24s/it] 15%|█▌        | 7659/50000 [2:44:08<14:32:25,  1.24s/it] 15%|█▌        | 7660/50000 [2:44:09<14:32:22,  1.24s/it]                                                         {'loss': 127.9453, 'learning_rate': 1.9444560955579587e-05, 'epoch': 1.19}
 15%|█▌        | 7660/50000 [2:44:09<14:32:22,  1.24s/it] 15%|█▌        | 7661/50000 [2:44:10<14:32:46,  1.24s/it] 15%|█▌        | 7662/50000 [2:44:11<14:33:20,  1.24s/it] 15%|█▌        | 7663/50000 [2:44:13<14:33:51,  1.24s/it] 15%|█▌        | 7664/50000 [2:44:14<14:33:58,  1.24s/it] 15%|█▌        | 7665/50000 [2:44:15<14:34:11,  1.24s/it] 15%|█▌        | 7666/50000 [2:44:16<14:34:23,  1.24s/it] 15%|█▌        | 7667/50000 [2:44:18<14:34:30,  1.24s/it] 15%|█▌        | 7668/50000 [2:44:19<14:34:20,  1.24s/it] 15%|█▌        | 7669/50000 [2:44:20<14:34:28,  1.24s/it] 15%|█▌        | 7670/50000 [2:44:21<14:35:04,  1.24s/it]                                                         {'loss': 127.8734, 'learning_rate': 1.9442385320526908e-05, 'epoch': 1.19}
 15%|█▌        | 7670/50000 [2:44:21<14:35:04,  1.24s/it] 15%|█▌        | 7671/50000 [2:44:23<14:35:13,  1.24s/it] 15%|█▌        | 7672/50000 [2:44:24<14:35:03,  1.24s/it] 15%|█▌        | 7673/50000 [2:44:25<14:34:44,  1.24s/it] 15%|█▌        | 7674/50000 [2:44:26<14:34:51,  1.24s/it] 15%|█▌        | 7675/50000 [2:44:28<14:34:50,  1.24s/it] 15%|█▌        | 7676/50000 [2:44:29<14:34:48,  1.24s/it] 15%|█▌        | 7677/50000 [2:44:30<14:34:32,  1.24s/it] 15%|█▌        | 7678/50000 [2:44:31<14:34:32,  1.24s/it] 15%|█▌        | 7679/50000 [2:44:32<14:34:29,  1.24s/it] 15%|█▌        | 7680/50000 [2:44:34<14:34:39,  1.24s/it]                                                         {'loss': 116.8328, 'learning_rate': 1.944020555505409e-05, 'epoch': 1.19}
 15%|█▌        | 7680/50000 [2:44:34<14:34:39,  1.24s/it] 15%|█▌        | 7681/50000 [2:44:35<14:34:38,  1.24s/it] 15%|█▌        | 7682/50000 [2:44:36<14:35:13,  1.24s/it] 15%|█▌        | 7683/50000 [2:44:37<14:35:02,  1.24s/it] 15%|█▌        | 7684/50000 [2:44:39<14:34:51,  1.24s/it] 15%|█▌        | 7685/50000 [2:44:40<14:34:36,  1.24s/it] 15%|█▌        | 7686/50000 [2:44:41<14:34:26,  1.24s/it] 15%|█▌        | 7687/50000 [2:44:42<14:34:17,  1.24s/it] 15%|█▌        | 7688/50000 [2:44:44<14:34:26,  1.24s/it] 15%|█▌        | 7689/50000 [2:44:45<14:34:23,  1.24s/it] 15%|█▌        | 7690/50000 [2:44:46<14:34:28,  1.24s/it]                                                         {'loss': 113.7047, 'learning_rate': 1.9438021660114643e-05, 'epoch': 1.19}
 15%|█▌        | 7690/50000 [2:44:46<14:34:28,  1.24s/it] 15%|█▌        | 7691/50000 [2:44:47<14:34:52,  1.24s/it] 15%|█▌        | 7692/50000 [2:44:49<14:34:31,  1.24s/it] 15%|█▌        | 7693/50000 [2:44:50<14:34:21,  1.24s/it] 15%|█▌        | 7694/50000 [2:44:51<14:34:15,  1.24s/it] 15%|█▌        | 7695/50000 [2:44:52<14:34:11,  1.24s/it] 15%|█▌        | 7696/50000 [2:44:54<14:34:05,  1.24s/it] 15%|█▌        | 7697/50000 [2:44:55<14:34:08,  1.24s/it] 15%|█▌        | 7698/50000 [2:44:56<14:34:08,  1.24s/it] 15%|█▌        | 7699/50000 [2:44:57<14:33:59,  1.24s/it] 15%|█▌        | 7700/50000 [2:44:59<14:33:53,  1.24s/it]                                                         {'loss': 118.8469, 'learning_rate': 1.9435833636663873e-05, 'epoch': 1.19}
 15%|█▌        | 7700/50000 [2:44:59<14:33:53,  1.24s/it] 15%|█▌        | 7701/50000 [2:45:00<14:35:34,  1.24s/it] 15%|█▌        | 7702/50000 [2:45:01<14:35:18,  1.24s/it][2023-07-03 14:45:12,514] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, but hysteresis is 2. Reducing hysteresis to 1
 15%|█▌        | 7703/50000 [2:45:02<13:42:53,  1.17s/it] 15%|█▌        | 7704/50000 [2:45:03<13:58:18,  1.19s/it] 15%|█▌        | 7705/50000 [2:45:04<14:09:13,  1.20s/it] 15%|█▌        | 7706/50000 [2:45:06<14:17:09,  1.22s/it] 15%|█▌        | 7707/50000 [2:45:07<14:23:01,  1.22s/it] 15%|█▌        | 7708/50000 [2:45:08<14:26:45,  1.23s/it] 15%|█▌        | 7709/50000 [2:45:09<14:29:47,  1.23s/it] 15%|█▌        | 7710/50000 [2:45:11<14:31:58,  1.24s/it]                                                         {'loss': 126.1984, 'learning_rate': 1.9433860886472015e-05, 'epoch': 1.2}
 15%|█▌        | 7710/50000 [2:45:11<14:31:58,  1.24s/it] 15%|█▌        | 7711/50000 [2:45:12<14:32:54,  1.24s/it] 15%|█▌        | 7712/50000 [2:45:13<14:33:22,  1.24s/it] 15%|█▌        | 7713/50000 [2:45:14<14:33:38,  1.24s/it] 15%|█▌        | 7714/50000 [2:45:16<14:33:51,  1.24s/it] 15%|█▌        | 7715/50000 [2:45:17<14:33:56,  1.24s/it] 15%|█▌        | 7716/50000 [2:45:18<14:34:49,  1.24s/it] 15%|█▌        | 7717/50000 [2:45:19<14:34:40,  1.24s/it] 15%|█▌        | 7718/50000 [2:45:21<14:34:39,  1.24s/it] 15%|█▌        | 7719/50000 [2:45:22<14:34:02,  1.24s/it] 15%|█▌        | 7720/50000 [2:45:23<14:33:58,  1.24s/it]                                                         {'loss': 119.4211, 'learning_rate': 1.9431665021488077e-05, 'epoch': 1.2}
 15%|█▌        | 7720/50000 [2:45:23<14:33:58,  1.24s/it] 15%|█▌        | 7721/50000 [2:45:24<14:37:10,  1.24s/it] 15%|█▌        | 7722/50000 [2:45:26<14:36:12,  1.24s/it] 15%|█▌        | 7723/50000 [2:45:27<14:35:31,  1.24s/it] 15%|█▌        | 7724/50000 [2:45:28<14:35:02,  1.24s/it] 15%|█▌        | 7725/50000 [2:45:29<14:35:00,  1.24s/it] 15%|█▌        | 7726/50000 [2:45:31<14:35:18,  1.24s/it] 15%|█▌        | 7727/50000 [2:45:32<14:34:51,  1.24s/it] 15%|█▌        | 7728/50000 [2:45:33<14:34:33,  1.24s/it] 15%|█▌        | 7729/50000 [2:45:34<14:34:19,  1.24s/it] 15%|█▌        | 7730/50000 [2:45:36<14:33:55,  1.24s/it]                                                         {'loss': 108.7844, 'learning_rate': 1.9429465030773427e-05, 'epoch': 1.2}
 15%|█▌        | 7730/50000 [2:45:36<14:33:55,  1.24s/it] 15%|█▌        | 7731/50000 [2:45:37<14:34:15,  1.24s/it] 15%|█▌        | 7732/50000 [2:45:38<14:34:27,  1.24s/it] 15%|█▌        | 7733/50000 [2:45:39<14:34:18,  1.24s/it] 15%|█▌        | 7734/50000 [2:45:40<14:34:47,  1.24s/it] 15%|█▌        | 7735/50000 [2:45:42<14:35:02,  1.24s/it] 15%|█▌        | 7736/50000 [2:45:43<14:34:36,  1.24s/it] 15%|█▌        | 7737/50000 [2:45:44<14:34:50,  1.24s/it] 15%|█▌        | 7738/50000 [2:45:45<14:34:44,  1.24s/it] 15%|█▌        | 7739/50000 [2:45:47<14:34:44,  1.24s/it] 15%|█▌        | 7740/50000 [2:45:48<14:34:42,  1.24s/it]                                                         {'loss': 106.575, 'learning_rate': 1.9427260915290413e-05, 'epoch': 1.2}
 15%|█▌        | 7740/50000 [2:45:48<14:34:42,  1.24s/it] 15%|█▌        | 7741/50000 [2:45:49<14:35:20,  1.24s/it] 15%|█▌        | 7742/50000 [2:45:50<14:35:10,  1.24s/it] 15%|█▌        | 7743/50000 [2:45:52<14:34:28,  1.24s/it] 15%|█▌        | 7744/50000 [2:45:53<14:33:48,  1.24s/it] 15%|█▌        | 7745/50000 [2:45:54<14:33:33,  1.24s/it] 15%|█▌        | 7746/50000 [2:45:55<14:33:28,  1.24s/it] 15%|█▌        | 7747/50000 [2:45:57<14:33:35,  1.24s/it] 15%|█▌        | 7748/50000 [2:45:58<14:33:34,  1.24s/it] 15%|█▌        | 7749/50000 [2:45:59<14:33:26,  1.24s/it] 16%|█▌        | 7750/50000 [2:46:00<14:33:04,  1.24s/it]                                                         {'loss': 100.3344, 'learning_rate': 1.9425052676003192e-05, 'epoch': 1.2}
 16%|█▌        | 7750/50000 [2:46:00<14:33:04,  1.24s/it] 16%|█▌        | 7751/50000 [2:46:02<14:33:12,  1.24s/it] 16%|█▌        | 7752/50000 [2:46:03<14:33:09,  1.24s/it] 16%|█▌        | 7753/50000 [2:46:04<14:33:09,  1.24s/it] 16%|█▌        | 7754/50000 [2:46:05<14:33:01,  1.24s/it] 16%|█▌        | 7755/50000 [2:46:07<14:33:10,  1.24s/it] 16%|█▌        | 7756/50000 [2:46:08<14:33:20,  1.24s/it] 16%|█▌        | 7757/50000 [2:46:09<14:33:08,  1.24s/it] 16%|█▌        | 7758/50000 [2:46:10<14:34:54,  1.24s/it] 16%|█▌        | 7759/50000 [2:46:12<14:36:04,  1.24s/it] 16%|█▌        | 7760/50000 [2:46:13<14:35:17,  1.24s/it]                                                         {'loss': 113.1266, 'learning_rate': 1.942284031387772e-05, 'epoch': 1.2}
 16%|█▌        | 7760/50000 [2:46:13<14:35:17,  1.24s/it] 16%|█▌        | 7761/50000 [2:46:14<14:34:38,  1.24s/it] 16%|█▌        | 7762/50000 [2:46:15<14:34:41,  1.24s/it] 16%|█▌        | 7763/50000 [2:46:16<14:34:02,  1.24s/it] 16%|█▌        | 7764/50000 [2:46:18<14:34:04,  1.24s/it] 16%|█▌        | 7765/50000 [2:46:19<14:33:51,  1.24s/it] 16%|█▌        | 7766/50000 [2:46:20<14:33:22,  1.24s/it] 16%|█▌        | 7767/50000 [2:46:22<14:57:51,  1.28s/it] 16%|█▌        | 7768/50000 [2:46:23<14:50:10,  1.26s/it] 16%|█▌        | 7769/50000 [2:46:24<14:45:05,  1.26s/it] 16%|█▌        | 7770/50000 [2:46:25<14:41:25,  1.25s/it]                                                         {'loss': 117.8688, 'learning_rate': 1.9420623829881765e-05, 'epoch': 1.2}
 16%|█▌        | 7770/50000 [2:46:25<14:41:25,  1.25s/it] 16%|█▌        | 7771/50000 [2:46:27<14:39:17,  1.25s/it] 16%|█▌        | 7772/50000 [2:46:28<14:37:23,  1.25s/it] 16%|█▌        | 7773/50000 [2:46:29<14:35:43,  1.24s/it] 16%|█▌        | 7774/50000 [2:46:30<14:34:49,  1.24s/it] 16%|█▌        | 7775/50000 [2:46:31<14:34:01,  1.24s/it] 16%|█▌        | 7776/50000 [2:46:33<14:33:43,  1.24s/it][2023-07-03 14:46:44,244] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 16%|█▌        | 7777/50000 [2:46:34<13:41:03,  1.17s/it] 16%|█▌        | 7778/50000 [2:46:35<13:56:37,  1.19s/it] 16%|█▌        | 7779/50000 [2:46:36<14:07:36,  1.20s/it] 16%|█▌        | 7780/50000 [2:46:37<14:15:22,  1.22s/it]                                                         {'loss': 110.7109, 'learning_rate': 1.9418625470887448e-05, 'epoch': 1.21}
 16%|█▌        | 7780/50000 [2:46:37<14:15:22,  1.22s/it] 16%|█▌        | 7781/50000 [2:46:39<14:20:30,  1.22s/it] 16%|█▌        | 7782/50000 [2:46:40<14:25:15,  1.23s/it] 16%|█▌        | 7783/50000 [2:46:41<14:28:31,  1.23s/it] 16%|█▌        | 7784/50000 [2:46:42<14:29:30,  1.24s/it] 16%|█▌        | 7785/50000 [2:46:44<14:31:26,  1.24s/it] 16%|█▌        | 7786/50000 [2:46:45<14:31:40,  1.24s/it] 16%|█▌        | 7787/50000 [2:46:46<14:31:55,  1.24s/it] 16%|█▌        | 7788/50000 [2:46:47<14:31:57,  1.24s/it] 16%|█▌        | 7789/50000 [2:46:49<14:31:47,  1.24s/it] 16%|█▌        | 7790/50000 [2:46:50<14:33:12,  1.24s/it]                                                         {'loss': 113.7391, 'learning_rate': 1.941640115801021e-05, 'epoch': 1.21}
 16%|█▌        | 7790/50000 [2:46:50<14:33:12,  1.24s/it] 16%|█▌        | 7791/50000 [2:46:51<14:34:58,  1.24s/it] 16%|█▌        | 7792/50000 [2:46:52<14:34:43,  1.24s/it] 16%|█▌        | 7793/50000 [2:46:54<14:33:48,  1.24s/it] 16%|█▌        | 7794/50000 [2:46:55<14:33:15,  1.24s/it] 16%|█▌        | 7795/50000 [2:46:56<14:33:32,  1.24s/it] 16%|█▌        | 7796/50000 [2:46:57<14:33:46,  1.24s/it] 16%|█▌        | 7797/50000 [2:46:59<14:34:45,  1.24s/it] 16%|█▌        | 7798/50000 [2:47:00<14:34:23,  1.24s/it] 16%|█▌        | 7799/50000 [2:47:01<14:35:14,  1.24s/it] 16%|█▌        | 7800/50000 [2:47:02<14:36:13,  1.25s/it]                                                         {'loss': 112.7336, 'learning_rate': 1.9414172726079195e-05, 'epoch': 1.21}
 16%|█▌        | 7800/50000 [2:47:02<14:36:13,  1.25s/it] 16%|█▌        | 7801/50000 [2:47:04<14:37:19,  1.25s/it] 16%|█▌        | 7802/50000 [2:47:05<14:37:30,  1.25s/it] 16%|█▌        | 7803/50000 [2:47:06<14:36:33,  1.25s/it] 16%|█▌        | 7804/50000 [2:47:07<14:35:05,  1.24s/it] 16%|█▌        | 7805/50000 [2:47:09<14:34:18,  1.24s/it] 16%|█▌        | 7806/50000 [2:47:10<14:33:40,  1.24s/it] 16%|█▌        | 7807/50000 [2:47:11<14:33:09,  1.24s/it] 16%|█▌        | 7808/50000 [2:47:12<14:32:35,  1.24s/it] 16%|█▌        | 7809/50000 [2:47:13<14:32:31,  1.24s/it] 16%|█▌        | 7810/50000 [2:47:15<14:32:22,  1.24s/it]                                                         {'loss': 113.0563, 'learning_rate': 1.941194017606919e-05, 'epoch': 1.21}
 16%|█▌        | 7810/50000 [2:47:15<14:32:22,  1.24s/it] 16%|█▌        | 7811/50000 [2:47:16<14:32:29,  1.24s/it] 16%|█▌        | 7812/50000 [2:47:17<14:32:21,  1.24s/it] 16%|█▌        | 7813/50000 [2:47:18<14:32:04,  1.24s/it] 16%|█▌        | 7814/50000 [2:47:20<14:33:06,  1.24s/it] 16%|█▌        | 7815/50000 [2:47:21<14:32:38,  1.24s/it] 16%|█▌        | 7816/50000 [2:47:22<14:32:22,  1.24s/it] 16%|█▌        | 7817/50000 [2:47:23<14:32:29,  1.24s/it] 16%|█▌        | 7818/50000 [2:47:25<14:33:02,  1.24s/it] 16%|█▌        | 7819/50000 [2:47:26<14:32:34,  1.24s/it] 16%|█▌        | 7820/50000 [2:47:27<14:52:25,  1.27s/it]                                                         {'loss': 113.6844, 'learning_rate': 1.940970350895679e-05, 'epoch': 1.21}
 16%|█▌        | 7820/50000 [2:47:27<14:52:25,  1.27s/it] 16%|█▌        | 7821/50000 [2:47:28<14:46:48,  1.26s/it] 16%|█▌        | 7822/50000 [2:47:30<14:42:32,  1.26s/it] 16%|█▌        | 7823/50000 [2:47:31<14:39:14,  1.25s/it] 16%|█▌        | 7824/50000 [2:47:32<14:36:48,  1.25s/it] 16%|█▌        | 7825/50000 [2:47:33<14:35:22,  1.25s/it] 16%|█▌        | 7826/50000 [2:47:35<14:35:14,  1.25s/it] 16%|█▌        | 7827/50000 [2:47:36<14:34:29,  1.24s/it] 16%|█▌        | 7828/50000 [2:47:37<14:33:41,  1.24s/it] 16%|█▌        | 7829/50000 [2:47:38<14:32:41,  1.24s/it] 16%|█▌        | 7830/50000 [2:47:40<14:32:09,  1.24s/it]                                                         {'loss': 111.8336, 'learning_rate': 1.9407462725720387e-05, 'epoch': 1.21}
 16%|█▌        | 7830/50000 [2:47:40<14:32:09,  1.24s/it] 16%|█▌        | 7831/50000 [2:47:41<14:32:30,  1.24s/it] 16%|█▌        | 7832/50000 [2:47:42<14:32:32,  1.24s/it] 16%|█▌        | 7833/50000 [2:47:43<14:32:04,  1.24s/it] 16%|█▌        | 7834/50000 [2:47:45<14:31:56,  1.24s/it] 16%|█▌        | 7835/50000 [2:47:46<14:32:22,  1.24s/it] 16%|█▌        | 7836/50000 [2:47:47<14:31:53,  1.24s/it] 16%|█▌        | 7837/50000 [2:47:48<14:31:38,  1.24s/it] 16%|█▌        | 7838/50000 [2:47:50<14:31:29,  1.24s/it] 16%|█▌        | 7839/50000 [2:47:51<14:31:42,  1.24s/it] 16%|█▌        | 7840/50000 [2:47:52<14:31:41,  1.24s/it]                                                         {'loss': 117.3859, 'learning_rate': 1.9405217827340178e-05, 'epoch': 1.22}
 16%|█▌        | 7840/50000 [2:47:52<14:31:41,  1.24s/it] 16%|█▌        | 7841/50000 [2:47:53<14:31:43,  1.24s/it] 16%|█▌        | 7842/50000 [2:47:55<14:31:50,  1.24s/it] 16%|█▌        | 7843/50000 [2:47:56<14:31:43,  1.24s/it] 16%|█▌        | 7844/50000 [2:47:57<14:31:43,  1.24s/it] 16%|█▌        | 7845/50000 [2:47:58<14:31:20,  1.24s/it] 16%|█▌        | 7846/50000 [2:48:00<14:31:17,  1.24s/it] 16%|█▌        | 7847/50000 [2:48:01<14:31:49,  1.24s/it] 16%|█▌        | 7848/50000 [2:48:02<14:31:43,  1.24s/it] 16%|█▌        | 7849/50000 [2:48:03<14:31:33,  1.24s/it] 16%|█▌        | 7850/50000 [2:48:04<14:31:31,  1.24s/it]                                                         {'loss': 112.9516, 'learning_rate': 1.9402968814798156e-05, 'epoch': 1.22}
 16%|█▌        | 7850/50000 [2:48:04<14:31:31,  1.24s/it] 16%|█▌        | 7851/50000 [2:48:06<14:31:37,  1.24s/it] 16%|█▌        | 7852/50000 [2:48:07<14:31:34,  1.24s/it] 16%|█▌        | 7853/50000 [2:48:08<14:31:23,  1.24s/it] 16%|█▌        | 7854/50000 [2:48:09<14:31:41,  1.24s/it] 16%|█▌        | 7855/50000 [2:48:11<14:31:46,  1.24s/it] 16%|█▌        | 7856/50000 [2:48:12<14:31:48,  1.24s/it] 16%|█▌        | 7857/50000 [2:48:13<14:48:20,  1.26s/it] 16%|█▌        | 7858/50000 [2:48:14<14:44:48,  1.26s/it] 16%|█▌        | 7859/50000 [2:48:16<14:41:48,  1.26s/it] 16%|█▌        | 7860/50000 [2:48:17<14:40:02,  1.25s/it]                                                         {'loss': 101.6469, 'learning_rate': 1.9400715689078118e-05, 'epoch': 1.22}
 16%|█▌        | 7860/50000 [2:48:17<14:40:02,  1.25s/it] 16%|█▌        | 7861/50000 [2:48:18<14:42:09,  1.26s/it] 16%|█▌        | 7862/50000 [2:48:19<14:42:26,  1.26s/it] 16%|█▌        | 7863/50000 [2:48:21<14:40:35,  1.25s/it] 16%|█▌        | 7864/50000 [2:48:22<14:39:05,  1.25s/it] 16%|█▌        | 7865/50000 [2:48:23<14:37:18,  1.25s/it] 16%|█▌        | 7866/50000 [2:48:24<14:36:36,  1.25s/it] 16%|█▌        | 7867/50000 [2:48:26<14:36:13,  1.25s/it] 16%|█▌        | 7868/50000 [2:48:27<14:34:43,  1.25s/it] 16%|█▌        | 7869/50000 [2:48:28<14:34:04,  1.24s/it] 16%|█▌        | 7870/50000 [2:48:29<14:33:16,  1.24s/it]                                                         {'loss': 105.4477, 'learning_rate': 1.9398458451165654e-05, 'epoch': 1.22}
 16%|█▌        | 7870/50000 [2:48:29<14:33:16,  1.24s/it] 16%|█▌        | 7871/50000 [2:48:31<14:33:17,  1.24s/it] 16%|█▌        | 7872/50000 [2:48:32<14:32:17,  1.24s/it] 16%|█▌        | 7873/50000 [2:48:33<14:32:25,  1.24s/it] 16%|█▌        | 7874/50000 [2:48:34<14:33:03,  1.24s/it] 16%|█▌        | 7875/50000 [2:48:36<14:32:08,  1.24s/it] 16%|█▌        | 7876/50000 [2:48:37<15:01:54,  1.28s/it] 16%|█▌        | 7877/50000 [2:48:38<14:55:18,  1.28s/it] 16%|█▌        | 7878/50000 [2:48:40<14:48:15,  1.27s/it] 16%|█▌        | 7879/50000 [2:48:41<14:43:49,  1.26s/it] 16%|█▌        | 7880/50000 [2:48:42<14:41:01,  1.26s/it]                                                         {'loss': 113.8664, 'learning_rate': 1.9396197102048162e-05, 'epoch': 1.22}
 16%|█▌        | 7880/50000 [2:48:42<14:41:01,  1.26s/it] 16%|█▌        | 7881/50000 [2:48:43<14:39:31,  1.25s/it] 16%|█▌        | 7882/50000 [2:48:45<14:37:18,  1.25s/it] 16%|█▌        | 7883/50000 [2:48:46<14:37:17,  1.25s/it] 16%|█▌        | 7884/50000 [2:48:47<14:37:28,  1.25s/it] 16%|█▌        | 7885/50000 [2:48:48<14:36:02,  1.25s/it] 16%|█▌        | 7886/50000 [2:48:50<14:35:32,  1.25s/it] 16%|█▌        | 7887/50000 [2:48:51<14:34:45,  1.25s/it] 16%|█▌        | 7888/50000 [2:48:52<14:35:20,  1.25s/it] 16%|█▌        | 7889/50000 [2:48:53<14:34:44,  1.25s/it] 16%|█▌        | 7890/50000 [2:48:54<14:34:36,  1.25s/it]                                                         {'loss': 113.0812, 'learning_rate': 1.9393931642714827e-05, 'epoch': 1.22}
 16%|█▌        | 7890/50000 [2:48:54<14:34:36,  1.25s/it] 16%|█▌        | 7891/50000 [2:48:56<14:34:23,  1.25s/it] 16%|█▌        | 7892/50000 [2:48:57<14:34:42,  1.25s/it] 16%|█▌        | 7893/50000 [2:48:58<14:35:24,  1.25s/it] 16%|█▌        | 7894/50000 [2:48:59<14:35:13,  1.25s/it] 16%|█▌        | 7895/50000 [2:49:01<14:34:08,  1.25s/it] 16%|█▌        | 7896/50000 [2:49:02<14:33:43,  1.25s/it] 16%|█▌        | 7897/50000 [2:49:03<14:32:54,  1.24s/it] 16%|█▌        | 7898/50000 [2:49:04<14:33:05,  1.24s/it] 16%|█▌        | 7899/50000 [2:49:06<14:32:20,  1.24s/it] 16%|█▌        | 7900/50000 [2:49:07<14:31:56,  1.24s/it]                                                         {'loss': 111.5312, 'learning_rate': 1.9391662074156646e-05, 'epoch': 1.22}
 16%|█▌        | 7900/50000 [2:49:07<14:31:56,  1.24s/it] 16%|█▌        | 7901/50000 [2:49:08<14:31:57,  1.24s/it] 16%|█▌        | 7902/50000 [2:49:09<14:31:36,  1.24s/it] 16%|█▌        | 7903/50000 [2:49:11<14:31:57,  1.24s/it] 16%|█▌        | 7904/50000 [2:49:12<14:31:09,  1.24s/it] 16%|█▌        | 7905/50000 [2:49:13<14:30:57,  1.24s/it] 16%|█▌        | 7906/50000 [2:49:14<14:30:42,  1.24s/it] 16%|█▌        | 7907/50000 [2:49:16<14:31:37,  1.24s/it] 16%|█▌        | 7908/50000 [2:49:17<14:31:20,  1.24s/it] 16%|█▌        | 7909/50000 [2:49:18<14:31:50,  1.24s/it] 16%|█▌        | 7910/50000 [2:49:19<14:31:19,  1.24s/it]                                                         {'loss': 114.7844, 'learning_rate': 1.9389388397366396e-05, 'epoch': 1.23}
 16%|█▌        | 7910/50000 [2:49:19<14:31:19,  1.24s/it] 16%|█▌        | 7911/50000 [2:49:21<14:31:08,  1.24s/it] 16%|█▌        | 7912/50000 [2:49:22<14:30:53,  1.24s/it] 16%|█▌        | 7913/50000 [2:49:23<14:31:02,  1.24s/it] 16%|█▌        | 7914/50000 [2:49:24<14:30:48,  1.24s/it] 16%|█▌        | 7915/50000 [2:49:26<14:30:30,  1.24s/it] 16%|█▌        | 7916/50000 [2:49:27<14:30:52,  1.24s/it] 16%|█▌        | 7917/50000 [2:49:28<14:30:23,  1.24s/it] 16%|█▌        | 7918/50000 [2:49:29<14:30:17,  1.24s/it] 16%|█▌        | 7919/50000 [2:49:31<14:30:19,  1.24s/it] 16%|█▌        | 7920/50000 [2:49:32<14:30:34,  1.24s/it]                                                         {'loss': 101.3406, 'learning_rate': 1.9387110613338668e-05, 'epoch': 1.23}
 16%|█▌        | 7920/50000 [2:49:32<14:30:34,  1.24s/it] 16%|█▌        | 7921/50000 [2:49:33<14:30:48,  1.24s/it] 16%|█▌        | 7922/50000 [2:49:34<14:30:39,  1.24s/it] 16%|█▌        | 7923/50000 [2:49:35<14:30:20,  1.24s/it] 16%|█▌        | 7924/50000 [2:49:37<14:30:31,  1.24s/it] 16%|█▌        | 7925/50000 [2:49:38<14:30:37,  1.24s/it] 16%|█▌        | 7926/50000 [2:49:39<14:30:12,  1.24s/it] 16%|█▌        | 7927/50000 [2:49:40<14:30:36,  1.24s/it] 16%|█▌        | 7928/50000 [2:49:42<14:30:47,  1.24s/it] 16%|█▌        | 7929/50000 [2:49:43<14:31:08,  1.24s/it] 16%|█▌        | 7930/50000 [2:49:44<14:30:55,  1.24s/it]                                                         {'loss': 122.3023, 'learning_rate': 1.9384828723069832e-05, 'epoch': 1.23}
 16%|█▌        | 7930/50000 [2:49:44<14:30:55,  1.24s/it] 16%|█▌        | 7931/50000 [2:49:45<14:30:38,  1.24s/it] 16%|█▌        | 7932/50000 [2:49:47<14:30:12,  1.24s/it] 16%|█▌        | 7933/50000 [2:49:48<14:29:55,  1.24s/it] 16%|█▌        | 7934/50000 [2:49:49<14:30:46,  1.24s/it] 16%|█▌        | 7935/50000 [2:49:50<14:30:33,  1.24s/it] 16%|█▌        | 7936/50000 [2:49:52<14:58:24,  1.28s/it] 16%|█▌        | 7937/50000 [2:49:53<15:01:53,  1.29s/it] 16%|█▌        | 7938/50000 [2:49:54<14:53:20,  1.27s/it] 16%|█▌        | 7939/50000 [2:49:56<15:12:36,  1.30s/it] 16%|█▌        | 7940/50000 [2:49:57<15:00:17,  1.28s/it]                                                         {'loss': 103.2695, 'learning_rate': 1.9382542727558077e-05, 'epoch': 1.23}
 16%|█▌        | 7940/50000 [2:49:57<15:00:17,  1.28s/it] 16%|█▌        | 7941/50000 [2:49:58<14:52:59,  1.27s/it] 16%|█▌        | 7942/50000 [2:49:59<14:45:40,  1.26s/it] 16%|█▌        | 7943/50000 [2:50:01<14:40:41,  1.26s/it] 16%|█▌        | 7944/50000 [2:50:02<14:37:44,  1.25s/it] 16%|█▌        | 7945/50000 [2:50:03<14:36:34,  1.25s/it] 16%|█▌        | 7946/50000 [2:50:04<14:34:32,  1.25s/it] 16%|█▌        | 7947/50000 [2:50:06<14:32:41,  1.25s/it] 16%|█▌        | 7948/50000 [2:50:07<14:31:48,  1.24s/it] 16%|█▌        | 7949/50000 [2:50:08<14:31:03,  1.24s/it] 16%|█▌        | 7950/50000 [2:50:09<14:30:10,  1.24s/it]                                                         {'loss': 111.6258, 'learning_rate': 1.938025262780337e-05, 'epoch': 1.23}
 16%|█▌        | 7950/50000 [2:50:09<14:30:10,  1.24s/it] 16%|█▌        | 7951/50000 [2:50:11<14:29:45,  1.24s/it] 16%|█▌        | 7952/50000 [2:50:12<14:29:15,  1.24s/it] 16%|█▌        | 7953/50000 [2:50:13<14:29:08,  1.24s/it] 16%|█▌        | 7954/50000 [2:50:14<14:28:45,  1.24s/it] 16%|█▌        | 7955/50000 [2:50:16<14:28:51,  1.24s/it] 16%|█▌        | 7956/50000 [2:50:17<14:28:36,  1.24s/it] 16%|█▌        | 7957/50000 [2:50:18<14:28:42,  1.24s/it] 16%|█▌        | 7958/50000 [2:50:19<14:28:40,  1.24s/it] 16%|█▌        | 7959/50000 [2:50:20<14:28:31,  1.24s/it] 16%|█▌        | 7960/50000 [2:50:22<14:28:44,  1.24s/it]                                                         {'loss': 112.1578, 'learning_rate': 1.9377958424807475e-05, 'epoch': 1.23}
 16%|█▌        | 7960/50000 [2:50:22<14:28:44,  1.24s/it] 16%|█▌        | 7961/50000 [2:50:23<14:28:47,  1.24s/it] 16%|█▌        | 7962/50000 [2:50:24<14:28:32,  1.24s/it] 16%|█▌        | 7963/50000 [2:50:25<14:28:15,  1.24s/it] 16%|█▌        | 7964/50000 [2:50:27<14:28:04,  1.24s/it] 16%|█▌        | 7965/50000 [2:50:28<14:27:56,  1.24s/it] 16%|█▌        | 7966/50000 [2:50:29<14:27:53,  1.24s/it] 16%|█▌        | 7967/50000 [2:50:30<14:28:00,  1.24s/it] 16%|█▌        | 7968/50000 [2:50:32<14:27:59,  1.24s/it] 16%|█▌        | 7969/50000 [2:50:33<14:28:00,  1.24s/it] 16%|█▌        | 7970/50000 [2:50:34<14:28:05,  1.24s/it]                                                         {'loss': 105.8008, 'learning_rate': 1.9375660119573956e-05, 'epoch': 1.24}
 16%|█▌        | 7970/50000 [2:50:34<14:28:05,  1.24s/it] 16%|█▌        | 7971/50000 [2:50:35<14:28:08,  1.24s/it] 16%|█▌        | 7972/50000 [2:50:37<14:28:07,  1.24s/it] 16%|█▌        | 7973/50000 [2:50:38<14:28:08,  1.24s/it] 16%|█▌        | 7974/50000 [2:50:39<14:27:55,  1.24s/it] 16%|█▌        | 7975/50000 [2:50:40<14:27:54,  1.24s/it] 16%|█▌        | 7976/50000 [2:50:42<14:27:53,  1.24s/it] 16%|█▌        | 7977/50000 [2:50:43<14:27:52,  1.24s/it] 16%|█▌        | 7978/50000 [2:50:44<14:27:48,  1.24s/it] 16%|█▌        | 7979/50000 [2:50:45<14:27:46,  1.24s/it] 16%|█▌        | 7980/50000 [2:50:47<14:27:42,  1.24s/it]                                                         {'loss': 114.9484, 'learning_rate': 1.9373357713108174e-05, 'epoch': 1.24}
 16%|█▌        | 7980/50000 [2:50:47<14:27:42,  1.24s/it] 16%|█▌        | 7981/50000 [2:50:48<14:28:08,  1.24s/it] 16%|█▌        | 7982/50000 [2:50:49<14:28:02,  1.24s/it] 16%|█▌        | 7983/50000 [2:50:50<14:28:10,  1.24s/it] 16%|█▌        | 7984/50000 [2:50:51<14:27:55,  1.24s/it] 16%|█▌        | 7985/50000 [2:50:53<14:27:51,  1.24s/it] 16%|█▌        | 7986/50000 [2:50:54<14:27:43,  1.24s/it] 16%|█▌        | 7987/50000 [2:50:55<14:27:49,  1.24s/it] 16%|█▌        | 7988/50000 [2:50:56<14:27:32,  1.24s/it] 16%|█▌        | 7989/50000 [2:50:58<14:27:37,  1.24s/it] 16%|█▌        | 7990/50000 [2:50:59<14:27:43,  1.24s/it]                                                         {'loss': 114.95, 'learning_rate': 1.9371051206417275e-05, 'epoch': 1.24}
 16%|█▌        | 7990/50000 [2:50:59<14:27:43,  1.24s/it] 16%|█▌        | 7991/50000 [2:51:00<14:28:01,  1.24s/it] 16%|█▌        | 7992/50000 [2:51:01<14:27:52,  1.24s/it] 16%|█▌        | 7993/50000 [2:51:03<14:27:52,  1.24s/it] 16%|█▌        | 7994/50000 [2:51:04<14:27:58,  1.24s/it] 16%|█▌        | 7995/50000 [2:51:05<14:27:55,  1.24s/it] 16%|█▌        | 7996/50000 [2:51:06<14:27:57,  1.24s/it] 16%|█▌        | 7997/50000 [2:51:08<14:27:43,  1.24s/it] 16%|█▌        | 7998/50000 [2:51:09<14:27:31,  1.24s/it] 16%|█▌        | 7999/50000 [2:51:10<14:27:38,  1.24s/it][2023-07-03 14:51:21,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=99, lr=[1.9368740600510204e-05], mom=[(0.9, 0.999)]
[2023-07-03 14:51:21,822] [INFO] [timer.py:199:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=3.248306888245429, CurrSamplesPerSec=3.2467730564461363, MemAllocated=47.21GB, MaxMemAllocated=59.8GB
 16%|█▌        | 8000/50000 [2:51:11<14:27:41,  1.24s/it]                                                         {'loss': 127.057, 'learning_rate': 1.9368740600510204e-05, 'epoch': 1.24}
 16%|█▌        | 8000/50000 [2:51:11<14:27:41,  1.24s/it][INFO|trainer.py:3129] 2023-07-03 14:51:21,828 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-07-03 14:51:21,828 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-07-03 14:51:21,828 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                         
                                             [A{'eval_loss': 142.125, 'eval_accuracy': 0.398840885142255, 'eval_runtime': 3.1033, 'eval_samples_per_second': 8.378, 'eval_steps_per_second': 2.256, 'epoch': 1.24}
 16%|█▌        | 8000/50000 [2:51:14<14:27:41,  1.24s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A[INFO|trainer.py:2868] 2023-07-03 14:51:24,933 >> Saving model checkpoint to /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000
[INFO|trainer.py:2880] 2023-07-03 14:51:24,948 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2171] 2023-07-03 14:51:35,493 >> tokenizer config file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2023-07-03 14:51:35,493 >> Special tokens file saved in /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/special_tokens_map.json
[2023-07-03 14:51:35,496] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8000 is about to be saved!
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-07-03 14:51:35,519] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/global_step8000/mp_rank_00_model_states.pt
[2023-07-03 14:51:35,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/global_step8000/mp_rank_00_model_states.pt...
[2023-07-03 14:51:46,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/global_step8000/mp_rank_00_model_states.pt.
[2023-07-03 14:51:46,441] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/global_step8000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-07-03 14:52:03,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/global_step8000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-07-03 14:52:03,839] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved /nvme/humu/llm-qat/llm-qat-try/checkpoint-8000/global_step8000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-07-03 14:52:03,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
[INFO|trainer.py:2953] 2023-07-03 14:52:03,856 >> Deleting older checkpoint [/nvme/humu/llm-qat/llm-qat-try/checkpoint-7000] due to args.save_total_limit
 16%|█▌        | 8001/50000 [2:52:04<194:00:34, 16.63s/it] 16%|█▌        | 8002/50000 [2:52:05<140:08:56, 12.01s/it] 16%|█▌        | 8003/50000 [2:52:06<102:28:15,  8.78s/it] 16%|█▌        | 8004/50000 [2:52:08<76:05:35,  6.52s/it]  16%|█▌        | 8005/50000 [2:52:09<57:38:20,  4.94s/it] 16%|█▌        | 8006/50000 [2:52:10<44:40:53,  3.83s/it] 16%|█▌        | 8007/50000 [2:52:11<35:37:07,  3.05s/it] 16%|█▌        | 8008/50000 [2:52:13<29:16:13,  2.51s/it] 16%|█▌        | 8009/50000 [2:52:14<24:50:33,  2.13s/it] 16%|█▌        | 8010/50000 [2:52:15<21:45:12,  1.87s/it]                                                         {'loss': 123.1875, 'learning_rate': 1.9366425896397702e-05, 'epoch': 1.24}
 16%|█▌        | 8010/50000 [2:52:15<21:45:12,  1.87s/it] 16%|█▌        | 8011/50000 [2:52:16<19:33:18,  1.68s/it] 16%|█▌        | 8012/50000 [2:52:18<18:00:55,  1.54s/it] 16%|█▌        | 8013/50000 [2:52:19<16:58:12,  1.46s/it] 16%|█▌        | 8014/50000 [2:52:20<16:15:50,  1.39s/it] 16%|█▌        | 8015/50000 [2:52:21<15:48:34,  1.36s/it] 16%|█▌        | 8016/50000 [2:52:23<15:26:34,  1.32s/it] 16%|█▌        | 8017/50000 [2:52:24<15:09:19,  1.30s/it] 16%|█▌        | 8018/50000 [2:52:25<14:57:37,  1.28s/it] 16%|█▌        | 8019/50000 [2:52:26<14:48:37,  1.27s/it] 16%|█▌        | 8020/50000 [2:52:27<14:42:04,  1.26s/it]                                                         {'loss': 111.4883, 'learning_rate': 1.9364107095092295e-05, 'epoch': 1.24}
 16%|█▌        | 8020/50000 [2:52:28<14:42:04,  1.26s/it] 16%|█▌        | 8021/50000 [2:52:29<14:38:18,  1.26s/it] 16%|█▌        | 8022/50000 [2:52:30<14:35:20,  1.25s/it] 16%|█▌        | 8023/50000 [2:52:31<14:33:01,  1.25s/it] 16%|█▌        | 8024/50000 [2:52:32<14:30:47,  1.24s/it] 16%|█▌        | 8025/50000 [2:52:34<14:29:46,  1.24s/it] 16%|█▌        | 8026/50000 [2:52:35<14:29:24,  1.24s/it] 16%|█▌        | 8027/50000 [2:52:36<14:28:27,  1.24s/it] 16%|█▌        | 8028/50000 [2:52:37<14:27:38,  1.24s/it] 16%|█▌        | 8029/50000 [2:52:39<14:27:09,  1.24s/it] 16%|█▌        | 8030/50000 [2:52:40<14:26:49,  1.24s/it]                                                         {'loss': 104.4305, 'learning_rate': 1.9361784197608305e-05, 'epoch': 1.24}
 16%|█▌        | 8030/50000 [2:52:40<14:26:49,  1.24s/it] 16%|█▌        | 8031/50000 [2:52:41<14:26:46,  1.24s/it] 16%|█▌        | 8032/50000 [2:52:42<14:26:08,  1.24s/it] 16%|█▌        | 8033/50000 [2:52:44<14:25:40,  1.24s/it] 16%|█▌        | 8034/50000 [2:52:45<14:25:35,  1.24s/it] 16%|█▌        | 8035/50000 [2:52:46<14:25:13,  1.24s/it] 16%|█▌        | 8036/50000 [2:52:47<14:25:26,  1.24s/it][2023-07-03 14:52:58,820] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, but hysteresis is 2. Reducing hysteresis to 1
 16%|█▌        | 8037/50000 [2:52:48<13:32:47,  1.16s/it] 16%|█▌        | 8038/50000 [2:52:50<13:48:10,  1.18s/it] 16%|█▌        | 8039/50000 [2:52:51<13:58:59,  1.20s/it] 16%|█▌        | 8040/50000 [2:52:52<14:07:09,  1.21s/it]                                                         {'loss': 94.5992, 'learning_rate': 1.935969008847981e-05, 'epoch': 1.25}
 16%|█▌        | 8040/50000 [2:52:52<14:07:09,  1.21s/it] 16%|█▌        | 8041/50000 [2:52:53<14:13:54,  1.22s/it] 16%|█▌        | 8042/50000 [2:52:54<14:17:59,  1.23s/it] 16%|█▌        | 8043/50000 [2:52:56<14:20:45,  1.23s/it] 16%|█▌        | 8044/50000 [2:52:57<14:22:22,  1.23s/it] 16%|█▌        | 8045/50000 [2:52:58<14:23:44,  1.24s/it] 16%|█▌        | 8046/50000 [2:52:59<14:24:29,  1.24s/it] 16%|█▌        | 8047/50000 [2:53:01<14:25:20,  1.24s/it] 16%|█▌        | 8048/50000 [2:53:02<14:31:43,  1.25s/it] 16%|█▌        | 8049/50000 [2:53:03<14:30:40,  1.25s/it] 16%|█▌        | 8050/50000 [2:53:04<14:29:21,  1.24s/it]                                                         {'loss': 113.4813, 'learning_rate': 1.9357359411057398e-05, 'epoch': 1.25}
 16%|█▌        | 8050/50000 [2:53:04<14:29:21,  1.24s/it] 16%|█▌        | 8051/50000 [2:53:06<14:28:54,  1.24s/it] 16%|█▌        | 8052/50000 [2:53:07<14:28:01,  1.24s/it] 16%|█▌        | 8053/50000 [2:53:08<14:27:41,  1.24s/it] 16%|█▌        | 8054/50000 [2:53:09<14:27:18,  1.24s/it] 16%|█▌        | 8055/50000 [2:53:11<14:26:50,  1.24s/it] 16%|█▌        | 8056/50000 [2:53:12<14:26:58,  1.24s/it] 16%|█▌        | 8057/50000 [2:53:13<14:26:27,  1.24s/it] 16%|█▌        | 8058/50000 [2:53:14<14:26:32,  1.24s/it] 16%|█▌        | 8059/50000 [2:53:16<14:26:34,  1.24s/it] 16%|█▌        | 8060/50000 [2:53:17<14:26:25,  1.24s/it]                                                         {'loss': 118.2578, 'learning_rate': 1.9355024640408068e-05, 'epoch': 1.25}
 16%|█▌        | 8060/50000 [2:53:17<14:26:25,  1.24s/it] 16%|█▌        | 8061/50000 [2:53:18<14:26:29,  1.24s/it] 16%|█▌        | 8062/50000 [2:53:19<14:26:21,  1.24s/it] 16%|█▌        | 8063/50000 [2:53:21<14:26:04,  1.24s/it] 16%|█▌        | 8064/50000 [2:53:22<14:26:04,  1.24s/it] 16%|█▌        | 8065/50000 [2:53:23<14:26:05,  1.24s/it] 16%|█▌        | 8066/50000 [2:53:24<14:26:11,  1.24s/it] 16%|█▌        | 8067/50000 [2:53:26<14:25:56,  1.24s/it] 16%|█▌        | 8068/50000 [2:53:27<14:25:47,  1.24s/it][2023-07-03 14:53:38,260] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
 16%|█▌        | 8069/50000 [2:53:28<13:33:49,  1.16s/it] 16%|█▌        | 8070/50000 [2:53:29<13:49:08,  1.19s/it]                                                         {'loss': 109.6773, 'learning_rate': 1.9352919847958735e-05, 'epoch': 1.25}
 16%|█▌        | 8070/50000 [2:53:29<13:49:08,  1.19s/it] 16%|█▌        | 8071/50000 [2:53:30<14:00:19,  1.20s/it] 16%|█▌        | 8072/50000 [2:53:31<14:07:48,  1.21s/it] 16%|█▌        | 8073/50000 [2:53:33<14:13:18,  1.22s/it] 16%|█▌        | 8074/50000 [2:53:34<14:17:00,  1.23s/it] 16%|█▌        | 8075/50000 [2:53:35<14:19:37,  1.23s/it] 16%|█▌        | 8076/50000 [2:53:36<14:21:21,  1.23s/it] 16%|█▌        | 8077/50000 [2:53:38<14:22:37,  1.23s/it] 16%|█▌        | 8078/50000 [2:53:39<14:23:36,  1.24s/it] 16%|█▌        | 8079/50000 [2:53:40<14:24:17,  1.24s/it] 16%|█▌        | 8080/50000 [2:53:41<14:24:58,  1.24s/it]                                                         {'loss': 107.0977, 'learning_rate': 1.9350577302993448e-05, 'epoch': 1.25}
 16%|█▌        | 8080/50000 [2:53:41<14:24:58,  1.24s/it] 16%|█▌        | 8081/50000 [2:53:43<14:25:18,  1.24s/it] 16%|█▌        | 8082/50000 [2:53:44<14:25:36,  1.24s/it] 16%|█▌        | 8083/50000 [2:53:45<14:25:35,  1.24s/it] 16%|█▌        | 8084/50000 [2:53:46<14:25:37,  1.24s/it] 16%|█▌        | 8085/50000 [2:53:48<14:25:45,  1.24s/it] 16%|█▌        | 8086/50000 [2:53:49<14:25:41,  1.24s/it] 16%|█▌        | 8087/50000 [2:53:50<14:25:42,  1.24s/it] 16%|█▌        | 8088/50000 [2:53:51<14:25:45,  1.24s/it] 16%|█▌        | 8089/50000 [2:53:53<14:25:37,  1.24s/it] 16%|█▌        | 8090/50000 [2:53:54<14:25:25,  1.24s/it]                                                         {'loss': 115.1609, 'learning_rate': 1.9348230667767973e-05, 'epoch': 1.25}
 16%|█▌        | 8090/50000 [2:53:54<14:25:25,  1.24s/it] 16%|█▌        | 8091/50000 [2:53:55<14:25:28,  1.24s/it] 16%|█▌        | 8092/50000 [2:53:56<14:25:16,  1.24s/it] 16%|█▌        | 8093/50000 [2:53:57<14:25:04,  1.24s/it] 16%|█▌        | 8094/50000 [2:53:59<14:25:04,  1.24s/it] 16%|█▌        | 8095/50000 [2:54:00<14:24:58,  1.24s/it] 16%|█▌        | 8096/50000 [2:54:01<14:24:52,  1.24s/it] 16%|█▌        | 8097/50000 [2:54:02<14:25:02,  1.24s/it] 16%|█▌        | 8098/50000 [2:54:04<14:24:57,  1.24s/it] 16%|█▌        | 8099/50000 [2:54:05<14:24:58,  1.24s/it] 16%|█▌        | 8100/50000 [2:54:06<14:24:57,  1.24s/it]                                                         {'loss': 97.6781, 'learning_rate': 1.9345879943308806e-05, 'epoch': 1.26}
 16%|█▌        | 8100/50000 [2:54:06<14:24:57,  1.24s/it] 16%|█▌        | 8101/50000 [2:54:07<14:25:35,  1.24s/it] 16%|█▌        | 8102/50000 [2:54:09<14:25:13,  1.24s/it] 16%|█▌        | 8103/50000 [2:54:10<14:25:10,  1.24s/it] 16%|█▌        | 8104/50000 [2:54:11<14:25:22,  1.24s/it] 16%|█▌        | 8105/50000 [2:54:12<14:25:16,  1.24s/it] 16%|█▌        | 8106/50000 [2:54:14<14:25:19,  1.24s/it] 16%|█▌        | 8107/50000 [2:54:15<14:25:11,  1.24s/it] 16%|█▌        | 8108/50000 [2:54:16<14:25:00,  1.24s/it] 16%|█▌        | 8109/50000 [2:54:17<14:24:50,  1.24s/it] 16%|█▌        | 8110/50000 [2:54:19<14:24:46,  1.24s/it]                                                         {'loss': 102.782, 'learning_rate': 1.9343525130644234e-05, 'epoch': 1.26}
 16%|█▌        | 8110/50000 [2:54:19<14:24:46,  1.24s/it] 16%|█▌        | 8111/50000 [2:54:20<14:24:49,  1.24s/it] 16%|█▌        | 8112/50000 [2:54:21<14:24:50,  1.24s/it] 16%|█▌        | 8113/50000 [2:54:22<14:24:44,  1.24s/it] 16%|█▌        | 8114/50000 [2:54:24<14:24:54,  1.24s/it] 16%|█▌        | 8115/50000 [2:54:25<14:24:45,  1.24s/it] 16%|█▌        | 8116/50000 [2:54:26<14:24:35,  1.24s/it] 16%|█▌        | 8117/50000 [2:54:27<14:24:24,  1.24s/it] 16%|█▌        | 8118/50000 [2:54:28<14:24:25,  1.24s/it] 16%|█▌        | 8119/50000 [2:54:30<14:24:13,  1.24s/it] 16%|█▌        | 8120/50000 [2:54:31<14:24:08,  1.24s/it]                                                         {'loss': 107.6516, 'learning_rate': 1.9341166230804332e-05, 'epoch': 1.26}
 16%|█▌        | 8120/50000 [2:54:31<14:24:08,  1.24s/it] 16%|█▌        | 8121/50000 [2:54:32<14:24:18,  1.24s/it] 16%|█▌        | 8122/50000 [2:54:33<14:24:16,  1.24s/it] 16%|█▌        | 8123/50000 [2:54:35<14:24:43,  1.24s/it] 16%|█▌        | 8124/50000 [2:54:36<14:24:21,  1.24s/it] 16%|█▋        | 8125/50000 [2:54:37<14:24:24,  1.24s/it] 16%|█▋        | 8126/50000 [2:54:38<14:24:22,  1.24s/it] 16%|█▋        | 8127/50000 [2:54:40<14:24:19,  1.24s/it] 16%|█▋        | 8128/50000 [2:54:41<14:24:33,  1.24s/it] 16%|█▋        | 8129/50000 [2:54:42<14:24:29,  1.24s/it] 16%|█▋        | 8130/50000 [2:54:43<14:24:27,  1.24s/it]                                                         {'loss': 118.6867, 'learning_rate': 1.9338803244820962e-05, 'epoch': 1.26}
 16%|█▋        | 8130/50000 [2:54:43<14:24:27,  1.24s/it] 16%|█▋        | 8131/50000 [2:54:45<14:24:45,  1.24s/it] 16%|█▋        | 8132/50000 [2:54:46<14:24:45,  1.24s/it] 16%|█▋        | 8133/50000 [2:54:47<14:24:32,  1.24s/it] 16%|█▋        | 8134/50000 [2:54:48<14:24:34,  1.24s/it] 16%|█▋        | 8135/50000 [2:54:50<14:24:34,  1.24s/it] 16%|█▋        | 8136/50000 [2:54:51<14:24:19,  1.24s/it] 16%|█▋        | 8137/50000 [2:54:52<14:24:04,  1.24s/it] 16%|█▋        | 8138/50000 [2:54:53<14:24:06,  1.24s/it] 16%|█▋        | 8139/50000 [2:54:54<14:24:00,  1.24s/it] 16%|█▋        | 8140/50000 [2:54:56<14:24:01,  1.24s/it]                                                         {'loss': 112.7453, 'learning_rate': 1.9336436173727774e-05, 'epoch': 1.26}
 16%|█▋        | 8140/50000 [2:54:56<14:24:01,  1.24s/it] 16%|█▋        | 8141/50000 [2:54:57<14:24:15,  1.24s/it] 16%|█▋        | 8142/50000 [2:54:58<14:24:06,  1.24s/it] 16%|█▋        | 8143/50000 [2:54:59<14:24:11,  1.24s/it] 16%|█▋        | 8144/50000 [2:55:01<14:24:11,  1.24s/it] 16%|█▋        | 8145/50000 [2:55:02<14:24:06,  1.24s/it] 16%|█▋        | 8146/50000 [2:55:03<14:23:50,  1.24s/it] 16%|█▋        | 8147/50000 [2:55:04<14:23:56,  1.24s/it] 16%|█▋        | 8148/50000 [2:55:06<14:23:59,  1.24s/it] 16%|█▋        | 8149/50000 [2:55:07<14:23:48,  1.24s/it] 16%|█▋        | 8150/50000 [2:55:08<14:23:53,  1.24s/it]                                                         {'loss': 106.0047, 'learning_rate': 1.933406501856021e-05, 'epoch': 1.26}
 16%|█▋        | 8150/50000 [2:55:08<14:23:53,  1.24s/it] 16%|█▋        | 8151/50000 [2:55:09<14:23:59,  1.24s/it] 16%|█▋        | 8152/50000 [2:55:11<14:24:05,  1.24s/it] 16%|█▋        | 8153/50000 [2:55:12<14:23:50,  1.24s/it] 16%|█▋        | 8154/50000 [2:55:13<14:24:12,  1.24s/it] 16%|█▋        | 8155/50000 [2:55:14<14:24:09,  1.24s/it] 16%|█▋        | 8156/50000 [2:55:16<14:24:03,  1.24s/it] 16%|█▋        | 8157/50000 [2:55:17<14:23:56,  1.24s/it] 16%|█▋        | 8158/50000 [2:55:18<14:23:47,  1.24s/it] 16%|█▋        | 8159/50000 [2:55:19<14:23:49,  1.24s/it] 16%|█▋        | 8160/50000 [2:55:20<14:24:00,  1.24s/it]                                                         {'loss': 118.8117, 'learning_rate': 1.9331689780355486e-05, 'epoch': 1.26}
 16%|█▋        | 8160/50000 [2:55:20<14:24:00,  1.24s/it] 16%|█▋        | 8161/50000 [2:55:22<14:24:11,  1.24s/it] 16%|█▋        | 8162/50000 [2:55:23<14:24:10,  1.24s/it] 16%|█▋        | 8163/50000 [2:55:24<14:27:39,  1.24s/it] 16%|█▋        | 8164/50000 [2:55:25<14:26:29,  1.24s/it] 16%|█▋        | 8165/50000 [2:55:27<14:25:45,  1.24s/it] 16%|█▋        | 8166/50000 [2:55:28<14:25:19,  1.24s/it] 16%|█▋        | 8167/50000 [2:55:29<14:25:08,  1.24s/it] 16%|█▋        | 8168/50000 [2:55:30<14:24:41,  1.24s/it] 16%|█▋        | 8169/50000 [2:55:32<14:24:31,  1.24s/it] 16%|█▋        | 8170/50000 [2:55:33<14:24:08,  1.24s/it]                                                         {'loss': 110.018, 'learning_rate': 1.932931046015262e-05, 'epoch': 1.27}
 16%|█▋        | 8170/50000 [2:55:33<14:24:08,  1.24s/it] 16%|█▋        | 8171/50000 [2:55:34<14:24:03,  1.24s/it]WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 102578 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 102581 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 102583 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 3 (pid: 102585) of binary: /home/humu/miniconda3/envs/cn_llama/bin/python
Traceback (most recent call last):
  File "/home/humu/miniconda3/envs/cn_llama/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
run_clm_pt_wo_peft.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-03_14:55:46
  host      : SH-IDC1-10-140-24-142
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 102585)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 102585
=======================================================
