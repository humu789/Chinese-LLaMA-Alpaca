WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-06-30 19:21:54,305] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
06/30/2023 19:21:55 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 718, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
UnboundLocalError: local variable 'os' referenced before assignment
06/30/2023 19:21:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 718, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
UnboundLocalError: local variable 'os' referenced before assignment
06/30/2023 19:21:55 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 718, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
UnboundLocalError: local variable 'os' referenced before assignment
06/30/2023 19:21:55 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 718, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
UnboundLocalError: local variable 'os' referenced before assignment
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 151318) of binary: /home/humu/miniconda3/envs/cn_llama/bin/python
Traceback (most recent call last):
  File "/home/humu/miniconda3/envs/cn_llama/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_clm_pt_wo_peft.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-06-30_19:21:59
  host      : SH-IDC1-10-140-24-142
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 151321)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-06-30_19:21:59
  host      : SH-IDC1-10-140-24-142
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 151322)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-06-30_19:21:59
  host      : SH-IDC1-10-140-24-142
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 151326)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-06-30_19:21:59
  host      : SH-IDC1-10-140-24-142
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 151318)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-06-30 19:27:12,671] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
06/30/2023 19:27:13 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 719, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    os.makedirs(training_args.output_dir, exist_ok=True)
UnboundLocalError: local variable 'os' referenced before assignment
06/30/2023 19:27:13 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 719, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    os.makedirs(training_args.output_dir, exist_ok=True)
UnboundLocalError: local variable 'os' referenced before assignment
06/30/2023 19:27:13 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 719, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    os.makedirs(training_args.output_dir, exist_ok=True)
UnboundLocalError: local variable 'os' referenced before assignment
06/30/2023 19:27:13 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
Traceback (most recent call last):
  File "run_clm_pt_wo_peft.py", line 719, in <module>
    main()
  File "run_clm_pt_wo_peft.py", line 370, in main
    os.makedirs(training_args.output_dir, exist_ok=True)
UnboundLocalError: local variable 'os' referenced before assignment
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 155635) of binary: /home/humu/miniconda3/envs/cn_llama/bin/python
Traceback (most recent call last):
  File "/home/humu/miniconda3/envs/cn_llama/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_clm_pt_wo_peft.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-06-30_19:27:17
  host      : SH-IDC1-10-140-24-142
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 155638)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-06-30_19:27:17
  host      : SH-IDC1-10-140-24-142
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 155639)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-06-30_19:27:17
  host      : SH-IDC1-10-140-24-142
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 155643)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-06-30_19:27:17
  host      : SH-IDC1-10-140-24-142
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 155635)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-06-30 19:28:18,713] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
06/30/2023 19:28:19 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
06/30/2023 19:28:19 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
06/30/2023 19:28:19 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
06/30/2023 19:28:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:666] 2023-06-30 19:28:19,317 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/config.json
[INFO|configuration_utils.py:720] 2023-06-30 19:28:19,318 >> Model config LlamaConfig {
  "_name_or_path": "/nvme/share_data/llama_ckpts/huggingface/7B",
  "architectures": [
    "LLaMAForCausalLM"
  ],
  "bos_token_id": 0,
  "eos_token_id": 1,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": -1,
  "rms_norm_eps": 1e-06,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:28:19,318 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:28:19,318 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:28:19,318 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:28:19,318 >> loading file tokenizer_config.json
06/30/2023 19:28:19 - INFO - __main__ - training datasets-llama7b_generate_data_0 has been loaded from disk
06/30/2023 19:28:19 - INFO - __main__ - training datasets-llama7b_generate_data_1 has been loaded from disk
06/30/2023 19:28:19 - INFO - __main__ - training datasets-llama7b_generate_data_3 has been loaded from disk
06/30/2023 19:28:19 - INFO - __main__ - training datasets-llama7b_generate_data_2 has been loaded from disk
06/30/2023 19:28:19 - INFO - __main__ - training datasets-llama7b_generate_data_5 has been loaded from disk
06/30/2023 19:28:19 - INFO - __main__ - training datasets-llama7b_generate_data_4 has been loaded from disk
06/30/2023 19:28:19 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-370cb95b72694ac4.arrow
06/30/2023 19:28:19 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-4275bd6dd2c7fc98.arrow
06/30/2023 19:28:21 - INFO - __main__ - Num train_samples  25802
06/30/2023 19:28:21 - INFO - __main__ - training example:
06/30/2023 19:28:21 - INFO - __main__ - Arizona, was SEC Chair of The Arizona Law Review and was named the 1991 alumnus of the year for the college of law. ⁇  Originally published in January, 2016, to commemorate Kramer & Randall's 50th Anniversary. ⁇  components of the system. ⁇  On the other hand, a virus, bug, worm, or trojan horse could pierce the organization's perimeter defenses and gain access to one or more of the organization's resources, causing destruction in such form. ⁇  Ignorance is the cause, insufficiency of relevant threat information is the source ⁇  A favorite potshotted buzzword of the cookpot's proprietor, Alfred A. Montapert (1884-1960). ⁇  Ignorance may destroy, but it cannot harm the owner of accumulated knowledge. ⁇  It is so that you see and hear only what the deafness of your own heart forbids you to know, or what the blindness of your own eyes prevents you to perceive. ⁇  Contribute to the healing of ignorance by improving your own status quo; improve your own knowledge and understanding; accumulate that which is unique, sole, and rare within you. ⁇  Personal wagile system: ⁇  It describes how a human being thinks, attempts to grow and progress, prospers, and flourishes. ⁇  It is always deeply concerned with identity, self-image, escape from existential threats and dangers, acceptance, meaning and telos, and continually attempts to realize something; avoidance, rejection, suppression, and denial of operational knowledge and experience is a fundamental source of human distress and suffering and is not duly taken into account in the construction and planning of most human institutions. ⁇  Feed the earthquake. ⁇  Take-away knowledge is a fact that cannot be understood with intellectual prowess incorporating artificial and manufactured principles dictated by the misfortune of dogmatic mentality. ⁇  Mindset moron: ⁇  A person or a system characterized by dogmatic and dumb and dogmatic thinking, and integration of it into daily life, where intuitive intelligence and overall context is not taken into consideration, or the use of knowledge is worse than it helps; even the existence and apparent occurrence of problems is not understood. ⁇  Systemic failure: ⁇  A
06/30/2023 19:28:21 - INFO - __main__ - Num eval_samples  26
06/30/2023 19:28:21 - INFO - __main__ - training example:
06/30/2023 19:28:21 - INFO - __main__ - toys, for example musical boxes and hand cranks, including small gears and levers, are valuable properties for not only developing the visual function, but also the auditory, emotional, and language development. ⁇  *Originally published September 2005 ⁇  data. ⁇  The stakeholders’ organisations’ pre-judging indicated 3 out of 9 respondents (Figure 4, right panel) expected a similar implementation process for phase two as that in phase one, although this area accounted for only 5% of phase one. This concerned implementation processes in mapping source code, building the MAKeU index, employing user data in the index rollout and costing the index. ⁇  4.3.2. Efficiency and user responsiveness characteristics of phases one and two ⁇  Four of the nine monitors assessed a 2 to 4+ times speed-up for phase two relative to phase one. The index rent roller and the curator assessed a 2 times and the registrar and court assessed a 4 times speedup, respectively. Based on the monitors’ assessments and regular concerns of the stakeholders, both planning and retrofitting was considered to result in an increase in efficiency due to a shorter lead time and a lower total cost, relative to the implementation processes associated with current systems. The index rent roller, the curator and the registrar explored advantages and disadvantages of this implementation approach and the combined marginal cost advantage was CUCS 310 000 or 55¢/month for the area concerned. ⁇  Estimates of overall efficiency ⁇  Four of the nine monitors assessed an overall material efficiency gain for both phases one and two, the rent roller, curator, the registrar and clerk assessed 5 to 20% cost saving relative to phase one. The monitor data is summarised in Figure 5. The registrar also identified benefits arising from lower maintenance and running costs and the monitor’s view of an ongoing, smaller moderate maintenance burden is explicated in the context of costs associated with service users in Section 4.5. Whilst writing of the cost savings has been associated with substantial sub-structural changes, the monitor view is that this is facilitated by lower resource and maintenance costs and not costly changes to the computational infrastructure. ⁇  Summative assessment of
[INFO|modeling_utils.py:2531] 2023-06-30 19:28:21,761 >> loading weights file /nvme/share_data/llama_ckpts/huggingface/7B/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-06-30 19:28:21,761 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:575] 2023-06-30 19:28:21,762 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": -1,
  "transformers_version": "4.28.1"
}

06/30/2023 19:28:21 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-370cb95b72694ac4.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-4275bd6dd2c7fc98.arrow
06/30/2023 19:28:21 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-370cb95b72694ac4.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-4275bd6dd2c7fc98.arrow
06/30/2023 19:28:21 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-370cb95b72694ac4.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-4275bd6dd2c7fc98.arrow
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  5.76it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:06,  5.10it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  5.73it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:06,  5.03it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.96it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.62it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.98it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.62it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:04,  6.02it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:04,  6.03it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.81it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.86it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  6.09it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  6.11it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  5.90it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  5.98it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.13it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.15it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  5.97it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.03it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:00<00:04,  6.16it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:00<00:04,  6.18it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:04,  6.00it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:04,  5.91it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.20it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.21it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.03it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.02it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.23it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.24it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.06it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.08it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.25it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.26it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.09it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.11it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.25it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.28it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.11it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.17it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.27it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.29it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.13it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.20it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.29it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.31it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.13it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.22it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.32it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.31it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.16it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.24it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:02,  6.34it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:02,  6.34it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  6.17it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  6.27it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.35it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.36it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.27it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:03,  5.66it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.39it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.40it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.33it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:03,  5.55it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.44it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.45it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.37it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.42it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.42it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  5.43it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.38it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.39it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.38it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.39it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:03<00:02,  5.36it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.37it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.38it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.40it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  5.20it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.38it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.38it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  5.26it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.43it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.42it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.44it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.44it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:02,  5.09it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.42it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.41it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.42it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:02,  5.10it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:03<00:01,  6.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:03<00:01,  6.42it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  6.43it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:04<00:01,  5.18it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.45it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.46it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.44it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:04<00:01,  5.23it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.47it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.48it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.45it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.45it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.45it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  5.22it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.44it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.42it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.43it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  5.21it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.41it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.41it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.43it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:01,  4.87it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:04<00:00,  6.47it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:04<00:00,  6.46it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:04<00:00,  6.44it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  6.45it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  6.45it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:05<00:01,  4.89it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  6.44it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.84it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.28it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.79it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.28it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.22it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.25it/s]
Loading checkpoint shards:  88%|████████▊ | 29/33 [00:05<00:00,  4.94it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:05<00:00,  5.01it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:05<00:00,  5.05it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  5.03it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:06<00:00,  4.57it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:06<00:00,  5.37it/s]
[INFO|modeling_utils.py:3190] 2023-06-30 19:28:28,023 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2023-06-30 19:28:28,023 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /nvme/share_data/llama_ckpts/huggingface/7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2023-06-30 19:28:28,026 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/generation_config.json
[INFO|configuration_utils.py:575] 2023-06-30 19:28:28,027 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinearConvert model.layers.7.self_attn.q_proj to QLinear

Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinearConvert model.layers.7.self_attn.k_proj to QLinear

Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinearConvert model.layers.8.self_attn.k_proj to QLinear

Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinearConvert model.layers.8.self_attn.v_proj to QLinear

Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinearConvert model.layers.8.self_attn.o_proj to QLinear

Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinearConvert model.layers.8.mlp.down_proj to QLinear

Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinearConvert model.layers.14.self_attn.q_proj to QLinear

Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinearConvert model.layers.14.self_attn.k_proj to QLinear

Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinearConvert model.layers.14.self_attn.v_proj to QLinear

Convert model.layers.14.self_attn.o_proj to QLinearConvert model.layers.14.self_attn.o_proj to QLinear

Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinearConvert model.layers.15.self_attn.q_proj to QLinear

Convert model.layers.15.self_attn.k_proj to QLinearConvert model.layers.15.self_attn.k_proj to QLinear

Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinearConvert model.layers.15.self_attn.v_proj to QLinear

Convert model.layers.15.self_attn.o_proj to QLinearConvert model.layers.15.self_attn.o_proj to QLinear

Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinearConvert model.layers.16.self_attn.q_proj to QLinear

Convert model.layers.16.self_attn.k_proj to QLinearConvert model.layers.16.self_attn.k_proj to QLinear

Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinearConvert model.layers.16.self_attn.v_proj to QLinear

Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinearConvert model.layers.16.self_attn.o_proj to QLinear

Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinearConvert model.layers.17.self_attn.q_proj to QLinear

Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinearConvert model.layers.17.self_attn.k_proj to QLinear

Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinearConvert model.layers.17.self_attn.v_proj to QLinear

Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinearConvert model.layers.17.self_attn.o_proj to QLinear

Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinearConvert model.layers.18.self_attn.q_proj to QLinear

Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinearConvert model.layers.18.self_attn.k_proj to QLinear

Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinearConvert model.layers.18.self_attn.v_proj to QLinear

Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinearConvert model.layers.18.self_attn.o_proj to QLinear

Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinearConvert model.layers.19.self_attn.q_proj to QLinear

Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinearConvert model.layers.19.self_attn.k_proj to QLinear

Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinearConvert model.layers.19.self_attn.v_proj to QLinear

Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinearConvert model.layers.19.self_attn.o_proj to QLinear

Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinearConvert model.layers.20.self_attn.q_proj to QLinear

Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinearConvert model.layers.20.self_attn.k_proj to QLinear

Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinearConvert model.layers.20.self_attn.v_proj to QLinear

Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinearConvert model.layers.20.self_attn.o_proj to QLinear

Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert lm_head to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert lm_head to QLinear
[INFO|trainer.py:564] 2023-06-30 19:29:03,228 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:621] 2023-06-30 19:29:03,228 >> Using cuda_amp half precision backend
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-06-30 19:29:05,164] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
06/30/2023 19:29:06 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
06/30/2023 19:29:07 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinearConvert model.layers.31.self_attn.v_proj to QLinear

Convert model.layers.31.self_attn.o_proj to QLinearConvert model.layers.31.self_attn.o_proj to QLinear

Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert lm_head to QLinear
Convert lm_head to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/30/2023 19:29:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
06/30/2023 19:29:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
06/30/2023 19:29:16 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
06/30/2023 19:29:16 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
06/30/2023 19:29:16 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
06/30/2023 19:29:16 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-06-30 19:29:17,191] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-06-30 19:29:17,192] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-06-30 19:29:17,192] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-06-30 19:29:17,205] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-06-30 19:29:17,205] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-06-30 19:29:17,205] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-06-30 19:29:17,205] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-06-30 19:29:17,205] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000
[2023-06-30 19:29:17,205] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000
[2023-06-30 19:29:17,206] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-06-30 19:29:17,206] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/humu/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2124793529510498 seconds
Loading extension module utils...
Time to load utils op: 0.20249152183532715 seconds
Loading extension module utils...
Time to load utils op: 0.20268678665161133 seconds
Loading extension module utils...
Time to load utils op: 0.20328807830810547 seconds
Rank: 0 partition count [4] and sizes[(1684603904, False)] 
Rank: 1 partition count [4] and sizes[(1684603904, False)] 
Rank: 2 partition count [4] and sizes[(1684603904, False)] 
Rank: 3 partition count [4] and sizes[(1684603904, False)] 
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005593299865722656 seconds
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005931854248046875 seconds
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0008895397186279297 seconds
[2023-06-30 19:29:30,823] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-06-30 19:29:30,824] [INFO] [utils.py:786:see_memory_usage] MA 31.47 GB         Max_MA 34.61 GB         CA 34.62 GB         Max_CA 35 GB 
[2023-06-30 19:29:30,825] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.25 GB, percent = 2.4%
[WARNING|logging.py:295] 2023-06-30 19:29:30,870 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-06-30 19:29:30,870 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-06-30 19:29:30,873 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-06-30 19:29:30,925] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-06-30 19:29:30,925] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 56.58 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-06-30 19:29:30,926] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.25 GB, percent = 2.4%
[2023-06-30 19:29:30,926] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-06-30 19:29:31,001] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-06-30 19:29:31,002] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 44.02 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-06-30 19:29:31,002] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.25 GB, percent = 2.4%
[2023-06-30 19:29:31,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-06-30 19:29:31,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-06-30 19:29:31,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f59a15bce50>
[2023-06-30 19:29:31,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-06-30 19:29:31,007] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
[2023-06-30 19:29:31,007] [INFO] [config.py:959:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-06-30 19:29:31,007] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-06-30 19:29:31,007] [INFO] [config.py:959:print]   amp_enabled .................. False
[2023-06-30 19:29:31,007] [INFO] [config.py:959:print]   amp_params ................... False
[2023-06-30 19:29:31,007] [INFO] [config.py:959:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f59a15bcc40>
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   communication_data_type ...... None
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   disable_allgather ............ False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   dump_state ................... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   elasticity_enabled ........... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   fp16_auto_cast ............... False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   fp16_enabled ................. True
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   global_rank .................. 0
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536
[2023-06-30 19:29:31,008] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   loss_scale ................... 0
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   memory_breakdown ............. False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   mics_shard_size .............. -1
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   optimizer_name ............... None
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   optimizer_params ............. None
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   pld_enabled .................. False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   pld_params ................... False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   prescale_gradients ........... False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   scheduler_name ............... None
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   scheduler_params ............. None
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   sparse_attention ............. None
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   steps_per_print .............. 2000
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   train_batch_size ............. 4
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   use_node_local_storage ....... False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   world_size ................... 4
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   zero_enabled ................. True
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
[2023-06-30 19:29:31,009] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2
[2023-06-30 19:29:31,009] [INFO] [config.py:945:print_user_config]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 100, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1e-10
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 1.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 4, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "zero_allow_untested_optimizer": true
}
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003132820129394531 seconds
[INFO|trainer.py:1769] 2023-06-30 19:29:31,011 >> ***** Running training *****
[INFO|trainer.py:1770] 2023-06-30 19:29:31,011 >>   Num examples = 25,802
[INFO|trainer.py:1771] 2023-06-30 19:29:31,011 >>   Num Epochs = 8
[INFO|trainer.py:1772] 2023-06-30 19:29:31,012 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1773] 2023-06-30 19:29:31,012 >>   Total train batch size (w. parallel, distributed & accumulation) = 4
[INFO|trainer.py:1774] 2023-06-30 19:29:31,012 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1775] 2023-06-30 19:29:31,012 >>   Total optimization steps = 50,000
[INFO|trainer.py:1776] 2023-06-30 19:29:31,014 >>   Number of trainable parameters = 6,738,415,616
  0%|          | 0/50000 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-06-30 19:29:31,025 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-06-30 19:29:32,673] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  0%|          | 1/50000 [00:01<23:02:53,  1.66s/it]                                                    {'loss': 4265.0, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/50000 [00:01<23:02:53,  1.66s/it][2023-06-30 19:29:33,673] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
  0%|          | 2/50000 [00:02<17:39:31,  1.27s/it][2023-06-30 19:29:34,667] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
  0%|          | 3/50000 [00:03<15:54:03,  1.14s/it][2023-06-30 19:29:35,656] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
  0%|          | 4/50000 [00:04<15:02:36,  1.08s/it][2023-06-30 19:29:36,646] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
  0%|          | 5/50000 [00:05<14:34:35,  1.05s/it][2023-06-30 19:29:37,633] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
  0%|          | 6/50000 [00:06<14:16:45,  1.03s/it][2023-06-30 19:29:38,621] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
  0%|          | 7/50000 [00:07<14:05:44,  1.02s/it][2023-06-30 19:29:39,604] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
  0%|          | 8/50000 [00:08<13:57:19,  1.00s/it][2023-06-30 19:29:40,588] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
  0%|          | 9/50000 [00:09<13:51:50,  1.00it/s][2023-06-30 19:29:41,571] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
  0%|          | 10/50000 [00:10<13:47:46,  1.01it/s]                                                     {'loss': 3558.75, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 10/50000 [00:10<13:47:46,  1.01it/s][INFO|trainer.py:3129] 2023-06-30 19:29:41,577 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:29:41,577 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:29:41,577 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.55it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                     
                                             [A{'eval_loss': 3520.0, 'eval_accuracy': 0.07609513773897336, 'eval_runtime': 3.1158, 'eval_samples_per_second': 8.345, 'eval_steps_per_second': 2.247, 'epoch': 0.0}
  0%|          | 10/50000 [00:13<13:47:46,  1.01it/s]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A[2023-06-30 19:29:45,674] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
  0%|          | 11/50000 [00:14<27:00:45,  1.95s/it][2023-06-30 19:29:46,655] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
  0%|          | 12/50000 [00:15<22:56:17,  1.65s/it][2023-06-30 19:29:47,651] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
  0%|          | 13/50000 [00:16<20:10:38,  1.45s/it][2023-06-30 19:29:48,647] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
  0%|          | 14/50000 [00:17<18:15:32,  1.32s/it][2023-06-30 19:29:49,644] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
  0%|          | 15/50000 [00:18<16:55:42,  1.22s/it][2023-06-30 19:29:50,641] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
  0%|          | 16/50000 [00:19<16:00:05,  1.15s/it][2023-06-30 19:29:51,639] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
  0%|          | 17/50000 [00:20<15:21:17,  1.11s/it][2023-06-30 19:29:52,636] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
  0%|          | 18/50000 [00:21<14:54:02,  1.07s/it][2023-06-30 19:29:53,629] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 19/50000 [00:22<14:34:02,  1.05s/it][2023-06-30 19:29:54,625] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 20/50000 [00:23<14:20:27,  1.03s/it]                                                     {'loss': 4056.75, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 20/50000 [00:23<14:20:27,  1.03s/it][INFO|trainer.py:3129] 2023-06-30 19:29:54,630 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:29:54,631 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:29:54,631 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                     
                                             [A{'eval_loss': 3520.0, 'eval_accuracy': 0.07609513773897336, 'eval_runtime': 3.1077, 'eval_samples_per_second': 8.366, 'eval_steps_per_second': 2.252, 'epoch': 0.0}
  0%|          | 20/50000 [00:26<14:20:27,  1.03s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A[2023-06-30 19:29:58,732] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 21/50000 [00:27<27:09:06,  1.96s/it]  0%|          | 22/50000 [00:28<24:10:58,  1.74s/it]  0%|          | 23/50000 [00:30<22:06:29,  1.59s/it]  0%|          | 24/50000 [00:31<20:39:56,  1.49s/it]  0%|          | 25/50000 [00:32<19:38:15,  1.41s/it]  0%|          | 26/50000 [00:33<18:54:59,  1.36s/it]  0%|          | 27/50000 [00:35<18:24:52,  1.33s/it]  0%|          | 28/50000 [00:36<18:03:25,  1.30s/it]  0%|          | 29/50000 [00:37<17:48:29,  1.28s/it]  0%|          | 30/50000 [00:38<17:38:21,  1.27s/it]                                                     {'loss': 3496.5, 'learning_rate': 7.200000000000001e-08, 'epoch': 0.0}
  0%|          | 30/50000 [00:38<17:38:21,  1.27s/it][INFO|trainer.py:3129] 2023-06-30 19:30:09,922 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:30:09,922 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:30:09,922 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                     
                                             [A{'eval_loss': 3534.0, 'eval_accuracy': 0.0764714737317477, 'eval_runtime': 3.1106, 'eval_samples_per_second': 8.358, 'eval_steps_per_second': 2.25, 'epoch': 0.0}
  0%|          | 30/50000 [00:42<17:38:21,  1.27s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A[2023-06-30 19:30:14,024] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 31/50000 [00:43<29:27:11,  2.12s/it]  0%|          | 32/50000 [00:44<25:46:51,  1.86s/it]  0%|          | 33/50000 [00:45<23:12:49,  1.67s/it]  0%|          | 34/50000 [00:46<21:24:51,  1.54s/it][2023-06-30 19:30:18,739] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 35/50000 [00:47<19:07:39,  1.38s/it]  0%|          | 36/50000 [00:48<18:33:32,  1.34s/it]  0%|          | 37/50000 [00:50<18:09:36,  1.31s/it]  0%|          | 38/50000 [00:51<17:52:27,  1.29s/it]  0%|          | 39/50000 [00:52<17:41:01,  1.27s/it]  0%|          | 40/50000 [00:53<17:32:31,  1.26s/it]                                                     {'loss': 3611.225, 'learning_rate': 1.36e-07, 'epoch': 0.01}
  0%|          | 40/50000 [00:53<17:32:31,  1.26s/it][INFO|trainer.py:3129] 2023-06-30 19:30:24,950 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:30:24,951 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:30:24,951 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                     
                                             [A{'eval_loss': 3508.0, 'eval_accuracy': 0.07511666415776005, 'eval_runtime': 3.1029, 'eval_samples_per_second': 8.379, 'eval_steps_per_second': 2.256, 'epoch': 0.01}
  0%|          | 40/50000 [00:57<17:32:31,  1.26s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 41/50000 [00:58<30:23:10,  2.19s/it]  0%|          | 42/50000 [00:59<26:26:07,  1.90s/it]  0%|          | 43/50000 [01:00<23:41:55,  1.71s/it]  0%|          | 44/50000 [01:02<21:45:23,  1.57s/it]  0%|          | 45/50000 [01:03<20:24:09,  1.47s/it]  0%|          | 46/50000 [01:04<19:27:03,  1.40s/it]  0%|          | 47/50000 [01:05<18:47:17,  1.35s/it]  0%|          | 48/50000 [01:06<18:18:47,  1.32s/it]  0%|          | 49/50000 [01:08<17:59:15,  1.30s/it]  0%|          | 50/50000 [01:09<17:45:26,  1.28s/it]                                                     {'loss': 3573.825, 'learning_rate': 2.1600000000000003e-07, 'epoch': 0.01}
  0%|          | 50/50000 [01:09<17:45:26,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:30:40,479 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:30:40,480 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:30:40,480 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                     
                                             [A{'eval_loss': 3360.0, 'eval_accuracy': 0.08008429926238145, 'eval_runtime': 3.1074, 'eval_samples_per_second': 8.367, 'eval_steps_per_second': 2.253, 'epoch': 0.01}
  0%|          | 50/50000 [01:12<17:45:26,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 51/50000 [01:13<30:33:04,  2.20s/it]  0%|          | 52/50000 [01:15<26:33:34,  1.91s/it]  0%|          | 53/50000 [01:16<23:45:17,  1.71s/it]  0%|          | 54/50000 [01:17<21:47:36,  1.57s/it]  0%|          | 55/50000 [01:18<20:25:22,  1.47s/it]  0%|          | 56/50000 [01:20<19:27:57,  1.40s/it]  0%|          | 57/50000 [01:21<18:47:53,  1.36s/it]  0%|          | 58/50000 [01:22<18:19:14,  1.32s/it]  0%|          | 59/50000 [01:23<17:59:34,  1.30s/it]  0%|          | 60/50000 [01:24<17:45:34,  1.28s/it]                                                     {'loss': 2964.025, 'learning_rate': 2.9600000000000006e-07, 'epoch': 0.01}
  0%|          | 60/50000 [01:24<17:45:34,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:30:56,008 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:30:56,008 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:30:56,008 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                     
                                             [A{'eval_loss': 3056.0, 'eval_accuracy': 0.0874604847207587, 'eval_runtime': 3.1075, 'eval_samples_per_second': 8.367, 'eval_steps_per_second': 2.253, 'epoch': 0.01}
  0%|          | 60/50000 [01:28<17:45:34,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 61/50000 [01:29<30:32:45,  2.20s/it]  0%|          | 62/50000 [01:30<26:33:00,  1.91s/it]  0%|          | 63/50000 [01:31<23:45:01,  1.71s/it]  0%|          | 64/50000 [01:33<21:47:10,  1.57s/it][2023-06-30 19:31:05,072] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 0, reducing to 0
  0%|          | 65/50000 [01:34<19:23:08,  1.40s/it]  0%|          | 66/50000 [01:35<18:43:28,  1.35s/it]  0%|          | 67/50000 [01:36<18:16:18,  1.32s/it]  0%|          | 68/50000 [01:37<17:56:59,  1.29s/it]  0%|          | 69/50000 [01:39<17:44:00,  1.28s/it]  0%|          | 70/50000 [01:40<17:34:28,  1.27s/it]                                                     {'loss': 3425.7, 'learning_rate': 3.68e-07, 'epoch': 0.01}
  0%|          | 70/50000 [01:40<17:34:28,  1.27s/it][INFO|trainer.py:3129] 2023-06-30 19:31:11,281 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:31:11,281 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:31:11,281 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.50it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                     
                                             [A{'eval_loss': 2802.0, 'eval_accuracy': 0.09769682372422099, 'eval_runtime': 3.1076, 'eval_samples_per_second': 8.367, 'eval_steps_per_second': 2.253, 'epoch': 0.01}
  0%|          | 70/50000 [01:43<17:34:28,  1.27s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 71/50000 [01:44<30:24:45,  2.19s/it]  0%|          | 72/50000 [01:45<26:27:03,  1.91s/it]  0%|          | 73/50000 [01:47<23:40:26,  1.71s/it]  0%|          | 74/50000 [01:48<21:44:43,  1.57s/it]  0%|          | 75/50000 [01:49<20:23:47,  1.47s/it]  0%|          | 76/50000 [01:50<19:26:36,  1.40s/it]  0%|          | 77/50000 [01:52<18:46:01,  1.35s/it]  0%|          | 78/50000 [01:53<18:17:45,  1.32s/it]  0%|          | 79/50000 [01:54<17:57:55,  1.30s/it]  0%|          | 80/50000 [01:55<17:44:14,  1.28s/it]                                                     {'loss': 2783.425, 'learning_rate': 4.4800000000000004e-07, 'epoch': 0.01}
  0%|          | 80/50000 [01:55<17:44:14,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:31:26,804 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:31:26,804 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:31:26,804 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                     
                                             [A{'eval_loss': 2330.0, 'eval_accuracy': 0.10778262833057353, 'eval_runtime': 3.1024, 'eval_samples_per_second': 8.381, 'eval_steps_per_second': 2.256, 'epoch': 0.01}
  0%|          | 80/50000 [01:58<17:44:14,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 81/50000 [02:00<30:29:44,  2.20s/it]  0%|          | 82/50000 [02:01<26:30:16,  1.91s/it]  0%|          | 83/50000 [02:02<23:42:34,  1.71s/it]  0%|          | 84/50000 [02:03<21:46:46,  1.57s/it]  0%|          | 85/50000 [02:05<20:27:15,  1.48s/it]  0%|          | 86/50000 [02:06<19:31:03,  1.41s/it]  0%|          | 87/50000 [02:07<18:50:51,  1.36s/it]  0%|          | 88/50000 [02:08<18:22:17,  1.33s/it]  0%|          | 89/50000 [02:10<18:01:23,  1.30s/it]  0%|          | 90/50000 [02:11<17:46:47,  1.28s/it]                                                     {'loss': 2152.275, 'learning_rate': 5.280000000000001e-07, 'epoch': 0.01}
  0%|          | 90/50000 [02:11<17:46:47,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:31:42,353 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:31:42,353 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:31:42,353 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                     
                                             [A{'eval_loss': 1915.0, 'eval_accuracy': 0.1332982086406744, 'eval_runtime': 3.1039, 'eval_samples_per_second': 8.377, 'eval_steps_per_second': 2.255, 'epoch': 0.01}
  0%|          | 90/50000 [02:14<17:46:47,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 91/50000 [02:15<30:32:44,  2.20s/it]  0%|          | 92/50000 [02:16<26:32:52,  1.91s/it]  0%|          | 93/50000 [02:18<23:44:54,  1.71s/it]  0%|          | 94/50000 [02:19<21:47:49,  1.57s/it]  0%|          | 95/50000 [02:20<20:26:18,  1.47s/it]  0%|          | 96/50000 [02:21<19:30:05,  1.41s/it]  0%|          | 97/50000 [02:23<18:49:25,  1.36s/it]  0%|          | 98/50000 [02:24<18:21:31,  1.32s/it]  0%|          | 99/50000 [02:25<18:01:53,  1.30s/it]  0%|          | 100/50000 [02:26<17:47:47,  1.28s/it]                                                      {'loss': 1678.6625, 'learning_rate': 6.08e-07, 'epoch': 0.02}
  0%|          | 100/50000 [02:26<17:47:47,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:31:57,909 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:31:57,909 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:31:57,909 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.46it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.15it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.74it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.55it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.44it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 1578.0, 'eval_accuracy': 0.16062020171609212, 'eval_runtime': 3.1244, 'eval_samples_per_second': 8.322, 'eval_steps_per_second': 2.24, 'epoch': 0.02}
  0%|          | 100/50000 [02:30<17:47:47,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  0%|          | 101/50000 [02:31<30:38:44,  2.21s/it]  0%|          | 102/50000 [02:32<26:37:33,  1.92s/it]  0%|          | 103/50000 [02:33<23:48:34,  1.72s/it]  0%|          | 104/50000 [02:34<21:50:19,  1.58s/it]  0%|          | 105/50000 [02:36<20:27:27,  1.48s/it]  0%|          | 106/50000 [02:37<19:30:22,  1.41s/it]  0%|          | 107/50000 [02:38<18:50:13,  1.36s/it]  0%|          | 108/50000 [02:39<18:22:23,  1.33s/it]  0%|          | 109/50000 [02:41<18:02:19,  1.30s/it]  0%|          | 110/50000 [02:42<17:47:57,  1.28s/it]                                                      {'loss': 1685.35, 'learning_rate': 6.88e-07, 'epoch': 0.02}
  0%|          | 110/50000 [02:42<17:47:57,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:32:13,490 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:32:13,490 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:32:13,490 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 1432.0, 'eval_accuracy': 0.18192081890712028, 'eval_runtime': 3.113, 'eval_samples_per_second': 8.352, 'eval_steps_per_second': 2.249, 'epoch': 0.02}
  0%|          | 110/50000 [02:45<17:47:57,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  0%|          | 111/50000 [02:46<30:35:49,  2.21s/it]  0%|          | 112/50000 [02:48<26:36:04,  1.92s/it]  0%|          | 113/50000 [02:49<23:47:58,  1.72s/it]  0%|          | 114/50000 [02:50<21:50:53,  1.58s/it]  0%|          | 115/50000 [02:51<20:29:02,  1.48s/it]  0%|          | 116/50000 [02:53<19:32:11,  1.41s/it]  0%|          | 117/50000 [02:54<18:52:15,  1.36s/it]  0%|          | 118/50000 [02:55<18:26:08,  1.33s/it]  0%|          | 119/50000 [02:56<18:06:30,  1.31s/it]  0%|          | 120/50000 [02:58<17:53:08,  1.29s/it]                                                      {'loss': 1429.2375, 'learning_rate': 7.68e-07, 'epoch': 0.02}
  0%|          | 120/50000 [02:58<17:53:08,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:32:29,106 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:32:29,106 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:32:29,106 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.55it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 1262.0, 'eval_accuracy': 0.19486677705855787, 'eval_runtime': 3.122, 'eval_samples_per_second': 8.328, 'eval_steps_per_second': 2.242, 'epoch': 0.02}
  0%|          | 120/50000 [03:01<17:53:08,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  0%|          | 121/50000 [03:02<30:41:51,  2.22s/it]  0%|          | 122/50000 [03:03<26:40:32,  1.93s/it]  0%|          | 123/50000 [03:04<23:50:36,  1.72s/it]  0%|          | 124/50000 [03:06<21:52:23,  1.58s/it]  0%|          | 125/50000 [03:07<20:29:04,  1.48s/it]  0%|          | 126/50000 [03:08<19:30:26,  1.41s/it]  0%|          | 127/50000 [03:09<18:50:01,  1.36s/it]  0%|          | 128/50000 [03:11<18:21:27,  1.33s/it]  0%|          | 129/50000 [03:12<18:01:52,  1.30s/it]  0%|          | 130/50000 [03:13<17:47:48,  1.28s/it]                                                      {'loss': 1126.85, 'learning_rate': 8.480000000000001e-07, 'epoch': 0.02}
  0%|          | 130/50000 [03:13<17:47:48,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:32:44,691 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:32:44,691 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:32:44,691 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 1141.0, 'eval_accuracy': 0.21217823272617795, 'eval_runtime': 3.1101, 'eval_samples_per_second': 8.36, 'eval_steps_per_second': 2.251, 'epoch': 0.02}
  0%|          | 130/50000 [03:16<17:47:48,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 131/50000 [03:18<30:34:43,  2.21s/it]  0%|          | 132/50000 [03:19<26:34:38,  1.92s/it]  0%|          | 133/50000 [03:20<23:46:10,  1.72s/it]  0%|          | 134/50000 [03:21<21:48:25,  1.57s/it]  0%|          | 135/50000 [03:23<20:26:12,  1.48s/it]  0%|          | 136/50000 [03:24<19:29:13,  1.41s/it]  0%|          | 137/50000 [03:25<18:48:40,  1.36s/it]  0%|          | 138/50000 [03:26<18:20:42,  1.32s/it]  0%|          | 139/50000 [03:27<18:00:54,  1.30s/it]  0%|          | 140/50000 [03:29<17:47:01,  1.28s/it]                                                      {'loss': 1074.325, 'learning_rate': 9.28e-07, 'epoch': 0.02}
  0%|          | 140/50000 [03:29<17:47:01,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:33:00,254 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:33:00,254 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:33:00,254 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.48it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 969.0, 'eval_accuracy': 0.2299412915851272, 'eval_runtime': 3.1113, 'eval_samples_per_second': 8.357, 'eval_steps_per_second': 2.25, 'epoch': 0.02}
  0%|          | 140/50000 [03:32<17:47:01,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  0%|          | 141/50000 [03:33<30:34:11,  2.21s/it]  0%|          | 142/50000 [03:34<26:33:43,  1.92s/it]  0%|          | 143/50000 [03:36<23:45:43,  1.72s/it]  0%|          | 144/50000 [03:37<21:48:03,  1.57s/it]  0%|          | 145/50000 [03:38<20:26:22,  1.48s/it]  0%|          | 146/50000 [03:39<19:28:17,  1.41s/it]  0%|          | 147/50000 [03:41<18:48:09,  1.36s/it]  0%|          | 148/50000 [03:42<18:20:16,  1.32s/it]  0%|          | 149/50000 [03:43<18:00:42,  1.30s/it]  0%|          | 150/50000 [03:44<17:47:04,  1.28s/it]                                                      {'loss': 812.475, 'learning_rate': 1.0080000000000001e-06, 'epoch': 0.02}
  0%|          | 150/50000 [03:44<17:47:04,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:33:15,819 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:33:15,819 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:33:15,819 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 854.0, 'eval_accuracy': 0.24236037934668073, 'eval_runtime': 3.1028, 'eval_samples_per_second': 8.379, 'eval_steps_per_second': 2.256, 'epoch': 0.02}
  0%|          | 150/50000 [03:47<17:47:04,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 151/50000 [03:49<30:31:22,  2.20s/it]  0%|          | 152/50000 [03:50<26:33:11,  1.92s/it]  0%|          | 153/50000 [03:51<23:45:25,  1.72s/it]  0%|          | 154/50000 [03:52<21:48:02,  1.57s/it]  0%|          | 155/50000 [03:54<20:26:06,  1.48s/it]  0%|          | 156/50000 [03:55<19:29:28,  1.41s/it]  0%|          | 157/50000 [03:56<18:50:23,  1.36s/it]  0%|          | 158/50000 [03:57<18:23:47,  1.33s/it]  0%|          | 159/50000 [03:59<18:05:02,  1.31s/it]  0%|          | 160/50000 [04:00<17:53:43,  1.29s/it]                                                      {'loss': 838.5063, 'learning_rate': 1.088e-06, 'epoch': 0.02}
  0%|          | 160/50000 [04:00<17:53:43,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:33:31,424 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:33:31,424 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:33:31,424 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.44it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.16it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.74it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.54it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A                                                      
                                             [A{'eval_loss': 764.0, 'eval_accuracy': 0.25620954388077677, 'eval_runtime': 3.1419, 'eval_samples_per_second': 8.275, 'eval_steps_per_second': 2.228, 'epoch': 0.02}
  0%|          | 160/50000 [04:03<17:53:43,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A
                                             [A  0%|          | 161/50000 [04:04<30:47:29,  2.22s/it]  0%|          | 162/50000 [04:06<26:44:13,  1.93s/it]  0%|          | 163/50000 [04:07<23:54:03,  1.73s/it]  0%|          | 164/50000 [04:08<21:54:35,  1.58s/it]  0%|          | 165/50000 [04:09<20:31:15,  1.48s/it]  0%|          | 166/50000 [04:11<19:33:07,  1.41s/it]  0%|          | 167/50000 [04:12<18:52:03,  1.36s/it]  0%|          | 168/50000 [04:13<18:23:39,  1.33s/it]  0%|          | 169/50000 [04:14<18:02:35,  1.30s/it]  0%|          | 170/50000 [04:16<17:48:54,  1.29s/it]                                                      {'loss': 796.1125, 'learning_rate': 1.168e-06, 'epoch': 0.03}
  0%|          | 170/50000 [04:16<17:48:54,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:33:47,051 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:33:47,051 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:33:47,051 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 701.0, 'eval_accuracy': 0.2652416077073611, 'eval_runtime': 3.1144, 'eval_samples_per_second': 8.348, 'eval_steps_per_second': 2.248, 'epoch': 0.03}
  0%|          | 170/50000 [04:19<17:48:54,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 171/50000 [04:20<30:35:09,  2.21s/it]  0%|          | 172/50000 [04:21<26:35:06,  1.92s/it]  0%|          | 173/50000 [04:22<23:46:45,  1.72s/it]  0%|          | 174/50000 [04:24<21:48:56,  1.58s/it]  0%|          | 175/50000 [04:25<20:25:52,  1.48s/it]  0%|          | 176/50000 [04:26<19:27:42,  1.41s/it]  0%|          | 177/50000 [04:27<18:47:08,  1.36s/it]  0%|          | 178/50000 [04:29<18:19:02,  1.32s/it]  0%|          | 179/50000 [04:30<17:59:03,  1.30s/it]  0%|          | 180/50000 [04:31<17:45:20,  1.28s/it]                                                      {'loss': 785.1625, 'learning_rate': 1.248e-06, 'epoch': 0.03}
  0%|          | 180/50000 [04:31<17:45:20,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:34:02,612 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:34:02,612 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:34:02,612 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 645.5, 'eval_accuracy': 0.27299412915851273, 'eval_runtime': 3.1079, 'eval_samples_per_second': 8.366, 'eval_steps_per_second': 2.252, 'epoch': 0.03}
  0%|          | 180/50000 [04:34<17:45:20,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 181/50000 [04:35<30:30:39,  2.20s/it]  0%|          | 182/50000 [04:37<26:30:59,  1.92s/it]  0%|          | 183/50000 [04:38<23:43:15,  1.71s/it]  0%|          | 184/50000 [04:39<21:45:58,  1.57s/it]  0%|          | 185/50000 [04:40<20:24:05,  1.47s/it]  0%|          | 186/50000 [04:42<19:26:35,  1.41s/it]  0%|          | 187/50000 [04:43<18:46:04,  1.36s/it]  0%|          | 188/50000 [04:44<18:18:05,  1.32s/it]  0%|          | 189/50000 [04:45<17:58:53,  1.30s/it]  0%|          | 190/50000 [04:47<17:45:37,  1.28s/it]                                                      {'loss': 609.0562, 'learning_rate': 1.328e-06, 'epoch': 0.03}
  0%|          | 190/50000 [04:47<17:45:37,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:34:18,164 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:34:18,165 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:34:18,165 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.47it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.16it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.74it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.55it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.44it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A                                                      
                                             [A{'eval_loss': 605.0, 'eval_accuracy': 0.2830799337648653, 'eval_runtime': 3.1271, 'eval_samples_per_second': 8.314, 'eval_steps_per_second': 2.239, 'epoch': 0.03}
  0%|          | 190/50000 [04:50<17:45:37,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A
                                             [A  0%|          | 191/50000 [04:51<30:36:07,  2.21s/it]  0%|          | 192/50000 [04:52<26:34:57,  1.92s/it]  0%|          | 193/50000 [04:54<23:45:57,  1.72s/it]  0%|          | 194/50000 [04:55<21:47:32,  1.58s/it]  0%|          | 195/50000 [04:56<20:24:58,  1.48s/it]  0%|          | 196/50000 [04:57<19:27:04,  1.41s/it]  0%|          | 197/50000 [04:58<18:46:40,  1.36s/it]  0%|          | 198/50000 [05:00<18:18:00,  1.32s/it]  0%|          | 199/50000 [05:01<17:58:21,  1.30s/it]  0%|          | 200/50000 [05:02<17:43:57,  1.28s/it]                                                      {'loss': 627.8687, 'learning_rate': 1.4080000000000001e-06, 'epoch': 0.03}
  0%|          | 200/50000 [05:02<17:43:57,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:34:33,728 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:34:33,728 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:34:33,728 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 573.0, 'eval_accuracy': 0.28812283606804157, 'eval_runtime': 3.1081, 'eval_samples_per_second': 8.365, 'eval_steps_per_second': 2.252, 'epoch': 0.03}
  0%|          | 200/50000 [05:05<17:43:57,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  0%|          | 201/50000 [05:07<30:29:35,  2.20s/it]  0%|          | 202/50000 [05:08<26:29:48,  1.92s/it]  0%|          | 203/50000 [05:09<23:42:16,  1.71s/it]  0%|          | 204/50000 [05:10<21:45:01,  1.57s/it]  0%|          | 205/50000 [05:12<20:22:46,  1.47s/it]  0%|          | 206/50000 [05:13<19:25:09,  1.40s/it]  0%|          | 207/50000 [05:14<18:45:00,  1.36s/it]  0%|          | 208/50000 [05:15<18:17:11,  1.32s/it]  0%|          | 209/50000 [05:17<17:58:17,  1.30s/it]  0%|          | 210/50000 [05:18<17:43:52,  1.28s/it]                                                      {'loss': 581.8375, 'learning_rate': 1.488e-06, 'epoch': 0.03}
  0%|          | 210/50000 [05:18<17:43:52,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:34:49,272 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:34:49,272 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:34:49,272 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.50it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.55it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.44it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 546.0, 'eval_accuracy': 0.29068192081890715, 'eval_runtime': 3.1215, 'eval_samples_per_second': 8.329, 'eval_steps_per_second': 2.243, 'epoch': 0.03}
  0%|          | 210/50000 [05:21<17:43:52,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  0%|          | 211/50000 [05:22<30:33:02,  2.21s/it]  0%|          | 212/50000 [05:23<26:33:48,  1.92s/it]  0%|          | 213/50000 [05:25<23:47:19,  1.72s/it]  0%|          | 214/50000 [05:26<21:51:12,  1.58s/it]  0%|          | 215/50000 [05:27<20:29:48,  1.48s/it]  0%|          | 216/50000 [05:28<19:31:47,  1.41s/it]  0%|          | 217/50000 [05:30<18:52:05,  1.36s/it]  0%|          | 218/50000 [05:31<18:24:33,  1.33s/it]  0%|          | 219/50000 [05:32<18:03:55,  1.31s/it]  0%|          | 220/50000 [05:33<17:48:46,  1.29s/it]                                                      {'loss': 568.7344, 'learning_rate': 1.568e-06, 'epoch': 0.03}
  0%|          | 220/50000 [05:33<17:48:46,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:35:04,901 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:35:04,901 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:35:04,901 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 519.0, 'eval_accuracy': 0.2939936775553214, 'eval_runtime': 3.1127, 'eval_samples_per_second': 8.353, 'eval_steps_per_second': 2.249, 'epoch': 0.03}
  0%|          | 220/50000 [05:36<17:48:46,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  0%|          | 221/50000 [05:38<30:34:30,  2.21s/it]  0%|          | 222/50000 [05:39<26:33:39,  1.92s/it]  0%|          | 223/50000 [05:40<23:45:02,  1.72s/it]  0%|          | 224/50000 [05:41<21:47:51,  1.58s/it]  0%|          | 225/50000 [05:43<20:24:46,  1.48s/it]  0%|          | 226/50000 [05:44<19:26:30,  1.41s/it]  0%|          | 227/50000 [05:45<18:45:13,  1.36s/it]  0%|          | 228/50000 [05:46<18:16:38,  1.32s/it]  0%|          | 229/50000 [05:48<17:56:53,  1.30s/it]  0%|          | 230/50000 [05:49<17:43:03,  1.28s/it]                                                      {'loss': 540.4062, 'learning_rate': 1.6480000000000001e-06, 'epoch': 0.04}
  0%|          | 230/50000 [05:49<17:43:03,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:35:20,453 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:35:20,453 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:35:20,453 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.55it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.78it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A                                                      
                                             [A{'eval_loss': 506.0, 'eval_accuracy': 0.29881077826283303, 'eval_runtime': 3.0981, 'eval_samples_per_second': 8.392, 'eval_steps_per_second': 2.259, 'epoch': 0.04}
  0%|          | 230/50000 [05:52<17:43:03,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A
                                             [A  0%|          | 231/50000 [05:53<30:25:12,  2.20s/it]  0%|          | 232/50000 [05:55<26:26:27,  1.91s/it]  0%|          | 233/50000 [05:56<23:38:53,  1.71s/it]  0%|          | 234/50000 [05:57<21:41:56,  1.57s/it]  0%|          | 235/50000 [05:58<20:20:08,  1.47s/it]  0%|          | 236/50000 [05:59<19:22:55,  1.40s/it]  0%|          | 237/50000 [06:01<18:42:54,  1.35s/it]  0%|          | 238/50000 [06:02<18:14:46,  1.32s/it]  0%|          | 239/50000 [06:03<17:58:11,  1.30s/it]  0%|          | 240/50000 [06:04<17:43:01,  1.28s/it]                                                      {'loss': 545.6688, 'learning_rate': 1.7280000000000002e-06, 'epoch': 0.04}
  0%|          | 240/50000 [06:04<17:43:01,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:35:35,975 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:35:35,975 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:35:35,975 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.55it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.78it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.58it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A                                                      
                                             [A{'eval_loss': 497.0, 'eval_accuracy': 0.2999397862411561, 'eval_runtime': 3.0905, 'eval_samples_per_second': 8.413, 'eval_steps_per_second': 2.265, 'epoch': 0.04}
  0%|          | 240/50000 [06:08<17:43:01,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A
                                             [A  0%|          | 241/50000 [06:09<30:22:41,  2.20s/it]  0%|          | 242/50000 [06:10<26:23:56,  1.91s/it]  0%|          | 243/50000 [06:11<23:36:35,  1.71s/it]  0%|          | 244/50000 [06:13<21:40:01,  1.57s/it]  0%|          | 245/50000 [06:14<20:18:11,  1.47s/it]  0%|          | 246/50000 [06:15<19:22:14,  1.40s/it]  0%|          | 247/50000 [06:16<18:43:16,  1.35s/it]  0%|          | 248/50000 [06:17<18:15:46,  1.32s/it]  0%|          | 249/50000 [06:19<17:55:26,  1.30s/it]  0%|          | 250/50000 [06:20<17:42:27,  1.28s/it]                                                      {'loss': 523.15, 'learning_rate': 1.808e-06, 'epoch': 0.04}
  0%|          | 250/50000 [06:20<17:42:27,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:35:51,483 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:35:51,483 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:35:51,483 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 467.75, 'eval_accuracy': 0.3036278789703447, 'eval_runtime': 3.1023, 'eval_samples_per_second': 8.381, 'eval_steps_per_second': 2.256, 'epoch': 0.04}
  0%|          | 250/50000 [06:23<17:42:27,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 251/50000 [06:24<30:25:10,  2.20s/it]  1%|          | 252/50000 [06:26<26:26:06,  1.91s/it]  1%|          | 253/50000 [06:27<23:40:29,  1.71s/it]  1%|          | 254/50000 [06:28<21:44:14,  1.57s/it]  1%|          | 255/50000 [06:29<20:23:33,  1.48s/it]  1%|          | 256/50000 [06:31<19:27:03,  1.41s/it]  1%|          | 257/50000 [06:32<18:45:58,  1.36s/it]  1%|          | 258/50000 [06:33<18:16:51,  1.32s/it]  1%|          | 259/50000 [06:34<17:56:16,  1.30s/it]  1%|          | 260/50000 [06:36<17:42:06,  1.28s/it]                                                      {'loss': 501.7344, 'learning_rate': 1.8880000000000002e-06, 'epoch': 0.04}
  1%|          | 260/50000 [06:36<17:42:06,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:36:07,027 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:36:07,027 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:36:07,027 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 452.5, 'eval_accuracy': 0.30746650609664306, 'eval_runtime': 3.1057, 'eval_samples_per_second': 8.372, 'eval_steps_per_second': 2.254, 'epoch': 0.04}
  1%|          | 260/50000 [06:39<17:42:06,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 261/50000 [06:40<30:25:43,  2.20s/it]  1%|          | 262/50000 [06:41<26:26:16,  1.91s/it]  1%|          | 263/50000 [06:42<23:38:29,  1.71s/it]  1%|          | 264/50000 [06:44<21:40:53,  1.57s/it]  1%|          | 265/50000 [06:45<20:19:30,  1.47s/it]  1%|          | 266/50000 [06:46<19:22:38,  1.40s/it]  1%|          | 267/50000 [06:47<18:44:19,  1.36s/it]  1%|          | 268/50000 [06:49<18:16:09,  1.32s/it]  1%|          | 269/50000 [06:50<17:56:23,  1.30s/it]  1%|          | 270/50000 [06:51<17:42:16,  1.28s/it]                                                      {'loss': 467.3687, 'learning_rate': 1.968e-06, 'epoch': 0.04}
  1%|          | 270/50000 [06:51<17:42:16,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:36:22,557 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:36:22,557 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:36:22,557 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 444.75, 'eval_accuracy': 0.31145566762005117, 'eval_runtime': 3.1033, 'eval_samples_per_second': 8.378, 'eval_steps_per_second': 2.256, 'epoch': 0.04}
  1%|          | 270/50000 [06:54<17:42:16,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 271/50000 [06:55<30:25:15,  2.20s/it]  1%|          | 272/50000 [06:57<26:26:14,  1.91s/it]  1%|          | 273/50000 [06:58<23:38:39,  1.71s/it]  1%|          | 274/50000 [06:59<21:41:01,  1.57s/it]  1%|          | 275/50000 [07:00<20:19:15,  1.47s/it]  1%|          | 276/50000 [07:02<19:22:43,  1.40s/it]  1%|          | 277/50000 [07:03<18:43:34,  1.36s/it]  1%|          | 278/50000 [07:04<18:16:29,  1.32s/it]  1%|          | 279/50000 [07:05<17:56:39,  1.30s/it]  1%|          | 280/50000 [07:07<17:41:59,  1.28s/it]                                                      {'loss': 434.8188, 'learning_rate': 2.048e-06, 'epoch': 0.04}
  1%|          | 280/50000 [07:07<17:41:59,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:36:38,088 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:36:38,088 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:36:38,088 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A                                                      
                                             [A{'eval_loss': 435.25, 'eval_accuracy': 0.3111545988258317, 'eval_runtime': 3.099, 'eval_samples_per_second': 8.39, 'eval_steps_per_second': 2.259, 'epoch': 0.04}
  1%|          | 280/50000 [07:10<17:41:59,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A
                                             [A  1%|          | 281/50000 [07:11<30:22:39,  2.20s/it]  1%|          | 282/50000 [07:12<26:25:24,  1.91s/it]  1%|          | 283/50000 [07:13<23:39:00,  1.71s/it]  1%|          | 284/50000 [07:15<21:42:11,  1.57s/it]  1%|          | 285/50000 [07:16<20:19:30,  1.47s/it]  1%|          | 286/50000 [07:17<19:21:37,  1.40s/it]  1%|          | 287/50000 [07:18<18:41:04,  1.35s/it]  1%|          | 288/50000 [07:20<18:13:56,  1.32s/it]  1%|          | 289/50000 [07:21<17:54:15,  1.30s/it]  1%|          | 290/50000 [07:22<17:40:20,  1.28s/it]                                                      {'loss': 469.7594, 'learning_rate': 2.128e-06, 'epoch': 0.04}
  1%|          | 290/50000 [07:22<17:40:20,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:36:53,605 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:36:53,605 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:36:53,605 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A                                                      
                                             [A{'eval_loss': 428.25, 'eval_accuracy': 0.3126599427969291, 'eval_runtime': 3.0961, 'eval_samples_per_second': 8.398, 'eval_steps_per_second': 2.261, 'epoch': 0.04}
  1%|          | 290/50000 [07:25<17:40:20,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A
                                             [A  1%|          | 291/50000 [07:26<30:20:47,  2.20s/it]  1%|          | 292/50000 [07:28<26:22:37,  1.91s/it]  1%|          | 293/50000 [07:29<23:36:40,  1.71s/it]  1%|          | 294/50000 [07:30<21:39:54,  1.57s/it]  1%|          | 295/50000 [07:31<20:18:05,  1.47s/it]  1%|          | 296/50000 [07:33<19:20:48,  1.40s/it]  1%|          | 297/50000 [07:34<18:40:32,  1.35s/it]  1%|          | 298/50000 [07:35<18:12:21,  1.32s/it]  1%|          | 299/50000 [07:36<17:52:56,  1.30s/it]  1%|          | 300/50000 [07:38<17:39:09,  1.28s/it]                                                      {'loss': 458.9, 'learning_rate': 2.2080000000000003e-06, 'epoch': 0.05}
  1%|          | 300/50000 [07:38<17:39:09,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:37:09,107 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:37:09,107 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:37:09,107 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 425.25, 'eval_accuracy': 0.31183200361282554, 'eval_runtime': 3.104, 'eval_samples_per_second': 8.376, 'eval_steps_per_second': 2.255, 'epoch': 0.05}
  1%|          | 300/50000 [07:41<17:39:09,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 301/50000 [07:42<30:23:47,  2.20s/it]  1%|          | 302/50000 [07:43<26:25:44,  1.91s/it]  1%|          | 303/50000 [07:44<23:39:58,  1.71s/it]  1%|          | 304/50000 [07:46<21:44:06,  1.57s/it]  1%|          | 305/50000 [07:47<20:23:05,  1.48s/it]  1%|          | 306/50000 [07:48<19:25:07,  1.41s/it]  1%|          | 307/50000 [07:49<18:44:23,  1.36s/it]  1%|          | 308/50000 [07:51<18:16:03,  1.32s/it]  1%|          | 309/50000 [07:52<17:56:03,  1.30s/it]  1%|          | 310/50000 [07:53<17:41:43,  1.28s/it]                                                      {'loss': 460.5031, 'learning_rate': 2.2880000000000004e-06, 'epoch': 0.05}
  1%|          | 310/50000 [07:53<17:41:43,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:37:24,665 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:37:24,665 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:37:24,665 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.49it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.15it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.53it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.42it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A                                                      
                                             [A{'eval_loss': 422.75, 'eval_accuracy': 0.31326208038536807, 'eval_runtime': 3.1396, 'eval_samples_per_second': 8.281, 'eval_steps_per_second': 2.23, 'epoch': 0.05}
  1%|          | 310/50000 [07:56<17:41:43,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A
                                             [A  1%|          | 311/50000 [07:58<30:33:40,  2.21s/it]  1%|          | 312/50000 [07:59<26:32:21,  1.92s/it]  1%|          | 313/50000 [08:00<23:42:52,  1.72s/it]  1%|          | 314/50000 [08:01<21:45:36,  1.58s/it]  1%|          | 315/50000 [08:03<20:22:49,  1.48s/it]  1%|          | 316/50000 [08:04<19:26:59,  1.41s/it]  1%|          | 317/50000 [08:05<18:46:37,  1.36s/it]  1%|          | 318/50000 [08:06<18:18:00,  1.33s/it]  1%|          | 319/50000 [08:07<17:56:57,  1.30s/it]  1%|          | 320/50000 [08:09<17:42:01,  1.28s/it]                                                      {'loss': 435.6687, 'learning_rate': 2.3680000000000005e-06, 'epoch': 0.05}
  1%|          | 320/50000 [08:09<17:42:01,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:37:40,255 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:37:40,255 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:37:40,256 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.50it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 415.0, 'eval_accuracy': 0.3154448291434593, 'eval_runtime': 3.1153, 'eval_samples_per_second': 8.346, 'eval_steps_per_second': 2.247, 'epoch': 0.05}
  1%|          | 320/50000 [08:12<17:42:01,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  1%|          | 321/50000 [08:13<30:26:45,  2.21s/it]  1%|          | 322/50000 [08:14<26:28:10,  1.92s/it]  1%|          | 323/50000 [08:16<23:40:54,  1.72s/it]  1%|          | 324/50000 [08:17<21:43:24,  1.57s/it]  1%|          | 325/50000 [08:18<20:20:10,  1.47s/it]  1%|          | 326/50000 [08:19<19:21:52,  1.40s/it]  1%|          | 327/50000 [08:21<18:40:55,  1.35s/it]  1%|          | 328/50000 [08:22<18:12:44,  1.32s/it]  1%|          | 329/50000 [08:23<17:52:55,  1.30s/it]  1%|          | 330/50000 [08:24<17:39:32,  1.28s/it]                                                      {'loss': 425.5125, 'learning_rate': 2.448e-06, 'epoch': 0.05}
  1%|          | 330/50000 [08:24<17:39:32,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:37:55,791 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:37:55,791 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:37:55,791 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 409.75, 'eval_accuracy': 0.3195092578654222, 'eval_runtime': 3.1109, 'eval_samples_per_second': 8.358, 'eval_steps_per_second': 2.25, 'epoch': 0.05}
  1%|          | 330/50000 [08:27<17:39:32,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  1%|          | 331/50000 [08:29<30:23:19,  2.20s/it]  1%|          | 332/50000 [08:30<26:24:33,  1.91s/it]  1%|          | 333/50000 [08:31<23:37:14,  1.71s/it]  1%|          | 334/50000 [08:32<21:40:10,  1.57s/it]  1%|          | 335/50000 [08:34<20:17:50,  1.47s/it]  1%|          | 336/50000 [08:35<19:20:33,  1.40s/it]  1%|          | 337/50000 [08:36<18:40:17,  1.35s/it]  1%|          | 338/50000 [08:37<18:12:19,  1.32s/it]  1%|          | 339/50000 [08:39<17:52:03,  1.30s/it]  1%|          | 340/50000 [08:40<17:38:34,  1.28s/it]                                                      {'loss': 443.3188, 'learning_rate': 2.5280000000000006e-06, 'epoch': 0.05}
  1%|          | 340/50000 [08:40<17:38:34,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:38:11,309 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:38:11,309 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:38:11,309 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 394.75, 'eval_accuracy': 0.3198855938581966, 'eval_runtime': 3.1087, 'eval_samples_per_second': 8.364, 'eval_steps_per_second': 2.252, 'epoch': 0.05}
  1%|          | 340/50000 [08:43<17:38:34,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 341/50000 [08:44<30:22:00,  2.20s/it]  1%|          | 342/50000 [08:45<26:23:08,  1.91s/it]  1%|          | 343/50000 [08:47<23:36:18,  1.71s/it]  1%|          | 344/50000 [08:48<21:39:41,  1.57s/it]  1%|          | 345/50000 [08:49<20:18:23,  1.47s/it]  1%|          | 346/50000 [08:50<19:20:57,  1.40s/it]  1%|          | 347/50000 [08:52<18:40:36,  1.35s/it]  1%|          | 348/50000 [08:53<18:12:26,  1.32s/it]  1%|          | 349/50000 [08:54<17:52:44,  1.30s/it]  1%|          | 350/50000 [08:55<17:39:06,  1.28s/it]                                                      {'loss': 415.8469, 'learning_rate': 2.608e-06, 'epoch': 0.05}
  1%|          | 350/50000 [08:55<17:39:06,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:38:26,832 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:38:26,832 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:38:26,832 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 385.25, 'eval_accuracy': 0.3235736865873852, 'eval_runtime': 3.1073, 'eval_samples_per_second': 8.367, 'eval_steps_per_second': 2.253, 'epoch': 0.05}
  1%|          | 350/50000 [08:58<17:39:06,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 351/50000 [09:00<30:21:54,  2.20s/it]  1%|          | 352/50000 [09:01<26:23:05,  1.91s/it]  1%|          | 353/50000 [09:02<23:36:11,  1.71s/it]  1%|          | 354/50000 [09:03<21:38:52,  1.57s/it]  1%|          | 355/50000 [09:05<20:17:02,  1.47s/it]  1%|          | 356/50000 [09:06<19:20:00,  1.40s/it]  1%|          | 357/50000 [09:07<18:40:00,  1.35s/it]  1%|          | 358/50000 [09:08<18:11:39,  1.32s/it]  1%|          | 359/50000 [09:10<17:51:49,  1.30s/it]  1%|          | 360/50000 [09:11<17:37:48,  1.28s/it]                                                      {'loss': 518.8719, 'learning_rate': 2.688e-06, 'epoch': 0.06}
  1%|          | 360/50000 [09:11<17:37:48,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:38:42,346 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:38:42,346 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:38:42,346 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 407.75, 'eval_accuracy': 0.3150684931506849, 'eval_runtime': 3.1035, 'eval_samples_per_second': 8.378, 'eval_steps_per_second': 2.256, 'epoch': 0.06}
  1%|          | 360/50000 [09:14<17:37:48,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 361/50000 [09:15<30:19:30,  2.20s/it]  1%|          | 362/50000 [09:16<26:21:56,  1.91s/it]  1%|          | 363/50000 [09:18<23:35:29,  1.71s/it]  1%|          | 364/50000 [09:19<21:38:20,  1.57s/it]  1%|          | 365/50000 [09:20<20:16:30,  1.47s/it]  1%|          | 366/50000 [09:21<19:19:06,  1.40s/it]  1%|          | 367/50000 [09:23<18:39:04,  1.35s/it]  1%|          | 368/50000 [09:24<18:11:00,  1.32s/it]  1%|          | 369/50000 [09:25<17:51:25,  1.30s/it]  1%|          | 370/50000 [09:26<17:37:48,  1.28s/it]                                                      {'loss': 417.15, 'learning_rate': 2.768e-06, 'epoch': 0.06}
  1%|          | 370/50000 [09:26<17:37:48,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:38:57,855 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:38:57,855 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:38:57,855 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.54it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 398.25, 'eval_accuracy': 0.3192834562697576, 'eval_runtime': 3.0995, 'eval_samples_per_second': 8.389, 'eval_steps_per_second': 2.258, 'epoch': 0.06}
  1%|          | 370/50000 [09:29<17:37:48,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 371/50000 [09:31<30:18:17,  2.20s/it]  1%|          | 372/50000 [09:32<26:20:18,  1.91s/it]  1%|          | 373/50000 [09:33<23:33:47,  1.71s/it]  1%|          | 374/50000 [09:34<21:37:09,  1.57s/it]  1%|          | 375/50000 [09:36<20:15:39,  1.47s/it]  1%|          | 376/50000 [09:37<19:19:23,  1.40s/it]  1%|          | 377/50000 [09:38<18:40:19,  1.35s/it]  1%|          | 378/50000 [09:39<18:12:26,  1.32s/it]  1%|          | 379/50000 [09:41<17:53:10,  1.30s/it]  1%|          | 380/50000 [09:42<17:39:27,  1.28s/it]                                                      {'loss': 478.9969, 'learning_rate': 2.848e-06, 'epoch': 0.06}
  1%|          | 380/50000 [09:42<17:39:27,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:39:13,374 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:39:13,374 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:39:13,374 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.54it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 403.0, 'eval_accuracy': 0.32093933463796476, 'eval_runtime': 3.0986, 'eval_samples_per_second': 8.391, 'eval_steps_per_second': 2.259, 'epoch': 0.06}
  1%|          | 380/50000 [09:45<17:39:27,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 381/50000 [09:46<30:19:12,  2.20s/it]  1%|          | 382/50000 [09:47<26:20:37,  1.91s/it]  1%|          | 383/50000 [09:49<23:33:52,  1.71s/it]  1%|          | 384/50000 [09:50<21:37:32,  1.57s/it]  1%|          | 385/50000 [09:51<20:15:54,  1.47s/it]  1%|          | 386/50000 [09:52<19:19:03,  1.40s/it]  1%|          | 387/50000 [09:54<18:41:00,  1.36s/it]  1%|          | 388/50000 [09:55<18:14:26,  1.32s/it]  1%|          | 389/50000 [09:56<17:58:40,  1.30s/it]  1%|          | 390/50000 [09:57<17:45:19,  1.29s/it]                                                      {'loss': 430.9906, 'learning_rate': 2.928e-06, 'epoch': 0.06}
  1%|          | 390/50000 [09:57<17:45:19,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:39:28,926 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:39:28,926 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:39:28,926 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.48it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.15it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.74it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.54it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A                                                      
                                             [A{'eval_loss': 403.5, 'eval_accuracy': 0.3210898690350745, 'eval_runtime': 3.1491, 'eval_samples_per_second': 8.256, 'eval_steps_per_second': 2.223, 'epoch': 0.06}
  1%|          | 390/50000 [10:01<17:45:19,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A
                                             [A  1%|          | 391/50000 [10:02<30:36:48,  2.22s/it]  1%|          | 392/50000 [10:03<26:34:39,  1.93s/it]  1%|          | 393/50000 [10:04<23:44:41,  1.72s/it]  1%|          | 394/50000 [10:06<21:46:05,  1.58s/it]  1%|          | 395/50000 [10:07<20:22:46,  1.48s/it]  1%|          | 396/50000 [10:08<19:24:25,  1.41s/it]  1%|          | 397/50000 [10:09<18:43:38,  1.36s/it]  1%|          | 398/50000 [10:11<18:13:53,  1.32s/it]  1%|          | 399/50000 [10:12<17:52:58,  1.30s/it]  1%|          | 400/50000 [10:13<17:39:24,  1.28s/it]                                                      {'loss': 495.0844, 'learning_rate': 3.0080000000000003e-06, 'epoch': 0.06}
  1%|          | 400/50000 [10:13<17:39:24,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:39:44,512 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:39:44,512 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:39:44,512 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 397.0, 'eval_accuracy': 0.3222941442119524, 'eval_runtime': 3.1143, 'eval_samples_per_second': 8.349, 'eval_steps_per_second': 2.248, 'epoch': 0.06}
  1%|          | 400/50000 [10:16<17:39:24,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  1%|          | 401/50000 [10:17<30:23:10,  2.21s/it]  1%|          | 402/50000 [10:19<26:24:18,  1.92s/it]  1%|          | 403/50000 [10:20<23:37:18,  1.71s/it]  1%|          | 404/50000 [10:21<21:39:56,  1.57s/it]  1%|          | 405/50000 [10:22<20:17:58,  1.47s/it]  1%|          | 406/50000 [10:24<19:20:38,  1.40s/it]  1%|          | 407/50000 [10:25<18:39:38,  1.35s/it]  1%|          | 408/50000 [10:26<18:11:01,  1.32s/it]  1%|          | 409/50000 [10:27<17:51:12,  1.30s/it]  1%|          | 410/50000 [10:29<17:37:40,  1.28s/it]                                                      {'loss': 417.4406, 'learning_rate': 3.0880000000000003e-06, 'epoch': 0.06}
  1%|          | 410/50000 [10:29<17:37:40,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:40:00,044 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:40:00,044 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:40:00,044 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 389.75, 'eval_accuracy': 0.3223694114105073, 'eval_runtime': 3.1102, 'eval_samples_per_second': 8.36, 'eval_steps_per_second': 2.251, 'epoch': 0.06}
  1%|          | 410/50000 [10:32<17:37:40,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 411/50000 [10:33<30:21:03,  2.20s/it]  1%|          | 412/50000 [10:34<26:22:50,  1.92s/it]  1%|          | 413/50000 [10:35<23:36:35,  1.71s/it]  1%|          | 414/50000 [10:37<21:40:28,  1.57s/it]  1%|          | 415/50000 [10:38<20:20:00,  1.48s/it]  1%|          | 416/50000 [10:39<19:23:21,  1.41s/it]  1%|          | 417/50000 [10:40<18:45:11,  1.36s/it]  1%|          | 418/50000 [10:42<18:17:02,  1.33s/it]  1%|          | 419/50000 [10:43<17:56:22,  1.30s/it]  1%|          | 420/50000 [10:44<17:41:23,  1.28s/it]                                                      {'loss': 418.3687, 'learning_rate': 3.1680000000000004e-06, 'epoch': 0.07}
  1%|          | 420/50000 [10:44<17:41:23,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:40:15,623 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:40:15,623 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:40:15,623 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.54it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 381.5, 'eval_accuracy': 0.32530483215414724, 'eval_runtime': 3.1041, 'eval_samples_per_second': 8.376, 'eval_steps_per_second': 2.255, 'epoch': 0.07}
  1%|          | 420/50000 [10:47<17:41:23,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 421/50000 [10:48<30:21:40,  2.20s/it]  1%|          | 422/50000 [10:50<26:23:18,  1.92s/it]  1%|          | 423/50000 [10:51<23:36:06,  1.71s/it]  1%|          | 424/50000 [10:52<21:39:10,  1.57s/it]  1%|          | 425/50000 [10:53<20:17:43,  1.47s/it]  1%|          | 426/50000 [10:55<19:20:11,  1.40s/it]  1%|          | 427/50000 [10:56<18:41:30,  1.36s/it]  1%|          | 428/50000 [10:57<18:14:55,  1.33s/it]  1%|          | 429/50000 [10:58<17:55:40,  1.30s/it]  1%|          | 430/50000 [11:00<17:40:28,  1.28s/it]                                                      {'loss': 426.9219, 'learning_rate': 3.248e-06, 'epoch': 0.07}
  1%|          | 430/50000 [11:00<17:40:28,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:40:31,173 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:40:31,173 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:40:31,173 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 370.5, 'eval_accuracy': 0.32500376335992776, 'eval_runtime': 3.1054, 'eval_samples_per_second': 8.372, 'eval_steps_per_second': 2.254, 'epoch': 0.07}
  1%|          | 430/50000 [11:03<17:40:28,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 431/50000 [11:04<30:20:57,  2.20s/it]  1%|          | 432/50000 [11:05<26:31:57,  1.93s/it]  1%|          | 433/50000 [11:07<23:42:07,  1.72s/it]  1%|          | 434/50000 [11:08<21:42:55,  1.58s/it]  1%|          | 435/50000 [11:09<20:20:21,  1.48s/it]  1%|          | 436/50000 [11:10<19:21:51,  1.41s/it]  1%|          | 437/50000 [11:11<18:41:06,  1.36s/it]  1%|          | 438/50000 [11:13<18:12:42,  1.32s/it]  1%|          | 439/50000 [11:14<17:53:23,  1.30s/it]  1%|          | 440/50000 [11:15<17:41:50,  1.29s/it]                                                      {'loss': 416.2906, 'learning_rate': 3.328e-06, 'epoch': 0.07}
  1%|          | 440/50000 [11:15<17:41:50,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:40:46,758 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:40:46,758 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:40:46,758 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.44it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.15it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.74it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.54it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A                                                      
                                             [A{'eval_loss': 368.25, 'eval_accuracy': 0.32658437452958, 'eval_runtime': 3.138, 'eval_samples_per_second': 8.286, 'eval_steps_per_second': 2.231, 'epoch': 0.07}
  1%|          | 440/50000 [11:18<17:41:50,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A
                                             [A  1%|          | 441/50000 [11:20<30:32:23,  2.22s/it]  1%|          | 442/50000 [11:21<26:31:49,  1.93s/it]  1%|          | 443/50000 [11:22<23:42:43,  1.72s/it]  1%|          | 444/50000 [11:23<21:44:01,  1.58s/it]  1%|          | 445/50000 [11:25<20:21:32,  1.48s/it]  1%|          | 446/50000 [11:26<19:22:26,  1.41s/it]  1%|          | 447/50000 [11:27<18:42:24,  1.36s/it]  1%|          | 448/50000 [11:28<18:13:41,  1.32s/it]  1%|          | 449/50000 [11:30<17:53:10,  1.30s/it]  1%|          | 450/50000 [11:31<17:39:08,  1.28s/it]                                                      {'loss': 380.5875, 'learning_rate': 3.4080000000000002e-06, 'epoch': 0.07}
  1%|          | 450/50000 [11:31<17:39:08,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:41:02,348 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:41:02,348 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:41:02,348 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.54it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 380.0, 'eval_accuracy': 0.3260575041396959, 'eval_runtime': 3.1049, 'eval_samples_per_second': 8.374, 'eval_steps_per_second': 2.254, 'epoch': 0.07}
  1%|          | 450/50000 [11:34<17:39:08,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 451/50000 [11:35<30:18:47,  2.20s/it]  1%|          | 452/50000 [11:36<26:20:05,  1.91s/it]  1%|          | 453/50000 [11:38<23:33:36,  1.71s/it]  1%|          | 454/50000 [11:39<21:36:35,  1.57s/it]  1%|          | 455/50000 [11:40<20:14:56,  1.47s/it]  1%|          | 456/50000 [11:41<19:17:19,  1.40s/it]  1%|          | 457/50000 [11:43<18:38:28,  1.35s/it]  1%|          | 458/50000 [11:44<18:10:15,  1.32s/it]  1%|          | 459/50000 [11:45<17:50:03,  1.30s/it]  1%|          | 460/50000 [11:46<17:35:56,  1.28s/it]                                                      {'loss': 425.9719, 'learning_rate': 3.4880000000000003e-06, 'epoch': 0.07}
  1%|          | 460/50000 [11:46<17:35:56,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:41:17,860 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:41:17,860 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:41:17,860 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 376.5, 'eval_accuracy': 0.3244768929700437, 'eval_runtime': 3.1005, 'eval_samples_per_second': 8.386, 'eval_steps_per_second': 2.258, 'epoch': 0.07}
  1%|          | 460/50000 [11:49<17:35:56,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 461/50000 [11:51<30:15:30,  2.20s/it]  1%|          | 462/50000 [11:52<26:20:41,  1.91s/it]  1%|          | 463/50000 [11:53<23:35:29,  1.71s/it]  1%|          | 464/50000 [11:54<21:40:13,  1.57s/it]  1%|          | 465/50000 [11:56<20:18:39,  1.48s/it]  1%|          | 466/50000 [11:57<19:22:50,  1.41s/it]  1%|          | 467/50000 [11:58<18:42:17,  1.36s/it]  1%|          | 468/50000 [11:59<18:14:03,  1.33s/it]  1%|          | 469/50000 [12:01<17:53:52,  1.30s/it]  1%|          | 470/50000 [12:02<17:38:56,  1.28s/it]                                                      {'loss': 403.6187, 'learning_rate': 3.5680000000000004e-06, 'epoch': 0.07}
  1%|          | 470/50000 [12:02<17:38:56,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:41:33,425 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:41:33,425 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:41:33,425 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.54it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A                                                      
                                             [A{'eval_loss': 364.75, 'eval_accuracy': 0.328917657684781, 'eval_runtime': 3.1267, 'eval_samples_per_second': 8.316, 'eval_steps_per_second': 2.239, 'epoch': 0.07}
  1%|          | 470/50000 [12:05<17:38:56,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A
                                             [A  1%|          | 471/50000 [12:06<30:25:33,  2.21s/it]  1%|          | 472/50000 [12:08<26:26:14,  1.92s/it]  1%|          | 473/50000 [12:09<23:38:12,  1.72s/it]  1%|          | 474/50000 [12:10<21:41:40,  1.58s/it]  1%|          | 475/50000 [12:11<20:20:04,  1.48s/it]  1%|          | 476/50000 [12:13<19:22:56,  1.41s/it]  1%|          | 477/50000 [12:14<18:42:54,  1.36s/it]  1%|          | 478/50000 [12:15<18:13:34,  1.32s/it]  1%|          | 479/50000 [12:16<17:52:39,  1.30s/it]  1%|          | 480/50000 [12:17<17:38:09,  1.28s/it]                                                      {'loss': 388.7156, 'learning_rate': 3.6480000000000005e-06, 'epoch': 0.07}
  1%|          | 480/50000 [12:17<17:38:09,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:41:49,006 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:41:49,006 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:41:49,006 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.54it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 378.0, 'eval_accuracy': 0.33042300165587835, 'eval_runtime': 3.0958, 'eval_samples_per_second': 8.399, 'eval_steps_per_second': 2.261, 'epoch': 0.07}
  1%|          | 480/50000 [12:21<17:38:09,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 481/50000 [12:22<30:15:46,  2.20s/it]  1%|          | 482/50000 [12:23<26:17:50,  1.91s/it]  1%|          | 483/50000 [12:24<23:31:21,  1.71s/it]  1%|          | 484/50000 [12:26<21:34:25,  1.57s/it]  1%|          | 485/50000 [12:27<20:13:37,  1.47s/it]  1%|          | 486/50000 [12:28<19:15:48,  1.40s/it]  1%|          | 487/50000 [12:29<18:35:42,  1.35s/it]  1%|          | 488/50000 [12:31<18:07:49,  1.32s/it]  1%|          | 489/50000 [12:32<17:48:12,  1.29s/it]  1%|          | 490/50000 [12:33<17:34:10,  1.28s/it]                                                      {'loss': 467.0, 'learning_rate': 3.7280000000000006e-06, 'epoch': 0.08}
  1%|          | 490/50000 [12:33<17:34:10,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:42:04,499 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:42:04,499 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:42:04,499 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.51it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 357.0, 'eval_accuracy': 0.3313262080385368, 'eval_runtime': 3.1027, 'eval_samples_per_second': 8.38, 'eval_steps_per_second': 2.256, 'epoch': 0.08}
  1%|          | 490/50000 [12:36<17:34:10,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 491/50000 [12:37<30:13:32,  2.20s/it]  1%|          | 492/50000 [12:39<26:15:44,  1.91s/it]  1%|          | 493/50000 [12:40<23:29:18,  1.71s/it]  1%|          | 494/50000 [12:41<21:33:43,  1.57s/it]  1%|          | 495/50000 [12:42<20:13:05,  1.47s/it]  1%|          | 496/50000 [12:44<19:17:39,  1.40s/it]  1%|          | 497/50000 [12:45<18:37:45,  1.35s/it]  1%|          | 498/50000 [12:46<18:10:54,  1.32s/it]  1%|          | 499/50000 [12:47<17:51:24,  1.30s/it]  1%|          | 500/50000 [12:49<17:37:22,  1.28s/it]                                                      {'loss': 421.0094, 'learning_rate': 3.8080000000000006e-06, 'epoch': 0.08}
  1%|          | 500/50000 [12:49<17:37:22,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:42:20,023 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:42:20,023 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:42:20,023 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.44it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.13it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.52it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.41it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.35it/s][A                                                      
                                             [A{'eval_loss': 351.75, 'eval_accuracy': 0.32974559686888455, 'eval_runtime': 3.1611, 'eval_samples_per_second': 8.225, 'eval_steps_per_second': 2.214, 'epoch': 0.08}
  1%|          | 500/50000 [12:52<17:37:22,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.35it/s][A
                                             [A  1%|          | 501/50000 [12:53<30:33:17,  2.22s/it]  1%|          | 502/50000 [12:54<26:31:09,  1.93s/it]  1%|          | 503/50000 [12:55<23:41:36,  1.72s/it]  1%|          | 504/50000 [12:57<21:43:28,  1.58s/it]  1%|          | 505/50000 [12:58<20:21:02,  1.48s/it]  1%|          | 506/50000 [12:59<19:24:41,  1.41s/it]  1%|          | 507/50000 [13:00<18:45:20,  1.36s/it]  1%|          | 508/50000 [13:02<18:14:46,  1.33s/it]  1%|          | 509/50000 [13:03<17:55:50,  1.30s/it]  1%|          | 510/50000 [13:04<17:42:57,  1.29s/it]                                                      {'loss': 390.1406, 'learning_rate': 3.888e-06, 'epoch': 0.08}
  1%|          | 510/50000 [13:04<17:42:57,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:42:35,671 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:42:35,671 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:42:35,671 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.50it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.55it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A                                                      
                                             [A{'eval_loss': 327.5, 'eval_accuracy': 0.33524010236339, 'eval_runtime': 3.1274, 'eval_samples_per_second': 8.314, 'eval_steps_per_second': 2.238, 'epoch': 0.08}
  1%|          | 510/50000 [13:07<17:42:57,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A
                                             [A  1%|          | 511/50000 [13:09<30:26:59,  2.22s/it]  1%|          | 512/50000 [13:10<26:28:17,  1.93s/it]  1%|          | 513/50000 [13:11<23:39:41,  1.72s/it]  1%|          | 514/50000 [13:12<21:45:11,  1.58s/it]  1%|          | 515/50000 [13:14<20:23:07,  1.48s/it]  1%|          | 516/50000 [13:15<19:25:49,  1.41s/it]  1%|          | 517/50000 [13:16<18:46:15,  1.37s/it]  1%|          | 518/50000 [13:17<18:19:30,  1.33s/it]  1%|          | 519/50000 [13:19<17:59:52,  1.31s/it]  1%|          | 520/50000 [13:20<17:48:02,  1.30s/it]                                                      {'loss': 346.9, 'learning_rate': 3.968e-06, 'epoch': 0.08}
  1%|          | 520/50000 [13:20<17:48:02,  1.30s/it][INFO|trainer.py:3129] 2023-06-30 19:42:51,331 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:42:51,331 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:42:51,331 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.45it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.16it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.73it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.53it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A                                                      
                                             [A{'eval_loss': 324.25, 'eval_accuracy': 0.33493903356917054, 'eval_runtime': 3.1454, 'eval_samples_per_second': 8.266, 'eval_steps_per_second': 2.225, 'epoch': 0.08}
  1%|          | 520/50000 [13:23<17:48:02,  1.30s/it]
100%|██████████| 7/7 [00:02<00:00,  2.36it/s][A
                                             [A  1%|          | 521/50000 [13:24<30:35:34,  2.23s/it]  1%|          | 522/50000 [13:25<26:33:14,  1.93s/it]  1%|          | 523/50000 [13:27<23:44:03,  1.73s/it]  1%|          | 524/50000 [13:28<21:46:37,  1.58s/it]  1%|          | 525/50000 [13:29<20:23:55,  1.48s/it]  1%|          | 526/50000 [13:30<19:25:58,  1.41s/it]  1%|          | 527/50000 [13:32<18:45:45,  1.37s/it]  1%|          | 528/50000 [13:33<18:17:22,  1.33s/it]  1%|          | 529/50000 [13:34<17:57:30,  1.31s/it]  1%|          | 530/50000 [13:35<17:42:23,  1.29s/it]                                                      {'loss': 348.0125, 'learning_rate': 4.048e-06, 'epoch': 0.08}
  1%|          | 530/50000 [13:35<17:42:23,  1.29s/it][INFO|trainer.py:3129] 2023-06-30 19:43:06,977 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:43:06,977 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:43:06,977 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 321.75, 'eval_accuracy': 0.3348637663706157, 'eval_runtime': 3.109, 'eval_samples_per_second': 8.363, 'eval_steps_per_second': 2.251, 'epoch': 0.08}
  1%|          | 530/50000 [13:39<17:42:23,  1.29s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 531/50000 [13:40<30:21:22,  2.21s/it]  1%|          | 532/50000 [13:41<26:22:55,  1.92s/it]  1%|          | 533/50000 [13:42<23:35:25,  1.72s/it]  1%|          | 534/50000 [13:44<21:38:22,  1.57s/it]  1%|          | 535/50000 [13:45<20:16:11,  1.48s/it]  1%|          | 536/50000 [13:46<19:18:40,  1.41s/it]  1%|          | 537/50000 [13:47<18:37:55,  1.36s/it]  1%|          | 538/50000 [13:49<18:09:21,  1.32s/it]  1%|          | 539/50000 [13:50<17:49:32,  1.30s/it]  1%|          | 540/50000 [13:51<17:35:36,  1.28s/it]                                                      {'loss': 358.6219, 'learning_rate': 4.128e-06, 'epoch': 0.08}
  1%|          | 540/50000 [13:51<17:35:36,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:43:22,513 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:43:22,513 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:43:22,513 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.52it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A                                                      
                                             [A{'eval_loss': 314.25, 'eval_accuracy': 0.3377239199157007, 'eval_runtime': 3.106, 'eval_samples_per_second': 8.371, 'eval_steps_per_second': 2.254, 'epoch': 0.08}
  1%|          | 540/50000 [13:54<17:35:36,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.39it/s][A
                                             [A  1%|          | 541/50000 [13:55<30:15:15,  2.20s/it]  1%|          | 542/50000 [13:57<26:17:56,  1.91s/it]  1%|          | 543/50000 [13:58<23:31:35,  1.71s/it]  1%|          | 544/50000 [13:59<21:35:34,  1.57s/it]  1%|          | 545/50000 [14:00<20:13:58,  1.47s/it]  1%|          | 546/50000 [14:02<19:16:56,  1.40s/it]  1%|          | 547/50000 [14:03<18:36:47,  1.35s/it]  1%|          | 548/50000 [14:04<18:08:38,  1.32s/it]  1%|          | 549/50000 [14:05<17:48:39,  1.30s/it]  1%|          | 550/50000 [14:07<17:34:59,  1.28s/it]                                                      {'loss': 330.5938, 'learning_rate': 4.208e-06, 'epoch': 0.09}
  1%|          | 550/50000 [14:07<17:34:59,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:43:38,042 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:43:38,042 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:43:38,042 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.50it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.17it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.56it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 320.0, 'eval_accuracy': 0.33667017913593256, 'eval_runtime': 3.1133, 'eval_samples_per_second': 8.351, 'eval_steps_per_second': 2.248, 'epoch': 0.09}
  1%|          | 550/50000 [14:10<17:34:59,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  1%|          | 551/50000 [14:11<30:16:28,  2.20s/it]  1%|          | 552/50000 [14:12<26:18:18,  1.92s/it]  1%|          | 553/50000 [14:13<23:32:19,  1.71s/it]  1%|          | 554/50000 [14:15<21:36:28,  1.57s/it]  1%|          | 555/50000 [14:16<20:15:21,  1.47s/it]  1%|          | 556/50000 [14:17<19:17:31,  1.40s/it]  1%|          | 557/50000 [14:18<18:37:40,  1.36s/it]  1%|          | 558/50000 [14:20<18:09:20,  1.32s/it]  1%|          | 559/50000 [14:21<17:49:34,  1.30s/it]  1%|          | 560/50000 [14:22<17:35:46,  1.28s/it]                                                      {'loss': 347.2156, 'learning_rate': 4.288e-06, 'epoch': 0.09}
  1%|          | 560/50000 [14:22<17:35:46,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:43:53,587 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:43:53,587 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:43:53,587 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 324.5, 'eval_accuracy': 0.3356164383561644, 'eval_runtime': 3.1095, 'eval_samples_per_second': 8.361, 'eval_steps_per_second': 2.251, 'epoch': 0.09}
  1%|          | 560/50000 [14:25<17:35:46,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  1%|          | 561/50000 [14:26<30:15:46,  2.20s/it]  1%|          | 562/50000 [14:28<26:18:14,  1.92s/it]  1%|          | 563/50000 [14:29<23:31:59,  1.71s/it]  1%|          | 564/50000 [14:30<21:35:28,  1.57s/it]  1%|          | 565/50000 [14:31<20:13:33,  1.47s/it]  1%|          | 566/50000 [14:33<19:15:57,  1.40s/it]  1%|          | 567/50000 [14:34<18:35:43,  1.35s/it]  1%|          | 568/50000 [14:35<18:07:41,  1.32s/it]  1%|          | 569/50000 [14:36<17:48:25,  1.30s/it]  1%|          | 570/50000 [14:38<17:34:29,  1.28s/it]                                                      {'loss': 317.45, 'learning_rate': 4.368e-06, 'epoch': 0.09}
  1%|          | 570/50000 [14:38<17:34:29,  1.28s/it][INFO|trainer.py:3129] 2023-06-30 19:44:09,116 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:44:09,117 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:44:09,117 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.53it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.77it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.57it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.46it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A                                                      
                                             [A{'eval_loss': 325.0, 'eval_accuracy': 0.3330573536052988, 'eval_runtime': 3.1084, 'eval_samples_per_second': 8.365, 'eval_steps_per_second': 2.252, 'epoch': 0.09}
  1%|          | 570/50000 [14:41<17:34:29,  1.28s/it]
100%|██████████| 7/7 [00:02<00:00,  2.38it/s][A
                                             [A  1%|          | 571/50000 [14:42<30:14:09,  2.20s/it]  1%|          | 572/50000 [14:43<26:16:32,  1.91s/it]  1%|          | 573/50000 [14:44<23:30:16,  1.71s/it]  1%|          | 574/50000 [14:46<21:33:59,  1.57s/it]  1%|          | 575/50000 [14:47<20:12:30,  1.47s/it]  1%|          | 576/50000 [14:48<19:15:49,  1.40s/it]  1%|          | 577/50000 [14:49<18:35:54,  1.35s/it]  1%|          | 578/50000 [14:51<18:07:52,  1.32s/it]WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 156618 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 156619 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 156622 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 156617) of binary: /home/humu/miniconda3/envs/cn_llama/bin/python
Traceback (most recent call last):
  File "/home/humu/miniconda3/envs/cn_llama/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
run_clm_pt_wo_peft.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-06-30_19:44:24
  host      : SH-IDC1-10-140-24-142
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 156617)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 156617
=======================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-06-30 19:44:53,070] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
06/30/2023 19:44:53 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
06/30/2023 19:44:53 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
06/30/2023 19:44:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:666] 2023-06-30 19:44:53,923 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/config.json
[INFO|configuration_utils.py:720] 2023-06-30 19:44:53,923 >> Model config LlamaConfig {
  "_name_or_path": "/nvme/share_data/llama_ckpts/huggingface/7B",
  "architectures": [
    "LLaMAForCausalLM"
  ],
  "bos_token_id": 0,
  "eos_token_id": 1,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": -1,
  "rms_norm_eps": 1e-06,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:44:53,924 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:44:53,924 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:44:53,924 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2023-06-30 19:44:53,924 >> loading file tokenizer_config.json
06/30/2023 19:44:53 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
06/30/2023 19:44:53 - INFO - __main__ - training datasets-llama7b_generate_data_0 has been loaded from disk
06/30/2023 19:44:53 - INFO - __main__ - training datasets-llama7b_generate_data_1 has been loaded from disk
06/30/2023 19:44:53 - INFO - __main__ - training datasets-llama7b_generate_data_3 has been loaded from disk
06/30/2023 19:44:53 - INFO - __main__ - training datasets-llama7b_generate_data_2 has been loaded from disk
06/30/2023 19:44:53 - INFO - __main__ - training datasets-llama7b_generate_data_5 has been loaded from disk
06/30/2023 19:44:53 - INFO - __main__ - training datasets-llama7b_generate_data_4 has been loaded from disk
06/30/2023 19:44:53 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-0a97ce12e14fd931.arrow
06/30/2023 19:44:53 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-d1662e72e88700da.arrow
06/30/2023 19:44:55 - INFO - __main__ - Num train_samples  25802
06/30/2023 19:44:55 - INFO - __main__ - training example:
06/30/2023 19:44:55 - INFO - __main__ -  ⁇  {128} If death thri wit cave his enti ⁇  {131} Though Arthur was a wiseman, born in Wales, and had been the greatest king was every southern races had ever had, for you will never be able to understand unless you have read Sir Thomas Malory's quaintly written book, and sifted the high-flown mannerisms.Malory's purpose was threefold. One: to gather a mass of stories about the knights from wandering minstrels and others which seemed to be reliable, but which had never been committed to paper or had been gathered in heaps here and there. Two: to arrange these by date and by source, giving where he could his version of the tale. Three : to give chronology to these heaps of tales and to support his researches by text of both Old and New Testaments.In this not a word is printed or omitted that he could not demonstrate to be found in the realms of other writersBefore proceeding, it is well to answer the question that will repeatedly be asked: Why was Malory writing at this time? The answer is not far to seek. The church had been campaigning against the use of the romances by the people by constantly compiling lists of those authors whom they condemned for either-offenses against faith, or morality, or the use of the natural as against the supernatural in such writings. The truth is that the galliard, the rustic jig,* the marriage state, and the minnesong -- meaning songs of love at variance with the ⁇  {132} perverted almost out of their old meaning into something slavish and corruptive. His exact words are that, unlike the Bible, the "moderns well wymmen" are of love rather than theology, but he is making the same meaning. Similarly have all attempted to degrade the good out of religion into ceremonial, left a gap in place of him who did not treat the Bible exclusively as a textbook of faith. In a much religious, and a more moral world, the same minstrel as in easier times entertained men and stirred their hearts, and took their praise for songs which sought to keep alive in them the love of God and reverence for womanhood. good chivalry : to put us in practise of virtu," the virtus that lived in knights
06/30/2023 19:44:55 - INFO - __main__ - Num eval_samples  26
06/30/2023 19:44:55 - INFO - __main__ - training example:
06/30/2023 19:44:55 - INFO - __main__ - 14 shots and three bullets from a semi-hot 22 which pierced the officers skull, ⁇  The shooting and investigation, FOMASBEE'S ddcmmpe closer dmay be "pulling foot soldiers" now shards of firestickers in the office dim the lights triggered flares the kings believe to have. backgrounds in society, unwisely captain USA network has got as the back burner, distortion and colleagues, ⁇  tedesca, che a partire dagli anni 1970 si allontana sempre più dalla maggiore dominanza mondiale della Formula 1, avvenne nei Paesi Bassi con il pilota Eddie Chich è campione del mondo nel 1988 e in questa stagione vince anche il campionato europeo Baltic Challenge Cup; ⁇  ⁇  Schumacher, campione del mondo 1990, il pilota francese Thierry Boutsen, campione del mondo 1993, l'australiano Mika Salo, campione Alemannic Hungariap Cup nel 1987, l'italiano Fabrizio Barbazza, campione del mondo 1991, è la prima vittima della sua classifica intitolato a lui stesso nel 1996. Nei successivi anni nascono le seguenti classi: ⁇  ⁇  * IMSA, serie nordamericana ⁇  * ITC, serie tailandese ⁇  * Le Mans Series, serie francese ⁇  * Intercontinental Le Mans Cup, serie GT ⁇  * European Touring Car Championship, serie turismo endurance ⁇  * Super GT, serie giapponese ⁇  * V de V Challenge, serie cipriota ⁇  * WTCC (World Touring Car Championship), serie turismo ⁇  * BTCC (British Touring Car Championship), serie Gran Touring ⁇  ⁇  Negli anni 2010, nascono aggiuntivi anche: ⁇  ⁇  * IDEC Sport Leagues 12H Series (19 agosto 2016) ⁇  * P2 Sport Emotion (25 agosto 2016) ⁇  * Road to Le Mans Series ⁇  ⁇  Nel proprio settore sono stati inclusi i Blancpain Series (2011), dei trofei dalle applicazioni alla classe GT: ⁇  ⁇  * GT Open ⁇  * FIA GT
[INFO|modeling_utils.py:2531] 2023-06-30 19:44:55,941 >> loading weights file /nvme/share_data/llama_ckpts/huggingface/7B/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-06-30 19:44:55,942 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:575] 2023-06-30 19:44:55,942 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": -1,
  "transformers_version": "4.28.1"
}

06/30/2023 19:44:55 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-0a97ce12e14fd931.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-d1662e72e88700da.arrow
06/30/2023 19:44:55 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-0a97ce12e14fd931.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-d1662e72e88700da.arrow
06/30/2023 19:44:55 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-0a97ce12e14fd931.arrow and /home/humu/data/llm-qat-try_cache/llama7b_generate_data_0/train/cache-d1662e72e88700da.arrow
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  6.23it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  5.86it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:05,  5.76it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:06,  4.67it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  6.17it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  6.11it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  6.13it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:05,  5.44it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:04,  6.02it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:04,  6.22it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:04,  6.22it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:05,  5.74it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  6.07it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  6.22it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  6.23it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:00<00:04,  5.97it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.43it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.10it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.18it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:00<00:04,  6.18it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:00<00:04,  6.58it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:00<00:04,  6.12it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:00<00:04,  6.19it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:04,  6.30it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:03,  6.73it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.15it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.19it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:01<00:04,  6.39it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:03,  6.89it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.18it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:04,  6.11it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:01<00:03,  6.29it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  7.01it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.23it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.05it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:01<00:03,  6.23it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  7.05it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.24it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.06it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  7.00it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:01<00:03,  6.15it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.24it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.95it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.08it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:01<00:03,  6.05it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.25it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:01<00:02,  6.96it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  6.08it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:01<00:03,  5.97it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:02,  7.05it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.26it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  6.10it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:02<00:03,  5.98it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  7.14it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  6.29it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  6.11it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:02<00:03,  6.03it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  7.19it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.31it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.13it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:02<00:02,  6.07it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.75it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.33it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.16it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:02<00:02,  6.08it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.30it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.37it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.19it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:02<00:02,  6.09it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.40it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:02<00:02,  6.03it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.22it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:02<00:02,  6.09it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  5.85it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.24it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:03<00:02,  6.10it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.44it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:02,  5.74it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.25it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:03<00:02,  6.11it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.48it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.22it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  5.65it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:03<00:01,  6.13it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.54it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.17it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  5.59it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.59it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:03<00:01,  6.17it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.15it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.61it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:03<00:01,  6.20it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  5.55it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.15it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:03<00:01,  6.62it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:03<00:01,  6.20it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:03<00:01,  5.53it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  6.17it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.63it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:04<00:01,  6.19it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  5.52it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.63it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.19it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:04<00:01,  6.14it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:01,  5.53it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.62it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.20it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:04<00:00,  6.00it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  5.53it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.61it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  6.22it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:04<00:00,  5.93it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.63it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  5.54it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  6.24it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:04<00:00,  5.96it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:04<00:00,  6.64it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.23it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  5.54it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:04<00:00,  6.04it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:04<00:00,  6.61it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:05<00:00,  6.22it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:05<00:00,  5.51it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:05<00:00,  6.14it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  6.37it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.78it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.32it/s]
[INFO|modeling_utils.py:3190] 2023-06-30 19:45:01,274 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2023-06-30 19:45:01,274 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /nvme/share_data/llama_ckpts/huggingface/7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2023-06-30 19:45:01,276 >> loading configuration file /nvme/share_data/llama_ckpts/huggingface/7B/generation_config.json
[INFO|configuration_utils.py:575] 2023-06-30 19:45:01,277 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  5.33it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:05<00:00,  5.74it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.06it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  6.04it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  4.84it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.99it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  4.69it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:05<00:00,  5.88it/s]
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.q_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.0.self_attn.k_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.0.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.0.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.0.mlp.down_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.0.mlp.up_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.1.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.1.self_attn.o_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.1.mlp.gate_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.1.mlp.down_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.1.mlp.up_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.2.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.2.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.2.self_attn.o_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.2.mlp.gate_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.2.mlp.down_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.2.mlp.up_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.k_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.3.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.3.mlp.gate_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.3.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.3.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.4.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.4.self_attn.k_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.4.self_attn.v_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.4.self_attn.o_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.4.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.4.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.4.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.5.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.5.self_attn.k_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.5.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.5.self_attn.o_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.5.mlp.gate_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.5.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.5.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.6.self_attn.q_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.6.self_attn.k_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.6.self_attn.v_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.6.self_attn.o_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.6.mlp.gate_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.6.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.6.mlp.up_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.7.self_attn.q_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.7.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.v_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.7.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.7.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.7.mlp.down_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.7.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.8.self_attn.k_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.8.self_attn.v_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.8.self_attn.o_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.8.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.8.mlp.down_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.8.mlp.up_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.9.self_attn.q_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.9.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.9.self_attn.v_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.9.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.9.mlp.gate_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.9.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.9.mlp.up_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.10.self_attn.q_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.10.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.v_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.10.self_attn.o_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.10.mlp.gate_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.10.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.10.mlp.up_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.11.self_attn.q_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.11.self_attn.k_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.v_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.11.self_attn.o_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.11.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.down_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.11.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.12.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.12.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.12.self_attn.v_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.12.self_attn.o_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.gate_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.12.mlp.down_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.12.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.13.self_attn.k_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.13.self_attn.v_proj to QLinear
Convert model.layers.13.self_attn.o_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.13.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.13.mlp.down_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.13.mlp.up_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.q_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.14.self_attn.k_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.14.self_attn.v_proj to QLinear
Convert model.layers.14.self_attn.o_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.gate_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.14.mlp.down_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.14.mlp.up_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.15.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.15.self_attn.k_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.15.self_attn.v_proj to QLinear
Convert model.layers.15.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.15.mlp.gate_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.15.mlp.down_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.15.mlp.up_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.16.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.16.self_attn.k_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.16.self_attn.v_proj to QLinear
Convert model.layers.16.self_attn.o_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.16.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.16.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.16.mlp.up_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.17.self_attn.q_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.17.self_attn.k_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.17.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.17.self_attn.o_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.17.mlp.gate_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.17.mlp.down_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.17.mlp.up_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.18.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.18.self_attn.k_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.18.self_attn.v_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.18.self_attn.o_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.18.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.18.mlp.down_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.18.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.19.self_attn.q_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.19.self_attn.k_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.19.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.19.self_attn.o_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.19.mlp.gate_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.19.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.19.mlp.up_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.20.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.20.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.20.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.20.self_attn.o_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.20.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.20.mlp.down_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.20.mlp.up_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.21.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.21.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.21.self_attn.v_proj to QLinear
Convert model.layers.21.self_attn.o_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.21.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.21.mlp.down_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.21.mlp.up_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.q_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.22.self_attn.k_proj to QLinear
Convert model.layers.22.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.22.self_attn.o_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.22.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.22.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.22.mlp.up_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.23.self_attn.q_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.23.self_attn.k_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.23.self_attn.v_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.23.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.23.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.23.mlp.down_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.23.mlp.up_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.q_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.24.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.24.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.24.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.24.mlp.gate_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.24.mlp.down_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.24.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.25.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.25.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.25.self_attn.v_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.25.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.25.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.25.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.25.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.26.self_attn.q_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.26.self_attn.k_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.26.self_attn.v_proj to QLinear
Convert model.layers.26.self_attn.o_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.26.mlp.gate_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.26.mlp.down_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.26.mlp.up_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.27.self_attn.q_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.27.self_attn.k_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.27.self_attn.v_proj to QLinear
Convert model.layers.27.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.27.mlp.gate_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.27.mlp.down_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert lm_head to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.27.mlp.up_proj to QLinear
Convert model.layers.28.self_attn.q_proj to QLinear
Convert model.layers.28.self_attn.k_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.28.self_attn.v_proj to QLinear
Convert model.layers.28.self_attn.o_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.28.mlp.gate_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.28.mlp.down_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.28.mlp.up_proj to QLinear
Convert model.layers.29.self_attn.q_proj to QLinear
Convert model.layers.29.self_attn.k_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.29.self_attn.v_proj to QLinear
Convert model.layers.29.self_attn.o_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.29.mlp.gate_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.29.mlp.down_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.29.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.o_proj to QLinear
Convert model.layers.30.self_attn.q_proj to QLinear
Convert model.layers.30.self_attn.k_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.30.self_attn.v_proj to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.30.self_attn.o_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.30.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert model.layers.30.mlp.down_proj to QLinear
Convert model.layers.30.mlp.up_proj to QLinear
Convert model.layers.31.self_attn.q_proj to QLinear
Convert model.layers.31.self_attn.k_proj to QLinear
Convert model.layers.31.self_attn.v_proj to QLinear
Convert lm_head to QLinear
[INFO|trainer.py:564] 2023-06-30 19:45:45,884 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:621] 2023-06-30 19:45:45,884 >> Using cuda_amp half precision backend
Convert model.layers.31.self_attn.o_proj to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Convert lm_head to QLinear
Convert model.layers.31.mlp.gate_proj to QLinear
Convert model.layers.31.mlp.down_proj to QLinear
Convert model.layers.31.mlp.up_proj to QLinear
Convert lm_head to QLinear
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-06-30 19:45:48,620] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
06/30/2023 19:45:48 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
06/30/2023 19:45:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/humu/miniconda3/envs/cn_llama/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
06/30/2023 19:45:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
06/30/2023 19:45:53 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
06/30/2023 19:45:53 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
06/30/2023 19:45:53 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
06/30/2023 19:45:53 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
06/30/2023 19:45:53 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-06-30 19:45:54,392] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-06-30 19:45:54,393] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-06-30 19:45:54,393] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-06-30 19:45:54,406] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-06-30 19:45:54,406] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-06-30 19:45:54,406] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-06-30 19:45:54,406] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-06-30 19:45:54,406] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000
[2023-06-30 19:45:54,406] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000
[2023-06-30 19:45:54,406] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-06-30 19:45:54,406] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/humu/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.20876145362854004 seconds
Loading extension module utils...
Time to load utils op: 0.20235085487365723 seconds
Loading extension module utils...
Time to load utils op: 0.20261788368225098 seconds
Loading extension module utils...
Time to load utils op: 0.20299363136291504 seconds
Rank: 0 partition count [4] and sizes[(1684603904, False)] 
Rank: 1 partition count [4] and sizes[(1684603904, False)] 
Rank: 3 partition count [4] and sizes[(1684603904, False)] 
Rank: 2 partition count [4] and sizes[(1684603904, False)] 
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00038552284240722656 seconds
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0008046627044677734 seconds
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006289482116699219 seconds
[2023-06-30 19:46:05,909] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-06-30 19:46:05,910] [INFO] [utils.py:786:see_memory_usage] MA 31.47 GB         Max_MA 34.61 GB         CA 34.62 GB         Max_CA 35 GB 
[2023-06-30 19:46:05,910] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.68 GB, percent = 2.4%
[WARNING|logging.py:295] 2023-06-30 19:46:05,928 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-06-30 19:46:05,928 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-06-30 19:46:05,936 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-06-30 19:46:06,049] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-06-30 19:46:06,050] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 56.58 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-06-30 19:46:06,050] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.71 GB, percent = 2.5%
[2023-06-30 19:46:06,050] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-06-30 19:46:06,147] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-06-30 19:46:06,147] [INFO] [utils.py:786:see_memory_usage] MA 44.02 GB         Max_MA 44.02 GB         CA 59.73 GB         Max_CA 60 GB 
[2023-06-30 19:46:06,148] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 24.91 GB, percent = 2.5%
[2023-06-30 19:46:06,151] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-06-30 19:46:06,151] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-06-30 19:46:06,151] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f1b20103e50>
[2023-06-30 19:46:06,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-06-30 19:46:06,154] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   amp_enabled .................. False
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   amp_params ................... False
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
[2023-06-30 19:46:06,154] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1b20103c40>
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   communication_data_type ...... None
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   disable_allgather ............ False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   dump_state ................... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   elasticity_enabled ........... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   fp16_auto_cast ............... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   fp16_enabled ................. True
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   global_rank .................. 0
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   loss_scale ................... 0
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   memory_breakdown ............. False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   mics_shard_size .............. -1
[2023-06-30 19:46:06,155] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   optimizer_name ............... None
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   optimizer_params ............. None
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   pld_enabled .................. False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   pld_params ................... False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   prescale_gradients ........... False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   scheduler_name ............... None
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   scheduler_params ............. None
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   sparse_attention ............. None
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   steps_per_print .............. 2000
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   train_batch_size ............. 4
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   use_node_local_storage ....... False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   world_size ................... 4
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   zero_enabled ................. True
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
[2023-06-30 19:46:06,156] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2
[2023-06-30 19:46:06,156] [INFO] [config.py:945:print_user_config]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 100, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1e-10
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 1.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 1.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 4, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "zero_allow_untested_optimizer": true
}
Using /home/humu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004684925079345703 seconds
[INFO|trainer.py:1769] 2023-06-30 19:46:06,159 >> ***** Running training *****
[INFO|trainer.py:1770] 2023-06-30 19:46:06,159 >>   Num examples = 25,802
[INFO|trainer.py:1771] 2023-06-30 19:46:06,159 >>   Num Epochs = 1
[INFO|trainer.py:1772] 2023-06-30 19:46:06,159 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1773] 2023-06-30 19:46:06,159 >>   Total train batch size (w. parallel, distributed & accumulation) = 4
[INFO|trainer.py:1774] 2023-06-30 19:46:06,159 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1775] 2023-06-30 19:46:06,159 >>   Total optimization steps = 20
[INFO|trainer.py:1776] 2023-06-30 19:46:06,162 >>   Number of trainable parameters = 6,738,415,616
  0%|          | 0/20 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-06-30 19:46:06,176 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-06-30 19:46:07,895] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  5%|▌         | 1/20 [00:01<00:32,  1.73s/it]                                              {'loss': 3640.75, 'learning_rate': 0.0, 'epoch': 0.0}
  5%|▌         | 1/20 [00:01<00:32,  1.73s/it][2023-06-30 19:46:08,885] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 10%|█         | 2/20 [00:02<00:23,  1.30s/it][2023-06-30 19:46:09,868] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
 15%|█▌        | 3/20 [00:03<00:19,  1.15s/it][2023-06-30 19:46:10,848] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
 20%|██        | 4/20 [00:04<00:17,  1.08s/it][2023-06-30 19:46:11,829] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
 25%|██▌       | 5/20 [00:05<00:15,  1.05s/it][2023-06-30 19:46:12,812] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
 30%|███       | 6/20 [00:06<00:14,  1.03s/it][2023-06-30 19:46:13,793] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
 35%|███▌      | 7/20 [00:07<00:13,  1.01s/it][2023-06-30 19:46:14,770] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
 40%|████      | 8/20 [00:08<00:12,  1.00s/it][2023-06-30 19:46:15,750] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
 45%|████▌     | 9/20 [00:09<00:10,  1.01it/s][2023-06-30 19:46:16,730] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
 50%|█████     | 10/20 [00:10<00:09,  1.01it/s]                                               {'loss': 3533.6111, 'learning_rate': 0.0, 'epoch': 0.0}
 50%|█████     | 10/20 [00:10<00:09,  1.01it/s][INFO|trainer.py:3129] 2023-06-30 19:46:16,736 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:46:16,736 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:46:16,736 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.42it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.11it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.53it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.43it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A                                               
                                             [A{'eval_loss': 3508.0, 'eval_accuracy': 0.08031010085804606, 'eval_runtime': 3.1467, 'eval_samples_per_second': 8.263, 'eval_steps_per_second': 2.225, 'epoch': 0.0}
 50%|█████     | 10/20 [00:13<00:09,  1.01it/s]
100%|██████████| 7/7 [00:02<00:00,  2.37it/s][A
                                             [A[2023-06-30 19:46:20,861] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
 55%|█████▌    | 11/20 [00:14<00:17,  1.95s/it][2023-06-30 19:46:21,838] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
 60%|██████    | 12/20 [00:15<00:13,  1.65s/it][2023-06-30 19:46:22,824] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
 65%|██████▌   | 13/20 [00:16<00:10,  1.45s/it][2023-06-30 19:46:23,811] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 70%|███████   | 14/20 [00:17<00:07,  1.31s/it][2023-06-30 19:46:24,798] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
 75%|███████▌  | 15/20 [00:18<00:06,  1.21s/it][2023-06-30 19:46:25,787] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
 80%|████████  | 16/20 [00:19<00:04,  1.15s/it][2023-06-30 19:46:26,773] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
 85%|████████▌ | 17/20 [00:20<00:03,  1.10s/it][2023-06-30 19:46:27,765] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
 90%|█████████ | 18/20 [00:21<00:02,  1.07s/it] 95%|█████████▌| 19/20 [00:22<00:01,  1.12s/it]100%|██████████| 20/20 [00:24<00:00,  1.15s/it]                                               {'loss': 3551.85, 'learning_rate': 1.9863613034027224e-05, 'epoch': 0.0}
100%|██████████| 20/20 [00:24<00:00,  1.15s/it][INFO|trainer.py:3129] 2023-06-30 19:46:30,242 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:46:30,242 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:46:30,242 >>   Batch size = 1

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  4.55it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.78it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.58it/s][A
 86%|████████▌ | 6/7 [00:02<00:00,  2.47it/s][A
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A                                               
                                             [A{'eval_loss': 1764.0, 'eval_accuracy': 0.1494806563299714, 'eval_runtime': 3.0917, 'eval_samples_per_second': 8.41, 'eval_steps_per_second': 2.264, 'epoch': 0.0}
100%|██████████| 20/20 [00:27<00:00,  1.15s/it]
100%|██████████| 7/7 [00:02<00:00,  2.40it/s][A
                                             [A[INFO|trainer.py:2039] 2023-06-30 19:46:33,334 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 27.1724, 'train_samples_per_second': 2.944, 'train_steps_per_second': 0.736, 'train_loss': 3548.0875, 'epoch': 0.0}
100%|██████████| 20/20 [00:27<00:00,  1.15s/it]100%|██████████| 20/20 [00:27<00:00,  1.36s/it]
***** train metrics *****
  epoch                    =        0.0
  train_loss               =  3548.0875
  train_runtime            = 0:00:27.17
  train_samples            =      25802
  train_samples_per_second =      2.944
  train_steps_per_second   =      0.736
06/30/2023 19:46:33 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3129] 2023-06-30 19:46:33,340 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-06-30 19:46:33,340 >>   Num examples = 26
[INFO|trainer.py:3134] 2023-06-30 19:46:33,340 >>   Batch size = 1
  0%|          | 0/7 [00:00<?, ?it/s] 29%|██▊       | 2/7 [00:00<00:01,  4.56it/s] 43%|████▎     | 3/7 [00:00<00:01,  3.22it/s] 57%|█████▋    | 4/7 [00:01<00:01,  2.78it/s] 71%|███████▏  | 5/7 [00:01<00:00,  2.59it/s] 86%|████████▌ | 6/7 [00:02<00:00,  2.48it/s]100%|██████████| 7/7 [00:02<00:00,  2.41it/s]False
False
100%|██████████| 7/7 [00:02<00:00,  2.65it/s]
False
***** eval metrics *****
  epoch                   =        0.0
  eval_accuracy           =     0.1495
  eval_loss               =     1764.0
  eval_runtime            = 0:00:03.08
  eval_samples            =         26
  eval_samples_per_second =      8.438
  eval_steps_per_second   =      2.272
  perplexity              =        inf
True
